{"pages":[{"title":"Build the blog with hexo","text":"My work mainly follows the instruction on https://www.simon96.online/2018/10/12/hexo-tutorial/ 1. prepared workinstall nodejs/ git 2. install hexo command line1$ npm install -g hexo-cli 3. init the folder for hexo123$ hexo init &lt;folder&gt; # generate the hexo folder$ cd &lt;folder&gt;$ npm install # npm will download and install according to the dependency configuration in the package.json configuration file 4. start the server1$ hexo s Then open the link http://localhost:4000/ to check whether the blog can run in localhost 5.publish my blog on a remote addresschoose github pages and purchased domain new a repososity name &lt;githubname&gt;.github.io also set the original hexo folder as &lt;githubname&gt;.github.io install hexo-deployer-git plugin modifiy the configuration file named _config.yml1234deploy: type: git repo: git@github.com:&lt;Github account name&gt;/&lt;the repososity we created before&gt; branch: master generate new pages and push them to remote12$ hexo g$ hexo d now we can visit our blog on https://.github.io buy a domain on Godaddy set the DNS as follows create a file named CNAME under source folderand inside the file, write my domain and use command to push the modified to github files wait some miniutes then I could visit my website on my domain 6.change the theme of hexochoose the theme named icarus and its github link ishttps://github.com/ppoffice/hexo-theme-icarus.Put the downloaded folder under the themes folder. And change the setting of the whole hexo project in _config.yml and change the setting of the theme in the themes/hexo-theme-icarus/_config.yml.The main changes In the _config.yml set title, author and time zone1234567title: lawilet's websitesubtitle: ''description: ''keywords:author: lawiet019language: entimezone: 'America/New_York' set the theme 1theme: hexo-theme-icarus In the themes/hexo-theme-icarus/_config.yml change the logo and favicon 12345logo: /img/favicon.png# Page metadata configurationshead: # URL or path to the website's icon favicon: /img/favicon.png Also change the settings in the widgets labels like the link to github. block the comments,payment and so on. And remember to generate and deploy again. 7. add about page install the hexo-admin1npm install --save hexo-admin enter the admin address like localhost:4000/admin create a new page named about remaining problems: comments plugin: I tried the gitalk. But sincemy domain didnot support https so it doesnot work.","link":"/Build-the-blog-with-hexo/index.html"},{"title":"about","text":"this is just a demo of about","link":"/about/index.html"},{"title":"bit manipulation in python","text":"","link":"/bit-manipulation-in-python/index.html"}],"posts":[{"title":"Build the blog with hexo","text":"My work mainly follows the instruction on https://www.simon96.online/2018/10/12/hexo-tutorial/ 1. prepared workinstall nodejs/ git 2. install hexo command line1$ npm install -g hexo-cli 3. init the folder for hexo123$ hexo init &lt;folder&gt; # generate the hexo folder$ cd &lt;folder&gt;$ npm install # npm will download and install according to the dependency configuration in the package.json configuration file 4. start the server1$ hexo s Then open the link http://localhost:4000/ to check whether the blog can run in localhost 5.publish my blog on a remote addresschoose github pages and purchased domain new a repososity name &lt;githubname&gt;.github.io also set the original hexo folder as &lt;githubname&gt;.github.io install hexo-deployer-git plugin modifiy the configuration file named _config.yml1234deploy: type: git repo: git@github.com:&lt;Github account name&gt;/&lt;the repososity we created before&gt; branch: master generate new pages and push them to remote12$ hexo g$ hexo d now we can visit our blog on https://.github.io buy a domain on Godaddy set the DNS as follows create a file named CNAME under source folderand inside the file, write my domain and use command to push the modified to github files wait some miniutes then I could visit my website on my domain 6.change the theme of hexochoose the theme named icarus and its github link ishttps://github.com/ppoffice/hexo-theme-icarus.Put the downloaded folder under the themes folder. And change the setting of the whole hexo project in _config.yml and change the setting of the theme in the themes/hexo-theme-icarus/_config.yml.The main changes In the _config.yml set title, author and time zone1234567title: lawilet's websitesubtitle: ''description: ''keywords:author: lawiet019language: entimezone: 'America/New_York' set the theme 1theme: hexo-theme-icarus In the themes/hexo-theme-icarus/_config.yml change the logo and favicon 12345logo: /img/favicon.png# Page metadata configurationshead: # URL or path to the website's icon favicon: /img/favicon.png Also change the settings in the widgets labels like the link to github. block the comments,payment and so on. And remember to generate and deploy again. 7. add about page install the hexo-admin1npm install --save hexo-admin enter the admin address like localhost:4000/admin create a new page named about 8. support latex install hexo-math plugin1npm install hexo-math --save modifiy render engine 12345## uninstall the original onenpm uninstall hexo-renderer-marked --save## install new one hexo-renderer-kramednpm install hexo-renderer-kramed --save Modify the kramed configuration to resolve semantic conflictsHexo and MathJax perform perfectly most of the time, but when the special symbols of Markdwon itself conflict with the symbols in Latex:So I changed the file ~/myblogname/node_modules\\kramed\\lib\\rules\\inline.js 11 line: 1escape: /^\\\\([`*\\[\\]()#$+\\-.!_&gt;])/ 20 line: 1em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, turn on the mathjex render engine In the file themes/hexo-theme-icarus/_config.yml, to change code like follows 1mathjax: true -hexo clean 9. build the counter of visitorsIn fact, the icarus theme use busuanzi as plugin. So, I only need to change the line in the themes/hexo-theme-icarus/_config.yml 1busuanzi: true 10. change the node versionsometimes I may face the problem which needs me to change the version of node.Here is how to do it. 123npm install -g n # install n tool in globalsudo n latest # Install the latest version of nodejssudo n &lt;version number&gt; # Install the specific version of nodejs remaining problems: comments plugin: I tried the gitalk. But sincemy domain didnot support https so it doesnot work.","link":"/2020/05/12/Build-the-blog-with-hexo/"},{"title":"Floyd’s Cycle-Finding Algorithm","text":"Approach:This is the fastest method to detect cycles and has been described below: Traverse linked list using two pointers. Move one pointer(slow_p) by one and another pointer(fast_p) by two. If these pointers meet at the same node then there is a loop. If pointers do not meet then linked list doesn’t have a loop. why it works:1) When the slow pointer enters the loop, the fast pointer is already inside the loop. Set the distance between the fast pointer and the slow pointer be K.2) Now, if we consider the slow movement of the pointers, we can notice that the distance between them increases by one after each iteration. After an iteration (slow pointer = next node of slow pointer, fast pointer = next next node of fast pointer), the distance between fast pointer and slow pointer becomes K + 1, after two iterations, it is K + 2, and so on. When the distance becomes the length n of the ring, they will join because they move in a ring of length n.For example, we can see in the following figure that the initial distance is 2. After one iteration, the distance becomes 3, and after two iterations, the distance becomes 4. After three iterations, it becomes 5. It is also distance 0. They meet. complexity Time complexity: O(n).Only one traversal of the loop is needed. Auxiliary Space:O(1).There is no space required. implement in Python1234567891011# Leercide 141. Linked List Cycleclass Solution: def hasCycle(self, head): slow = head fast = head while slow and fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False Extend knowledge question： How to the length of the cycle? How to find the first node in a cycle (i.e. linked list cycle II)? How to change a linked list into a single linked list? How to judge whether two single linked lists have intersection? How to find the first intersection node? The distance traveled by slow at the first encounter: a+b, the distance traveled by fast: a+b+c+b.Because the speed of fast is twice that of slow, the distance traveled by fast is twice that of slow. There are 2(a+b) = a+b+c+b, and you can get a=c (this conclusion is very important! We found that L=b+c=a+b, that is, From the beginning to the first encounter of the two points, the number of cycles is equal to the length of the ring. We have already reached the conclusion a=c, then let the two pointers start from X and Z, one step at a time, then it will meet at Y!That is the first node of the ring. At the end of the previous question, the link between the node before point Y in segment c and Y can be cut off. How to judge whether two singly linked lists have intersection?First determine whether the two linked lists have cycles, if one has cycles and one has no cycles, it must not intersect; if both have no rings, determine whether the tails of the two lists are equal; if both have cycles, determine the Z on a linked list Whether the point is on another linked list.Here is an example to return the node cycles begins(142. Linked List Cycle II)1234567891011121314151617181920class Solution(object): def detectCycle(self, head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" slow = head fast = head while slow and fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: slow = head while True: if slow == fast: return slow slow = slow.next fast = fast.next return None referencehttps://blog.csdn.net/ccmlove123/article/details/104052556https://blog.csdn.net/xy010902100449/article/details/48995255","link":"/2020/07/01/Floyd%E2%80%99s-Cycle-Finding-Algorithm/"},{"title":"Dijkstra&#39;s algorithm","text":"overviewDijkstra’s algorithm (or Dijkstra’s Shortest Path First algorithm, SPF algorithm) is an algorithm for finding the shortest paths between nodes in a graph.(Dijkstra algorithm is mainly used to find the minimum path from any point in a digraph whose weight is not negative to any other node (when two points are connected with each other). Dijkstra’s algorithm vs BFSBreadth first search is suitable for unweighted graph to find the shortest path of two points. Dijkstras algorithm, which is suitable for weighted graph to find the path with the least total weight. steps of algorithms A node s in the graph is given as the starting point. Given an array dist [] to store the distance from all nodes in the graph to s. Initialize dist [s] to 0. For other nodes v in the graph, the initial dist [v] is infinite. The significance of initializing to infinity is that we assume that all other nodes are not connected with s in the current situation. With the implementation of the algorithm, dist [v] will save the shortest path distance from s to v in the graph. Given a minimum heap, record it as Q. The top of the stack is the nearest node to s and the corresponding distance. Put (s, 0) in the heap. Given a set, record it as S, and save all the visited nodes. Set it is initially empty. Based on the properties of Dijkstra algorithm, we always traverse every node with the shortest path, so for any node, once we have visited it, it means that we have got the shortest path from s to this node. When Q is not empty, take out the element (v, dist [v]) at the top of the heap, that is, the node v closest to s and its distance dist [v].If v is in S, it represents the shortest path that we have visited v. Then skip the current v and repeat step 1.Otherwise, put v in S. For each node t adjacent to v:If dist [v] + weight (v, t) &lt; dist [t], update dist [t] = dist [v] + weight (v, t). At the same time, put (t, dist [t]) into Q.Otherwise, do nothing. When the algorithm is finished, the weight value (or length) of the shortest path from each node except s in the graph to s is saved in dist []. If there is no connection path from s to v, dist [v] = ∞. the correctness of the algorithmSuppose that dist [v] stores the shortest path from the starting point s to v for each node v that has been visited.When the algorithm is initialized, only dist [s] = 0 is included indist [], and its correctness is obvious. For the rest n-1 nodes, if u has been accessed and v has not been accessed, and there is an edge u - &gt; v between u and v, whose weight is weight (u, v), then there must be dist [v] = dist [u] + weight (u, v). Otherwise, if there is another shorter path dist [t] satisfying dist [t] + weight (t, v), then according to the above algorithm, t must be accessed before u, which conflicts with our current assumption. The conclusion is valid for all the remaining nodes.Therefore, Dijkstra algorithm must be able to give the shortest path from the starting point to all other nodes . examples:a video from Youtube implement in Python12345678910111213141516171819202122232425262728293031323334353637import heapqdef calculate_distances(graph, starting_vertex): distances = {vertex: float('infinity') for vertex in graph} distances[starting_vertex] = 0 pq = [(0, starting_vertex)] while len(pq) &gt; 0: current_distance, current_vertex = heapq.heappop(pq) # Nodes can get added to the priority queue multiple times. We only # process a vertex the first time we remove it from the priority queue. if current_distance &gt; distances[current_vertex]: continue for neighbor, weight in graph[current_vertex].items(): distance = current_distance + weight # Only consider this new path if it's better than any path we've # already found. if distance &lt; distances[neighbor]: distances[neighbor] = distance heapq.heappush(pq, (distance, neighbor)) return distancesexample_graph = { 'U': {'V': 2, 'W': 5, 'X': 1}, 'V': {'U': 2, 'X': 2, 'W': 3}, 'W': {'V': 3, 'U': 5, 'X': 3, 'Y': 1, 'Z': 5}, 'X': {'U': 1, 'V': 2, 'W': 3, 'Y': 1}, 'Y': {'X': 1, 'W': 1, 'Z': 1}, 'Z': {'W': 5, 'Y': 1},}print(calculate_distances(example_graph, 'X')) complexitySuppose there are E edges and N nodes in the graph.Time complexity: O (ElogE). Because the minimum heap used is up to the O (logE) size,and we extract each element from it once.Spatial complexity: O (N + E). Where o (N) is the space used to store dist. O (E) is the space used to store the adjacency list and the minimum heap of a graph. referencehttps://www.jianshu.com/p/cd97f8197f18 https://bradfieldcs.com/algos/graphs/dijkstras-algorithm/","link":"/2020/06/16/Dijkstra-s-algorithm/"},{"title":"Learning of Clang","text":"1.What is ClangClang is a lightweight compiler for C, C++, and Objective-C 2.Install in Linux12sudo apt-get install clangsudo apt-get install llvm 3. the process clang++ worksIn the C/C++compiler, the process of compiling the program is mainly divided into the following four stages: preprocessPreprocessing is mainly the process of text replacement： Delete all #define and expand all macro definitions.To put it bluntly, it’s character substitution Handle all conditional compilation instructions, such as #ifdef, #ifndef, #endif, and so on Process #include, insert the file pointed to by #include into that line Delete all comments Add line numbers and file labels so that you can know which line is in which file when debugging and compiling errors occur Retain the #pragma compiler instructions because they are required by the compiler.The command we use:1clang++ -E test.cpp -o test.i compileIn this stage, the preprocessed file is transformed into assembly language by compiler. Every statement in assembly language accurately describes an address machine instruction in a standard text format. The files generated at this stage are machine code. This stage generates an assembly text file ending in .s.The command we use:1clang++ -S test.i -o test.s assemblyThis stage is to assemble the assembly file obtained in the previous step into machine instructions, thereby packaging these instructions into a redirectable target program format.At this time, a binary file ending with .o is generated.The command we use:1lang++ -c test.s -o test.o linkThis stage is mainly to deal with calling the system library such as cin and other functions in the file, then you need to merge the cin.o in the system library into the test.o we generated.Generate executable ending with .exe binary file1clang++ test.o -o test.exe Sum up In real work, we only use one instruction to represent this four stages to 1clang++ test.cpp -o test.exe referencehttps://www.jianshu.com/p/f46b05e8a0e8https://blog.csdn.net/qq_36752072/article/details/89026161","link":"/2020/08/07/Learning-of-Clang/"},{"title":"Learning of wireshark","text":"what is wiresharkWireshark is a free and open-source packet analyzer. It is used for network troubleshooting, analysis, software and communications protocol development, and education.Wireshark’s native network trace file format is the libpcap format supported by libpcap and WinPcap, so it can exchange captured network traces with other applications that use the same format, including tcpdump and CA NetMaster. It can also read captures from other network analyzers, such as snoop, Network General’s Sniffer, and Microsoft Network Monitor. downloadhttps://www.wireshark.org/ Intro to the interface How to use the filter Protocol filteringIt is relatively simple. You can directly input the protocol name in the packet capture filter box.(lowercase needed)123tcp # only displays the packet list of TCP protocolhttp # only view the packet list of HTTP protocolicmp # only view the packet list of icmp protocol IP filtering123ip.src ==192.168.1.104 # displays the list of packets with source address 192.168.1.104ip.dst==192.168.1.104 #display the list of packets whose destination address is 192.168.1.104ip.addr == 192.168.1.104 #displays a list of packets with source IP address or destination IP address 192.168.1.104 Port filtering123tcp.port ==80 # displays a list of packets with source port 80 or destination host port 80.tcp.srcport == 80, only displays the list of packets with TCP source port 80.、cp.dstport == 80, only display the list of packets whose destination port of TCP protocol is 80. http mode filtering1http.request.method==&quot;GET&quot; - Only the HTTP GET method is displayed. comparison operator== (equal to),!= (not equal),&gt; (greater than), &lt;(less than), &gt;= (greater than or equal), &lt;= (less than or equal) 1ip.src==IP-address and ip.dst==IP-address Logical operators&amp; &amp; (and), | (or),!（not) 1tcp.port == 80 || udp.port == 80 Filter by packet content.Suppose I want to filter the content in the IMCP layer, I can click the code stream in the selected interface and select the data below. data of requests (text form ) The first line: the overall overview of the data packet. The second line: link layer details, mainly the mac addresses of both parties The Third line: network layer details, mainly the IP addresses of both parties The fourth line: detailed information of the transport layer, mainly the port numbers of both parties. For the requests list part:Here are explainations about different columns column description No. This is the number order of the packet that got captured. The bracket indicates that this packet is part of a conversation. Time This column shows you how long after you started the capture that this packet got captured. You can change this value in the Settings menu if you need something different displayed. Source This is the address of the system that sent the packet. Destination This is the address of the destination of that packet. Protocol This is the type of packet, for example, TCP, DNS, DHCPv6, or ARP. Length This column shows you the length of the packet in bytes Info This column shows you more information about the packet contents, and will vary depending on what kind of packet it is referencehttps://www.cnblogs.com/mq0036/p/11187138.htmlhttps://www.varonis.com/blog/how-to-use-wireshark/","link":"/2020/07/22/Learning-of-wireshark/"},{"title":"Sieve of Eratosthenes","text":"It is the most efficient way to find all of the small primes steps: Create a list of consecutive integers from $2$ to $n$: $(2, 3, 4, …, n)$. Initially, let p equal 2, the first prime number. Starting from $p^2$, count up in increments of $p$ and mark each of these numbers greater than or equal to $p^2$ itself in the list. These numbers will be $p(p+1), p(p+2), p(p+3)$, etc..The reason why we can start from $p^2$ rather than $2p$ is that if a number can be divided by $np ( n is less than p)$, we could find the number will be removed in previous turn. For example, we don’t need to remove 6 when $p =3$ because it is removed when p =2 Find the first number greater than $p$ in the list that is not marked. If there was no such number, stop. Otherwise, let $p$ now equal this number (which is the next prime), and repeat from step 3. The upper limit of $p$ don’t need to be $n$ but $\\sqrt{n}$ python code123456789101112class Solution:# @param {integer} n# @return {integer} def countPrimes(self, n): if n &lt; 3: return 0 primes = [True] * n primes[0] = primes[1] = False for i in range(2, int(n ** 0.5) + 1): if primes[i]: primes[i * i: n: i] = [False] * len(primes[i * i: n: i]) return sum(primes)","link":"/2020/08/15/Sieve-of-Eratosthenes/"},{"title":"Levenshtein Distance&#x2F; minimum edit distance","text":"Since I am not familar with dynamic programming so I use an example to delve into this topic what is dynamic programmingDynamic Programming is mainly an optimization over plain recursion. Wherever we see a recursive solution that has repeated calls for same inputs, we can optimize it using Dynamic Programming. The idea is to simply store the results of subproblems, so that we do not have to re-compute them when needed later. what is Levenshtein DistanceLevenshtein distance (LD) is a measure of the similarity between two strings, which we will refer to as the source string (s) and the target string (t). the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. For example,If s is &quot;test&quot;and t is &quot;test&quot;, then LD(s,t) = 0, because no transformations are needed. The strings are already identical.If s is &quot;test&quot; and t is &quot;tent&quot;, then LD(s,t) = 1, because one substitution (change “s” to “n”) is sufficient to transform s into t. the difference between Levenshtein Distance and Hamming distancethe Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different.If the strings are the same size, the Hamming distance is an upper bound on the Levenshtein distance. how to use dynamic programming to solve the problemwe should construct a matrix based on the length of source string and target string. The problem we should focus on is in which direction we could solve it. And the induction formula of the matirx.Here is an example I saw on the Youtube the source string : abcdefthe target string : agced from there we can construct a matrix as below a b c d e f a g c e d The empty box represent null value we could start to fill the matrix by fullfill the first column and first row. to understand the value in the matrix, the value of (2,3) is how many steps we minimally need to convert ab to a a b c d e f 0 1 2 3 4 5 6 a 1 g 2 c 3 e 4 d 5 then we go to the induction part after observation, we found the formula 12345if row(a) == column(b): cost = 0else: cost = 1v(a,b) = min(v(a-1,b)+1, v(a-1,b-1)+cost,v(a,b-1)+1) v(a-1,b): convert s[:b] to t[:a-1] in previous steps v(a,b-1):convert s[:b-1] to t[:a] in previous steps v(a-1,b-1):convert s[:b-1] to t[:a-1] in previous steps v represents the value of a certain entry and first element a is the index of row while the second element b is the index of column. implement it in Python 1234567891011121314151617def LD(source,target): ori_mat = [[-1] *(len(source)+1) for i in range(len(target)+1)] # init for i in range(len(source)+1): ori_mat[0][i] = i for j in range(len(target)+1): ori_mat[j][0] = j print(ori_mat) for j in range(1,len(target)+1): for i in range(1,len(source)+1): if source[i-1] == target[j-1]: cost = 0 else: cost =1 ori_mat[j][i] = min(ori_mat[j-1][i]+1,ori_mat[j][i-1]+1,ori_mat[j-1][i-1]+cost) return ori_mat[-1][-1]","link":"/2020/06/15/Levenshtein-Distance-minimum-edit-distance/"},{"title":"Topological sorting","text":"1. defintation:Topological sorting problem: given digraph G = (V, E) ,find a linear ordering of vertices such that:for all edges (v, w) in E, v precedes w in the ordering. Topological sorting for Directed Acyclic Graph (DAG) is a linear ordering of vertices such that for every directed edge uv, vertex u comes before v in the ordering. Topological Sorting for a graph is not possible if the graph is not a DAG. 2.alogrithm Step 1: Identify vertices that have no incoming edge• The in-degree of these vertices is zeroIf no such edges, graph has cycles (cyclic graph) Step 2: Delete this vertex of in-degree 0 and all itsoutgoing edges from the graph. Place it in the output. Step 3: Repeat Steps 1 and Step 2 until graph is empty 3. implemented in Python12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#Python program to print topological sorting of a DAGfrom collections import defaultdict#Class to represent a graphclass Graph: def __init__(self,vertices): self.graph = defaultdict(list) #dictionary containing adjacency List self.V = vertices #No. of vertices # function to add an edge to graph def addEdge(self,u,v): self.graph[u].append(v) # A recursive function used by topologicalSort def topologicalSortUtil(self,v,visited,stack): # Mark the current node as visited. visited[v] = True # Recur for all the vertices adjacent to this vertex for i in self.graph[v]: if visited[i] == False: self.topologicalSortUtil(i,visited,stack) # Push current vertex to stack which stores result stack.insert(0,v) # The function to do Topological Sort. It uses recursive # topologicalSortUtil() def topologicalSort(self): # Mark all the vertices as not visited visited = [False]*self.V stack =[] # Call the recursive helper function to store Topological # Sort starting from all vertices one by one for i in range(self.V): if visited[i] == False: self.topologicalSortUtil(i,visited,stack) # Print contents of the stack print(stack)g= Graph(6)g.addEdge(5, 2);g.addEdge(5, 0);g.addEdge(4, 0);g.addEdge(4, 1);g.addEdge(2, 3);g.addEdge(3, 1);g.topologicalSort()#This code is contributed by Neelam Yadav Another implementation can be seen:1234567891011121314151617181920212223242526272829def topoSort(graph): in_degrees = dict((u,0) for u in graph) #初始化所有顶点入度为0 num = len(in_degrees) for u in graph: for v in graph[u]: in_degrees[v] += 1 #计算每个顶点的入度 Q = [u for u in in_degrees if in_degrees[u] == 0] # 筛选入度为0的顶点 Seq = [] while Q: u = Q.pop() #默认从最后一个删除 Seq.append(u) for v in graph[u]: in_degrees[v] -= 1 #移除其所有出边 if in_degrees[v] == 0: Q.append(v) #再次筛选入度为0的顶点 if len(Seq) == num: #输出的顶点数是否与图中的顶点数相等 return Seq else: return NoneG = { 'a':'bf', 'b':'cdf', 'c':'d', 'd':'ef', 'e':'f', 'f':''}print(topoSort(G)) referencegeeksforgeeks.org/topological-sorting/","link":"/2020/07/01/Topological-sorting/"},{"title":"Permutation vs Combination","text":"1. the basic permutation and Combination1.1 formula select k from n objects k sequence (order matter &amp; no repetition) $\\frac{n!}{(n-k)!}$ k sequence (order matter &amp; repetition) $n^{k}$ k subset (order does not matter &amp; no repetition)$\\tbinom{n}{k}$ k subset (order does not matter &amp; repetition)$\\tbinom{n+k-1}{n-1}$The proof of this can be seen as https://blog.csdn.net/qq_37806688/article/details/108479361 1.2 examples to compare four of themThere are 10 ice-cream sundae toppings from which you select 4, how many sundaes are possible if you don’t repeat toppings and the order added does not matter to you?no repeat，order does not matter,n =10, k = 4 $\\tbinom{n}{k}$ = $\\tbinom{10}{4}$ you don’t repeat toppings and the order added matters to you?no repeat, order matters,n =10,k=4$\\frac{n!}{(n-k)!}$ = $\\frac{10!}{(10-4)!}$ you may repeat toppings and the order added does not matter to you?repeat, order does not matter,n =10, k = 4$\\tbinom{n+k-1}{n-1}$ = $\\tbinom{10+4-1}{10-1}$ you may repeat toppings and the order added matters to you? repeat, order matters,n =10,k=4 $n^{k}$ = $10^{4}$ 2. other formulaThere are different kinds of things, the first kind has $n_{1}$ the same, the second kind has $n_{2}$ the same, the $r$ kind has $n_{r}$ the same, there are a total of things, arrange this thing in a straight line. To get how many different lines, we get use the formula below.$\\tbinom{n_{1}+n_{2}+…+n_{r}}{n_{1},n_{2},..n_{r}}$= $\\frac{(n_{1}+n_{2}+…+n_{r})!}{n_{1}!n_{2}!…n_{r}!}$proof:http://www.math.nsysu.edu.tw/eprob/PerComb/perm2/index.html","link":"/2020/07/27/Permutation%20vs%20Combination/"},{"title":"bytes order","text":"1. byte orders Little-endian: byte ordering places the least significant byte first. This method is used in Intel microprocessors, for example. Big-endian byte ordering places the most significant byte first. This method is used in IBM® z/Architecture® and S/390® mainframes and Motorola microprocessors, for example. examples:a variable x with value 0x01234567 will be stored as following. 2. Network byte order vs Host byte orderNetwork Byte Order refers to how bytes are arranged when sending data over a network. In TCP/IP, this is generally Big Endian. This means most significant byte in the smallest address in a word. Host Byte Order refers to how bytes are arranged when referring to the computer architecture of a host computing platform. Due to pervasiveness of the Intel architecture, this is generally Little Endian This means least significant byte in the smallest address in a word 3. function used in C htonl() translates an unsigned long integer into network byte order. htons() translates an unsigned short integer into network byte order. ntohl() translates an unsigned long integer into host byte order. ntohs() translates an unsigned short integer into host byte order.","link":"/2020/08/26/bytes-order/"},{"title":"catalan number","text":"1. formula about catalan numberThe Catalan numbers $C_0,C_1, \\ldots, C_n$,are given by the formula$C_n = \\frac{1}{n+1}\\dbinom{2n}{n}$.The proof can be foundhttps://blog.csdn.net/stpeace/article/details/45938477 and https://brilliant.org/wiki/catalan-numbers/. And the Catalan numbers satisfy the recurrence relation$C_{n+1}=C_0C_n +C_1C_{n-1}+ \\ldots+ C_nC_0 $ 2.examples Leetcode 96. Unique Binary Search Trees In a BST, only the relative ordering between the elements matter. So, without any loss on generality, we can assume the distinct elements in the tree are 1, 2, 3, 4, …., n. Also, let the number of BST be represented by f(n) for n elements. Now we have the multiple cases for choosing the root. choose 1 as root, no element can be inserted on the left sub-tree. n-1 elements will be inserted on the right sub-tree.Choose 2 as root, 1 element can be inserted on the left sub-tree. n-2 elements can be inserted on the right sub-tree.Choose 3 as root, 2 element can be inserted on the left sub-tree. n-3 elements can be inserted on the right sub-tree.…… Similarly, for i-th element as the root, i-1 elements can be on the left and n-i on the right. These sub-trees are itself BST, thus, we can summarize the formula as: f(n) = f(0)f(n-1) + f(1)f(n-2) + .......... + f(n-1)f(0)The python code: 123class Solution: def numTrees(self, n: int) -&gt; int: return int((1/(n+1))* math.factorial(2*n)/math.factorial(n)/math.factorial(n))","link":"/2020/08/15/catalan-number/"},{"title":"common_modules_in_python(4):itertools","text":"Functions creating iterators for efficient looping.The module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python. Infinite iterators: itertools.count(start=0, step=1)Make an iterator that returns evenly spaced values starting with number start. Often used as an argument to map() to generate consecutive data points. Also, used with zip() to add sequence numbers. Roughly equivalent to:1234567def count(start=0, step=1): # count(10) --&gt; 10 11 12 13 14 ... # count(2.5, 0.5) -&gt; 2.5 3.0 3.5 ... n = start while True: yield n n += step Here is an example to use12345678import itertoolsnatuals = itertools.count(1)for n in natuals: print(n)#1#2#3 itertools.cycle(iterable)Make an iterator returning elements from the iterable and saving a copy of each. When the iterable is exhausted, return elements from the saved copy. Repeats indefinitely. Roughly equivalent to:123456789def cycle(iterable): # cycle('ABCD') --&gt; A B C D A B C D A B C D ... saved = [] for element in iterable: yield element saved.append(element) while saved: for element in saved: yield element Here is an example12345678910111213import itertoolscs = itertools.cycle('ABC')for c in cs: print(c)''''A''B''C''A''B''C'...''' itertools.repeat(object[, times])Make an iterator that returns object over and over again. Runs indefinitely unless the times argument is specified. Used as argument to map() for invariant parameters to the called function. Also used with zip() to create an invariant part of a tuple record. Roughly equivalent to: 12345678def repeat(object, times=None): # repeat(10, 3) --&gt; 10 10 10 if times is None: while True: yield object else: for i in range(times): yield object Here is an example 12345678ns = itertools.repeat('A', 3)for n in ns: print(n)# A# A# A#... Iterators terminating on the shortest input sequence: itertools.accumulate(iterable[, func, *, initial=None])Make an iterator that returns accumulated sums, or accumulated results of other binary functions (specified via the optional func argument). If func is supplied, it should be a function of two arguments. Elements of the input iterable may be any type that can be accepted as arguments to func. (For example, with the default operation of addition, elements may be any addable type including Decimal or Fraction.) Usually, the number of elements output matches the input iterable. However, if the keyword argument initial is provided, the accumulation leads off with the initial value so that the output has one more element than the input iterable. Roughly equivalent to: 12345678910111213141516def accumulate(iterable, func=operator.add, *, initial=None): 'Return running totals' # accumulate([1,2,3,4,5]) --&gt; 1 3 6 10 15 # accumulate([1,2,3,4,5], initial=100) --&gt; 100 101 103 106 110 115 # accumulate([1,2,3,4,5], operator.mul) --&gt; 1 2 6 24 120 it = iter(iterable) total = initial if initial is None: try: total = next(it) except StopIteration: return yield total for element in it: total = func(total, element) yield total Below are some examples 12345data = [3, 4, 6, 2, 1, 9, 0, 7, 5, 8]list(accumulate(data, operator.mul)) # running product[3, 12, 72, 144, 144, 1296, 0, 0, 0, 0]list(accumulate(data, max)) # running maximum[3, 4, 6, 6, 6, 9, 9, 9, 9, 9] itertools.chain(*iterables)Make an iterator that returns elements from the first iterable until it is exhausted, then proceeds to the next iterable, until all of the iterables are exhausted. Used for treating consecutive sequences as a single sequence. Roughly equivalent to:12345def chain(*iterables): # chain('ABC', 'DEF') --&gt; A B C D E F for it in iterables: for element in it: yield element classmethod chain.from_iterable(iterable) Alternate constructor for chain(). Gets chained inputs from a single iterable argument that is evaluated lazily. Roughly equivalent to: 12345def from_iterable(iterables): # chain.from_iterable(['ABC', 'DEF']) --&gt; A B C D E F for it in iterables: for element in it: yield element itertools.compress(data, selectors)Make an iterator that filters elements from data returning only those that have a corresponding element in selectors that evaluates to True. Stops when either the data or selectors iterables has been exhausted. Roughly equivalent to: 123def compress(data, selectors): # compress('ABCDEF', [1,0,1,0,1,1]) --&gt; A C E F return (d for d, s in zip(data, selectors) if s) itertools.dropwhile(predicate, iterable)Make an iterator that filters elements from iterable returning only those for which the predicate is False. If predicate is None, return the items that are false. Roughly equivalent to:1234567def filterfalse(predicate, iterable): # filterfalse(lambda x: x%2, range(10)) --&gt; 0 2 4 6 8 if predicate is None: predicate = bool for x in iterable: if not predicate(x): yield x groupby() picks out the adjacent repeating elements in the iterator and puts them together: itertools.groupby(iterable, key=None)groupby() picks out the adjacent repeating elements in the iterator and puts them together.groupby() is roughly equivalent to:1234567891011121314151617181920212223242526class groupby: # [k for k, g in groupby('AAAABBBCCDAABBB')] --&gt; A B C D A B # [list(g) for k, g in groupby('AAAABBBCCD')] --&gt; AAAA BBB CC D def __init__(self, iterable, key=None): if key is None: key = lambda x: x self.keyfunc = key self.it = iter(iterable) self.tgtkey = self.currkey = self.currvalue = object() def __iter__(self): return self def __next__(self): self.id = object() while self.currkey == self.tgtkey: self.currvalue = next(self.it) # Exit on StopIteration self.currkey = self.keyfunc(self.currvalue) self.tgtkey = self.currkey return (self.currkey, self._grouper(self.tgtkey, self.id)) def _grouper(self, tgtkey, id): while self.id is id and self.currkey == tgtkey: yield self.currvalue try: self.currvalue = next(self.it) except StopIteration: return self.currkey = self.keyfunc(self.currvalue) Here is an example :123456groups = []uniquekeys = []data = sorted(data, key=keyfunc)for k, g in groupby(data, keyfunc): groups.append(list(g)) # Store group iterator as a list uniquekeys.append(k) itertools.islice(iterable, stop)another parameters settings is itertools.islice(iterable, start, stop[, step])Make an iterator that returns selected elements from the iterable. If start is non-zero, then elements from the iterable are skipped until start is reached. Afterward, elements are returned consecutively unless step is set higher than one which results in items being skipped. If stop is None, then iteration continues until the iterator is exhausted, if at all; otherwise, it stops at the specified position. Unlike regular slicing, islice() does not support negative values for start, stop, or step. Can be used to extract related fields from data where the internal structure has been flattened (for example, a multi-line report may list a name field on every third line). Roughly equivalent to:123456789101112131415161718192021222324def islice(iterable, *args): # islice('ABCDEFG', 2) --&gt; A B # islice('ABCDEFG', 2, 4) --&gt; C D # islice('ABCDEFG', 2, None) --&gt; C D E F G # islice('ABCDEFG', 0, None, 2) --&gt; A C E G s = slice(*args) start, stop, step = s.start or 0, s.stop or sys.maxsize, s.step or 1 it = iter(range(start, stop, step)) try: nexti = next(it) except StopIteration: # Consume *iterable* up to the *start* position. for i, element in zip(range(start), iterable): pass return try: for i, element in enumerate(iterable): if i == nexti: yield element nexti = next(it) except StopIteration: # Consume to *stop*. for i, element in zip(range(i + 1, stop), iterable): pass If start is None, then iteration starts at zero. If step is None, then the step defaults to one. itertools.starmap(function, iterable)Make an iterator that computes the function using arguments obtained from the iterable. Used instead of map() when argument parameters are already grouped in tuples from a single iterable (the data has been “pre-zipped”). The difference between map() and starmap() parallels the distinction between function(a,b) and function(*c). Roughly equivalent to:1234def starmap(function, iterable): # starmap(pow, [(2,5), (3,2), (10,3)]) --&gt; 32 9 1000 for args in iterable: yield function(*args) itertools.takewhile(predicate, iterable)Make an iterator that returns elements from the iterable as long as the predicate is true. Roughly equivalent to:1234567def takewhile(predicate, iterable): # takewhile(lambda x: x&lt;5, [1,4,6,4,1]) --&gt; 1 4 for x in iterable: if predicate(x): yield x else: break itertools.tee(iterable, n=2)Return n independent iterators from a single iterable. The following Python code helps explain what tee does (although the actual implementation is more complex and uses only a single underlying FIFO queue). Roughly equivalent to: 1234567891011121314def tee(iterable, n=2): it = iter(iterable) deques = [collections.deque() for i in range(n)] def gen(mydeque): while True: if not mydeque: # when the local deque is empty try: newval = next(it) # fetch a new value and except StopIteration: return for d in deques: # load it to all the deques d.append(newval) yield mydeque.popleft() return tuple(gen(d) for d in deques) itertools.zip_longest(*iterables, fillvalue=None)Make an iterator that aggregates elements from each of the iterables. If the iterables are of uneven length, missing values are filled-in with fillvalue. Iteration continues until the longest iterable is exhausted. Roughly equivalent to: 12345678910111213141516171819def zip_longest(*args, fillvalue=None): # zip_longest('ABCD', 'xy', fillvalue='-') --&gt; Ax By C- D- iterators = [iter(it) for it in args] num_active = len(iterators) if not num_active: return while True: values = [] for i, it in enumerate(iterators): try: value = next(it) except StopIteration: num_active -= 1 if not num_active: return iterators[i] = repeat(fillvalue) value = fillvalue values.append(value) yield tuple(values) itertools.product(*iterables, repeat=1)Cartesian product of input iterables. Roughly equivalent to nested for-loops in a generator expression. For example, product(A, B) returns the same as ((x,y) for x in A for y in B). The nested loops cycle like an odometer with the rightmost element advancing on every iteration. This pattern creates a lexicographic ordering so that if the input’s iterables are sorted, the product tuples are emitted in sorted order. To compute the product of an iterable with itself, specify the number of repetitions with the optional repeat keyword argument. For example, product(A, repeat=4) means the same as product(A, A, A, A). This function is roughly equivalent to the following code, except that the actual implementation does not build up intermediate results in memory: 123456789def product(*args, repeat=1): # product('ABCD', 'xy') --&gt; Ax Ay Bx By Cx Cy Dx Dy # product(range(2), repeat=3) --&gt; 000 001 010 011 100 101 110 111 pools = [tuple(pool) for pool in args] * repeat result = [[]] for pool in pools: result = [x+[y] for x in result for y in pool] for prod in result: yield tuple(prod) itertools.permutations(iterable, r=None)Return successive r length permutations of elements in the iterable. If r is not specified or is None, then r defaults to the length of the iterable and all possible full-length permutations are generated. The permutation tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each permutation. Roughly equivalent to: 123456789101112131415161718192021222324def permutations(iterable, r=None): # permutations('ABCD', 2) --&gt; AB AC AD BA BC BD CA CB CD DA DB DC # permutations(range(3)) --&gt; 012 021 102 120 201 210 pool = tuple(iterable) n = len(pool) r = n if r is None else r if r &gt; n: return indices = list(range(n)) cycles = list(range(n, n-r, -1)) yield tuple(pool[i] for i in indices[:r]) while n: for i in reversed(range(r)): cycles[i] -= 1 if cycles[i] == 0: indices[i:] = indices[i+1:] + indices[i:i+1] cycles[i] = n - i else: j = cycles[i] indices[i], indices[-j] = indices[-j], indices[i] yield tuple(pool[i] for i in indices[:r]) break else: return itertools.combinations(iterable, r)Return r length subsequences of elements from the input iterable. The combination tuples are emitted in lexicographic ordering according to the order of the input iterable. So, if the input iterable is sorted, the combination tuples will be produced in sorted order. Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination. Roughly equivalent to: 12345678910111213141516171819def combinations(iterable, r): # combinations('ABCD', 2) --&gt; AB AC AD BC BD CD # combinations(range(4), 3) --&gt; 012 013 023 123 pool = tuple(iterable) n = len(pool) if r &gt; n: return indices = list(range(r)) yield tuple(pool[i] for i in indices) while True: for i in reversed(range(r)): if indices[i] != i + n - r: break else: return indices[i] += 1 for j in range(i+1, r): indices[j] = indices[j-1] + 1 yield tuple(pool[i] for i in indices) referencehttps://docs.python.org/3/library/itertools.html#itertools.combinations_with_replacement","link":"/2020/07/06/common-modules-in-python-4/"},{"title":"bit manipulation in python","text":"Basic of bitwise Operations not (~)If the number of not is an unsigned integer (it cannot represent a negative number), the result is the difference between it and the upper bound of the type12~1 -&gt; 0~0 -&gt; 1 and （&amp;）The and operation is usually used for get a certain bit. For example, the result of x &amp; 1 is to get the last bit of x. This can be used to judge the parity of an integer. The last bit of a binary is 0, which means the number is even, and the last bit is 1, which means the number is odd 1231 &amp; 1 -&gt; 10 &amp; 0 -&gt; 00 &amp; 1 -&gt; 0 or (|)Or operation is usually used for assignment on binary special bit. For example, the result of x |1 is to make the last bit of x to 1. If you need to change the last bit of binary number to 0, you can use x &amp; 1 -1. Its practical meaning is to force the number to the nearest even number1231 | 1 -&gt; 10 | 0 -&gt; 01 | 0 -&gt; 1 xor (^)if we xor the same number twice,we will get the number itself1231 ^ 1 -&gt; 00 ^ 0 -&gt; 01 ^ 0 -&gt; 1 right shift(&gt;&gt;):Right shift is equivalent to dividing the bit pattern with 2k ( if we are shifting k bits ).the least-significant bit is lost, and a 0 bit is inserted on the other end(if the number has no sign bit) 12a = 8: 0000 0000 0000 0000 0000 0000 0000 1000a &gt;&gt; 3: 0000 0000 0000 0000 0000 0000 0000 0001 left shift(&lt;&lt;): Right shift is equivalent to multipling the bit pattern with 2k ( if we are shifting k bits ).the most-significant bit is lost, and a 0 bit is inserted on the other end(if the number has no sign bit) 12a = 8： 0000 0000 0000 0000 0000 0000 0000 1000a &lt;&lt; 3：0000 0000 0000 0000 0000 0000 0100 0000 the order of bitwise operation in Python operator precedence（from highest to lowest) ~x x&lt;&gt;y x&amp;y x^y x|y Original code vs Inverse code vs Complement code original code: sign bit &amp; real value 123 [+1]ori = 0000 0001[-1]ori = 1000 0001 Inverse code The inverse of a positive number is itselfThe inverse code of a negative number is based on the original code, the sign bit remains unchanged, and the remaining bits are reversed. 123 [+1] = [00000001]ori = [00000001]inv[-1] = [10000001]ori = [11111110]inv complement code (Two’s complement ) The complement of a positive number is itself.The complement of a negative number is based on the original code, the sign bit remains unchanged, the rest of the bits are fliped, and finally +1. (That is, +1 on the basis of the inverse code) 123 [+1] = [00000001]ori = [00000001]inv = [00000001]com[-1] = [10000001]ori = [11111110]inv = [11111111]com why will we use inverse code and complement code Because we can not directly add two numbers by using original code, one of which is negative and the other is positive. Like the example below which gives us the wrong answer. 11 - 1 = 1 + (-1) = [00000001]ori + [10000001]ori = [10000010]ori = -2 (wrong) Then we turn to inverse code 11 - 1 = 1 + (-1) = [0000 0001]ori + [1000 0001]ori= [0000 0001]inv + [1111 1110]inv = [1111 1111]inv = [1000 0000]ori = -0 It will cause a new problem that there will be two codes [0000 0000] original and [1000 0000] original to indicate 0. So we will use complement code 11-1 = 1 + (-1) = [0000 0001]ori + [1000 0001]ori = [0000 0001]com + [1111 1111]com = [0000 0000]com=[0000 0000]ori = 0 an note for -128. we set its complememnt code is [10000000] And it didnot have any orginal code and inverse code because it actually use the complement of -0. This is why 8-bit binary, the range represented by the original code or the complement code is [-127, +127], and the range represented by the complement code is [-128, 127]. Because the machine uses complement code, for the 32-bit int type commonly used in programming, the range that can be expressed is: [-2^31, 2^31-1] Because the first bit represents the sign bit. When using the complement, it can save a minimum value -0. common problems about bitwise operations: x &amp; (x - 1) used to erase the last bit of Xto understand x &amp; (x - 1), we should understand x-1 first.it means for x, set 0 to the position where the first 1 appears from the right to the left; set 1 to the all 0 after that position. application: Using O (1) time to detect whether the integer n is the power of 2that is to say, if n is the power of 2. It must greateror equal than 1 and it only have one bit is 1.1234567def checkPowerOfTwo(n): if n &lt;= 0: return False if n &amp; (n-1) == 0: return True else: return False to find how many bits are 1 for a number123456def checkHowManyOne(n): num = 0 while n: n = n&amp;(n-1) num+=1 return num compute the hamming distance of a and b. (how many bits we need to change from a to b) 123456789101112def hammingDistance(x, y): \"\"\" :type x: int :type y: int :rtype: int \"\"\" xor = x ^ y num = 0 while xor: xor = xor &amp; (xor -1) num += 1 return num to check whether a bit of number is 1 12# set the number a, and the index of the bit we intend to check is ba &amp; 1&lt;&lt; b (the execution priority of &lt;&lt; is higher ) application: Given a set of distinct integers, nums, return all possible subsets (the power set). （the question can also use backtracking too)123456789101112class Solution: def subsets(self, nums): res = [] for i in range(1&lt;&lt;len(nums)): tmp = [] # a certain bit is 1 or not for j in range(len(nums)): if i &amp; (1 &lt;&lt; j): tmp.append(nums[j]) res.append(tmp) return res a ^ b ^ b = a 123456to prove it: a ^ b b -&gt; a 0 (same) 0 0 0 (same) 1 1 1（diff) 0 1 1（diff) 1 0 application: Can be used to encrypt In the array, only one number appears once, and the rest appear twice, find the number that appears once. Because there is only one number appears once while others appear twice which can elminate by XOR 123456def findAppearOnce(nums): n = len(nums) ret = nums[0] for i in range(1,n): ret = ret ^ nums[i] return ret Given an array where every element occurs three times, except one element which occurs only once. Find the element that occurs once. Expected time complexity is O(n) and O(1) extra space. idea: There are two numbers ones and twos.ones: The bits that have appeared 1st time or 4th time or 7th time .. etc.twos: The bits that have appeared 2nd time or 5th time or 8th time .. etc.If a bit is already in ones, add it to twos.XOR will add this bit to ones if it’s not there or remove this bit from ones if it’s already there.If a bit is in both ones and twos, remove it from ones and twos.When finished, ones contains the bits that only appeared 3*n+1 times, which are the bits for the element that only appeared once.12345678910111213141516171819202122232425262728293031323334 def getSingle(arr, n): ones = 0 twos = 0 for i in range(n): # one &amp; arr[i]\" gives the bits that # are there in both 'ones' and new # element from arr[]. We add these # bits to 'twos' using bitwise OR twos = twos | (ones &amp; arr[i]) # XOR will add this bit to ones #if it's not there or remove this bit # from ones if it's already there ones = ones ^ arr[i] # The common bits are those bits # which appear third time. So these # bits should not be there in both # 'ones' and 'twos'. common_bit_mask # contains all these bits as 0, so # that the bits can be removed from # 'ones' and 'twos' common_bit_mask = ~(ones &amp; twos) # Remove common bits (the bits that # appear third time) from 'ones' ones &amp;= common_bit_mask # Remove common bits (the bits that # appear third time) from 'twos' twos &amp;= common_bit_mask return ones #https://www.geeksforgeeks.org/find-the-element-that-appears-once/ to check the lowest set bit x &amp; (-x) since the computer store the two’s complement so that the and operator will be implemented by two’s complement. 12345678 &amp; (-8) = 87 &amp; (-7) = 197 &amp; (-97) = 10000 1000 &amp; (1111 1000) = 0000 10000000 0111 &amp; (1111 1001) = 0000 00010110 0001 &amp; (1001 1111) = 0000 0001 x- x&amp;(-x) is to move the lowest set bit and we will use the equation y = x- x&amp;(-x) in the binary indexed tree common operations Function example bit operation Remove the last digit (101101 - &gt; 10110) x &gt;&gt; 1 Add a 0 (101101 - &gt; 1011010) x &lt;&lt; 1 Add a 1 (101101 - &gt; 1011011) x &lt;&lt; 1 + 1 at the end Change the last bit to 1 (101100 - &gt; 101101) x | 1 Change the last bit to 0 (101101 - &gt; 101100) x | 1-1 The last bit is inverted (101101 - &gt; 101100) x ^ 1 Change the k-th bit from right to 1 (101001 - &gt; 101101, k = 3) x | (1 &lt;&lt; (k-1)) Change the k-th bit of the right number to 0 (101101 - &gt; 101001, k = 3) X &amp; ~(1 &lt;&lt; (k-1)) Negate The k-th digit of number from right (101001 - &gt; 101101, k = 3) x ^ (1 &gt;&gt; (k-1)) only keep the last three digits (1101101 - &gt; 101) x &amp; 7 keep the last K digits (1101101 - &gt; 1101, k = 5) x &amp; ( (1 &gt;&gt; k)-1) Take the k-th digit (1101101 - &gt; 1, k = 4) x &gt;&gt; (k-1) &amp; 1 Change the kth bit to last to 1 (101001 - &gt; 101111, k = 4) x | (1 &gt;&gt; k-1) negate the kth bit to last (101001 - &gt; 100110, k = 4) x ^ (1 &lt;&lt; k-1) Change the continuous 1 on the right side to 0 (100101111 - &gt; 100100000) x &amp; (x + 1) Change the first 0 from the right to 1 (100101111 - &gt; 100111111) x | (x + 1) Change the continuous 0 on the right into 1 (11011000 - &gt; 11011111) x or (x-1) Take the continuous 1 (100101111 - &gt; 1111) (x ^ (x + 1))&gt;&gt; 1 Remove the left side of the last 1 to the last (100101000 - &gt; 1000) x &amp; (x ^ (x-1)) referenceshttps://www.zhihu.com/question/38206659https://blog.csdn.net/zl10086111/article/details/80907428","link":"/2020/05/27/bit-manipulation-in-python/"},{"title":"common modules in Python(1) :collections","text":"This is a summary based on https://www.geeksforgeeks.org/ and https://docs.python.org/3/library/collections.html1.ChainMap(*maps) def:A ChainMap class is provided for quickly linking a number of mappings so they can be treated as a single unit. It is often much faster than creating a new dictionary and running multiple update() calls.A ChainMap groups multiple dicts or other mappings together to create a single, updateable view. If no maps are specified, a single empty dictionary is provided so that a new chain always has at least one mapping.1234&gt;&gt;&gt; baseline = {'music': 'bach', 'art': 'rembrandt'}&gt;&gt;&gt; adjustments = {'art': 'van gogh', 'opera': 'carmen'}&gt;&gt;&gt; list(ChainMap(adjustments, baseline))['music', 'art', 'opera'] methods and attributes: mapsThis function is used to display keys with corresponding values of all the dictionaries in ChainMap.1234567891011121314151617181920212223242526import collections# initializing dictionariesdic1 = { 'a' : 1, 'b' : 2 }dic2 = { 'b' : 3, 'c' : 4 }# initializing ChainMapchain = collections.ChainMap(dic1, dic2)# printing chainMap using mapsprint (\"All the ChainMap contents are : \")print (chain.maps)# All the ChainMap contents are :# [{'b': 2, 'a': 1}, {'c': 4, 'b': 3}]# printing keys using keys()print (\"All keys of ChainMap are : \")print (list(chain.keys()))#All keys of ChainMap are :# ['a', 'c', 'b']# printing keys using keys()print (\"All values of ChainMap are : \")print (list(chain.values()))#All values of ChainMap are :#[1, 4, 2] Notice： the key named “b” exists in both dictionaries, but only first dictionary key is taken as key value of “b”. Ordering is done as the dictionaries are passed in function. new_child() This function adds a new dictionary in the beginning of the ChainMap. 123456789101112131415161718192021222324import collections# initializing dictionariesdic1 = { 'a' : 1, 'b' : 2 }dic2 = { 'b' : 3, 'c' : 4 }dic3 = { 'f' : 5 }# initializing ChainMapchain = collections.ChainMap(dic1, dic2)# printing chainMap using mapprint (\"All the ChainMap contents are : \")print (chain.maps)#All the ChainMap contents are :# [{'b': 2, 'a': 1}, {'b': 3, 'c': 4}]# using new_child() to add new dictionarychain1 = chain.new_child(dic3)# printing chainMap using mapprint (\"Displaying new ChainMap : \")print (chain1.maps)#Displaying new ChainMap :# [{'f': 5}, {'b': 2, 'a': 1}, {'b': 3, 'c': 4}] reversed()This function reverses the relative ordering of dictionaries in the ChainMap12345678910111213# all data are previous example# displaying value associated with b before reversingprint (\"Value associated with b before reversing is : \",end=\"\")print (chain1['b'])#Value associated with b before reversing is : 2# reversing the ChainMapchain1.maps = reversed(chain1.maps)# displaying value associated with b after reversingprint (\"Value associated with b after reversing is : \",end=\"\")print (chain1['b'])#Value associated with b after reversing is : 3 parentsProperty returning a new ChainMap containing all of the maps in the current instance except the first one. This is useful for skipping the first map in the search. Use cases are similar to those for the nonlocal keyword used in nested scopes. The use cases also parallel those for the built-in super() function. A reference to d.parents is equivalent to: ChainMap(*d.maps[1:]). 2.counter() def: dict subclass for counting hashable objects Elements are counted from an iterable or initialized from another mapping1234&gt;&gt;&gt; c = Counter() # a new, empty counter&gt;&gt;&gt; c = Counter('gallahad') # a new counter from an iterable&gt;&gt;&gt; c = Counter({'red': 4, 'blue': 2}) # a new counter from a mapping&gt;&gt;&gt; c = Counter(cats=4, dogs=8) # a new counter from keyword args Counter objects have a dictionary interface except that they return a zero count for missing items instead of raising a KeyError:123&gt;&gt;&gt; c = Counter(['eggs', 'ham'])&gt;&gt;&gt; c['bacon'] # count of a missing element is zero0 method: elements()Return an iterator over elements repeating each as many times as its count. Elements are returned in arbitrary order. If an element’s count is less than one, elements() will ignore it.123c = Counter(a=4, b=2, c=0, d=-2)sorted(c.elements())['a', 'a', 'a', 'a', 'b', 'b'] most_common([n])eturn a list of the n most common elements and their counts from the most common to the least. If n is omitted or None, most_common() returns all elements in the counter. Elements with equal counts are ordered in the order first encountered:12Counter('abracadabra').most_common(3)[('a', 5), ('b', 2), ('r', 2)] subtract([iterable-or-mapping])Elements are subtracted from an iterable or from another mapping (or counter). Like dict.update() but subtracts counts instead of replacing them. Both inputs and outputs may be zero or negative.12345c = Counter(a=4, b=2, c=0, d=-2)d = Counter(a=1, b=2, c=3, d=4)c.subtract(d)cCounter({'a': 3, 'b': 0, 'c': -3, 'd': -6}) other methods12345678910111213141516171819sum(c.values()) # total of all countsc.clear() # reset all countslist(c) # list unique elementsset(c) # convert to a setdict(c) # convert to a regular dictionaryc.items() # convert to a list of (elem, cnt) pairsCounter(dict(list_of_pairs)) # convert from a list of (elem, cnt) pairsc.most_common()[:-n-1:-1] # n least common elements+c # remove zero and negative countsc = Counter(a=3, b=1)d = Counter(a=1, b=2)c + d # add two counters together: c[x] + d[x]Counter({'a': 4, 'b': 3})c - d # subtract (keeping only positive counts)Counter({'a': 2})c &amp; d # intersection: min(c[x], d[x])Counter({'a': 1, 'b': 1})c | d # union: max(c[x], d[x])Counter({'a': 3, 'b': 2}) 3. deque([iterable[, maxlen]]) def: Deques are a generalization of stacks and queues (the name is pronounced “deck” and is short for “double-ended queue”). Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the same O(1) performance in either direction. methods: append(x)Add x to the right side of the deque. appendleft(x)Add x to the left side of the deque. clear()Remove all elements from the deque leaving it with length 0. copy()Create a shallow copy of the deque. count(x)Count the number of deque elements equal to x. extend(iterable)Extend the right side of the deque by appending elements from the iterable argument. extendleft(iterable)Extend the left side of the deque by appending elements from iterable. Note, the series of left appends results in reversing the order of elements in the iterable argument. index(x[, start[, stop]])Return the position of x in the deque (at or after index start and before index stop). Returns the first match or raises ValueError if not found. insert(i, x)Insert x into the deque at position i.If the insertion would cause a bounded deque to grow beyond maxlen, an IndexError is raised. pop()Remove and return an element from the right side of the deque. If no elements are present, raises an IndexError. popleft()Remove and return an element from the left side of the deque. If no elements are present, raises an IndexError. remove(value)Remove the first occurrence of value. If not found, raises a ValueError. reverse()Reverse the elements of the deque in-place and then return None. rotate(n=1)Rotate the deque n steps to the right. If n is negative, rotate to the left.When the deque is not empty, rotating one step to the right is equivalent to d.appendleft(d.pop()), and rotating one step to the left is equivalent to d.append(d.popleft()). maxlenMaximum size of a deque or None if unbounded. examples:12345#Bounded length deques provide functionality similar to the tail filter in Unix:def tail(filename, n=10): 'Return the last n lines of a file' with open(filename) as f: return deque(f, n) 4. defaultdict([default_factory[, ...]]) ref: Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. . The functionality of both dictionaries and defualtdict are almost same except for the fact that defualtdict never raises a KeyError. It provides a default value for the key that does not exists. methods defaultdict(default_factory)default_factory: A function returning the default value for the dictionary defined. If this argument is absent then the dictionary raises a KeyError. __missing__(): This function is used to provide the default value for the dictionary. This function takes default_factory as an argument and if this argument is None, a KeyError is raised otherwise it provides a default value for the given key. This method is basically called by the __getitem__() method of the dict class when the requested key is not found.__getitem__() raises or return the value returned by the __missing__(). method. 12345678910111213141516# Python program to demonstrate# defaultdictfrom collections import defaultdict# Defining the dictd = defaultdict(lambda: \"Not Present\")d[\"a\"] = 1d[\"b\"] = 2# Provides the default value # for the keyprint(d.__missing__('a'))print(d.__missing__('d')) The usage can be seen as follows: 123456789101112131415161718192021``` __5.`namedtuple()`__ - def: Named tuples assign meaning to each position in a tuple and allow for more readable, self-documenting code. They can be used wherever regular tuples are used, and they add the ability to access fields by name instead of position index. ```Python &gt;&gt;&gt; # Basic example &gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y']) &gt;&gt;&gt; p = Point(11, y=22) # instantiate with positional or keyword arguments &gt;&gt;&gt; p[0] + p[1] # indexable like the plain tuple (11, 22) 33 &gt;&gt;&gt; x, y = p # unpack like a regular tuple &gt;&gt;&gt; x, y (11, 22) &gt;&gt;&gt; p.x + p.y # fields also accessible by name 33 &gt;&gt;&gt; p # readable __repr__ with a name=value style # set default value for namedtuple Node = namedtuple('Node', 'val left right') Node.__new__.func_defaults = (None,) * len(Node._fields) Operations on namedtuple() : Access Operations 1. __Access by index__ : The attribute values of namedtuple() are ordered and can be accessed using the index number unlike dictionaries which are not accessible by index. 2. __Access by keyname__ : Access by keyname is also allowed as in dictionaries. 3. __using getattr()__ :- This is yet another way to access the value by giving namedtuple and key value as its argument. methods: _makeClass method that makes a new instance from an existing sequence or iterable.123#Basic examplePoint = namedtuple('Point', ['x', 'y'])p = Point(11, y=22) # instantiate with positional or keyword arguments _asdict()Return a new dict which maps field names to their corresponding values123p = Point(x=11, y=22)p._asdict()# {'x': 11, 'y': 22} _replace(**kwargs)Return a new instance of the named tuple replacing specified fields with new values:123p = Point(x=11, y=22)p._replace(x=33)# Point(x=33, y=22) _fields 12p._fields # view the field names# ('x', 'y') 7.OrderedDict() def: Ordered dictionaries are just like regular dictionaries but have some extra capabilities relating to ordering operations. They have become less important now that the built-in dict class gained the ability to remember insertion order methods: popitem(last=True)the popitem() method for ordered dictionaries returns and removes a (key, value) pair. The pairs are returned in LIFO order if last is true or FIFO order if false. move_to_end(key, last=True)Move an existing key to either end of an ordered dictionary. The item is moved to the right end if last is true (the default) or to the beginning if last is false. Raises KeyError if the key does not exist:1234567d = OrderedDict.fromkeys('abcde')d.move_to_end('b')''.join(d.keys())# 'acdeb'd.move_to_end('b', last=False)''.join(d.keys())# 'bacde' 8.UserDict([initialdata]) def: Python supports a dictionary like a container called UserDict present in the collections module. This class acts as a wrapper class around the dictionary objects. This class is useful when one wants to create a dictionary of their own with some modified functionality or with some new functionality. It can be considered as a way of adding new behaviors for the dictionary. This class takes a dictionary instance as an argument and simulates a dictionary that is kept in a regular dictionary. The dictionary is accessible by the data attribute of this class. example:Let’s create a class inherting from UserDict to implement a customised dictionary.12345678910111213141516171819202122232425262728293031from collections import UserDict # Creating a Dictionary where# deletion is not allowedclass MyDict(UserDict): # Function to stop deleltion # from dictionary def __del__(self): raise RuntimeError(\"Deletion not allowed\") # Function to stop pop from # dictionary def pop(self, s = None): raise RuntimeError(\"Deletion not allowed\") # Function to stop popitem # from Dictionary def popitem(self, s = None): raise RuntimeError(\"Deletion not allowed\")# Driver's coded = MyDict({'a':1, 'b': 2, 'c': 3})print(\"Original Dictionary\")print(d)d.pop(1) 9.UserList([list]) def:This class acts as a wrapper around list objects. It is a useful base class for your own list-like classes which can inherit from them and override existing methods or add new ones. In this way, one can add new behaviors to lists.The need for this class has been partially supplanted by the ability to subclass directly from list; however, this class can be easier to work with because the underlying list is accessible as an attribute. 10.UserString(seq) def: Class that simulates a string object. The instance’s content is kept in a regular string object, which is accessible via the data attribute of UserString instances. The instance’s contents are initially set to a copy of seq. The seq argument can be any object which can be converted into a string using the built-in str() function.","link":"/2020/06/26/common-modules-in-Python-1-collections/"},{"title":"common_modules_in_python(3):heapq","text":"descriptionThis module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm. To understood the heap sort, we could see the passage sorting alogrithms in python siftup siftdown function function description heappush (heap, x) pushes x into the heap heappop(heap) pops the smallest element out of the heap heapify(heap) makes the list like a heap heapreplace(heap, x) pops up the smallest element and pushes x into the heap heappushpop(heap, x) Push x on the heap, then pop and return the smallest item from the heap. The combined action runs more efficiently than heappush() followed by a separate call to heappop(). nlargest(n, iter) returns the largest n elements in iter nsmallest(n, iter) returns the smallest n elements in iter _heappop_max(heap) Maxheap version of a heappop _heapreplace_max(heap,item) Maxheap version of a heappop followed by a heappush. _heapify_max(x) Transform list into a maxheap, in-place, in O(len(x)) time _siftdown(heap,startpos,pos) Follow the path to the root, moving parents down until finding a place _siftup(heap,pos) Bubble up the smaller child until hitting a leaf _siftdown_max(heap,startpos,pos) Maxheap variant of _siftdown _siftup_max(heap,pos) Maxheap variant of _siftup difference between heappushpop and heapreplaceheapreplace(a, x) returns the smallest value originally in a regardless of the value of x, while, as the name suggests, heappushpop(a, x) pushes x onto a before popping the smallest value. Using your data, here’s a sequence that shows the difference: an exmapleLeetcode 23. Merge k Sorted Lists 12345678910111213141516171819 # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = nextimport heapqclass Solution: def mergeKLists(self, lists: List[ListNode]) -&gt; ListNode: nodeLists = [] for node in lists: while node: heapq.heappush(nodeLists, node.val) node = node.next res = curr = ListNode(None) while nodeLists: curr.next = ListNode(heapq.heappop(nodeLists)) curr = curr.next return res.next","link":"/2020/07/06/common-modules-in-python-3/"},{"title":"common_modules_in_python(5):math","text":"Constants description math.pi the ratio of a circle’s circumference (c) to its diameter math.e Euler’s number (e) is a constant that is the base of the natural logarithm math.inf Infinity(Positive) math.nan Not a number, or NaN Arithmetic Functions description example math.factorial() get Factorial math.factorial(4) #24(1*2*3*4) math.ceil()Find the Ceiling Value Find the Ceiling Value math.ceil(-11.453)#11 math.ceil(4.23)#5 math.floor() floor() will return the closest integer value that is less than or equal to the given number. math.floor(5.532) #5 math.trunc() When you get a number with a decimal point, you might want to keep only the integer part and eliminate the decimal part. math.trunc(-43.24) #-43 math.isclose() determine whether two numbers are close to each other math.isclose(6.999999999, 7) #True math.pow() Calculate the Power of a Number math.pow(2, 5) # 32.0 pow(5, 2.4) #47.59134846789696 (32 ** 6) % 5 == pow(32, 6, 5) #True exp() Find the Natural Exponent math.exp(21) #1318815734.4832146 log() get natural log value(whose base is e) math.log(4) #1.3862943611198906 log2() get the log value whose base is 2 log10() get the log value whose base is 10 math.gcd() compute the greatest common divisor math.gcd(25,15) #5 math.fsum() o find the sum of the values of an iterable without using a loop. A built-in function called sum() lets you calculate the sum of iterables as well, but fsum() is more accurate than sum(). sum([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1]) #0.9999999999999999 fsum([.1, .1, .1, .1, .1, .1, .1, .1, .1, .1]) #1.0 math.sqrt() Calculate the Square Root sqrt(4) #2 math.radians() convert degrees to radians math.degrees() convert radians to degrees math.sin() Return the sine of x radians. math.tan(x) Return the tangent of x radians. math.cos(x) Return the cosine of x radians. math.hypot() calculate the hypotenuse of a triangle math.comb(n, k) returns the number of ways to choose k items from n items without repetition and without particular order. math.perm(n, k) returns the number of ways to choose k items from n items without repetition and with order. math.isqrt() returns the integer square root of a non-negative integer. math.prod() calculates the product of all of the elements in the input iterable. As with fsum(), this method can take iterables such as arrays, lists, or tuples. math.dist() returns the Euclidean distance between two points p and q, each given as a sequence (or iterable) of coordinates. The two points must have the same dimension. math.hypot() now handles more than two dimensions. Previously, it supported a maximum of two dimensions. math.isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)rel_tol: maximum difference for being considered “close”, relative to the magnitude of the input valuesabs_tol: rel_tol is the relative tolerance – it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass rel_tol=0.05. The default tolerance is 1e-09, which assures that the two values are the same within about 9 decimal digits. rel_tol must be greater than zero.The IEEE 754 special values of NaN, inf, and -inf will be handled according to IEEE rules. Specifically, NaN is not considered close to any other value, including NaN. inf and -inf are only considered close to themselves. math.floor/math.ceil/int()/math.trunc/round()1234567891011121314151617181920212223a = 5.3b = - 5.3import mathprint(math.floor(a))print(math.floor(b))print(math.ceil(a))print(math.ceil(b))print(math.trunc(a))print(math.trunc(b))print(int(a))print(int(b))print(round(a))print(round(b))# 5# -6# 6# -5# 5# -5# 5# -5# 5# -5 reference:https://realpython.com/python-math-module/https://docs.python.org/3/library/math.html","link":"/2020/07/12/common-modules-in-python-5-math/"},{"title":"common_modules_in_python(2):functools","text":"Higher-order functions and operations on callable objects @functools.cached_property(func)Transform a method of a class into a property whose value is computed once and then cached as a normal attribute for the life of the instance. Similar to property(), with the addition of caching. Useful for expensive computed properties of instances that are otherwise effectively immutable.1234567891011class DataSet: def __init__(self, sequence_of_numbers): self._data = sequence_of_numbers @cached_property def stdev(self): return statistics.stdev(self._data) @cached_property def variance(self): return statistics.variance(self._data) @functools.lru_cache(maxsize=128, typed=False)@functools.lru_cache(user_function)Decorator to wrap a function with a memoizing callable that saves up to the maxsize most recent calls. It can save time when an expensive or I/O bound function is periodically called with the same arguments.lru is least recently used, lru_cache can record the result of the function call, and directly use the previous return value when it is used again, without really calling it again. Distinct argument patterns may be considered to be distinct calls with separate cache entries. For example, f(a=1, b=2) and f(b=2, a=1) differ in their keyword argument order and may have two separate cache entries. If typed is set to true, function arguments of different types will be cached separately. For example, f(3) and f(3.0) will be treated as distinct calls with distinct results. If maxsize is set to None, the LRU feature is disabled and the cache can grow without bound. 1234567891011121314151617181920212223242526272829303132333435363738394041424344#Calling recursion multiple times can be slowdef fibonacci(n): if n==1: return 1 elif n==2: return 1 elif n&gt;2: return fibonacci(n-1)+fibonacci(n-2)for i in range(1,101): print(i,\":\",fibonacci(i))#solution1：# fibonacci_dic={}def fibonacci(n): if n in fibonacci_dic: return fibonacci_dic[n] if n==1: value= 1 elif n==2: value= 1 elif n&gt;2: value=fibonacci(n-1)+fibonacci(n-2) fibonacci_dic[n]=value return valuefor i in range(1,101): print(i,\":\",fibonacci(i))# solution2:Decoratorfrom functools import lru_cache@lru_cache(maxsize=1000)def fibonacci(n): if n==1: return 1 elif n==2: return 1 elif n&gt;2: return fibonacci(n-1)+fibonacci(n-2)for i in range(1,101): print(i,\":\",fibonacci(i)) how to realize the lru_cache @functools.total_orderingGiven a class defining one or more rich comparison ordering methods, this class decorator supplies the rest. This simplifies the effort involved in specifying all of the possible rich comparison operations: The class must define one of __lt__(),__le__(), __gt__(), or __ge__(). In addition, the class should supply an __eq__() method.123456789101112131415@total_orderingclass Student: def _is_valid_operand(self, other): return (hasattr(other, \"lastname\") and hasattr(other, \"firstname\")) def __eq__(self, other): if not self._is_valid_operand(other): return NotImplemented return ((self.lastname.lower(), self.firstname.lower()) == (other.lastname.lower(), other.firstname.lower())) def __lt__(self, other): if not self._is_valid_operand(other): return NotImplemented return ((self.lastname.lower(), self.firstname.lower()) &lt; (other.lastname.lower(), other.firstname.lower())) functools.partial(func, /, *args, **keywords) When we need to call a function frequently, but some of the parameters are known fixed values, which may make the code appear redundant. At this time, you can consider using partial function. 123456789int('1000000', base=2)# 64int('1010101', base=2)# 85# to simplifyfrom functools import partialint2 = partial(int, base=2)int2('110')# 6 functools.partialmethod(func, /, *args, **keywords)Return a new partialmethod descriptor which behaves like partial except that it is designed to be used as a method definition rather than being directly callable.Partial will throw error when used in class methods while partialmethod will not.1234567891011121314151617class Cell(object): def __init__(self): self._alive = False @property def alive(self): return self._alive def set_state(self, state): self._alive = bool(state) set_alive = partialmethod(set_state, True) set_dead = partialmethod(set_state, False)c = Cell()c.alive # Falsec.set_alive()c.alive # True functools.reduce(function, iterable[, initializer])Apply function of two arguments cumulatively to the items of iterable, from left to right, so as to reduce the iterable to a single value. For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates ((((1+2)+3)+4)+5).The left argument, x, is the accumulated value and the right argument, y, is the update value from the iterable functools.singledispatchSimilar to java’s overloading mechanism, you can define multiple overloaded variants for the same method in a class, which is better than using a long list of if/elif in a function.It is used as a decorator. It can split the whole scheme into several small modules, The origial function becomes a generic function: a set of functions that perform the same operation in different ways, depending on the type of the first parameter.1234567891011121314from functools import singledispatch import numbers @singledispatch def sort_type(obj): print(obj, type(obj), 'obj') @sort_type.register(str) def _(text): print(text, type(text), 'str') @sort_type.register(numbers.Integral) def _(n): print(n, type(n), 'int') class functools.singledispatchmethod(func)To define a generic method, decorate it with the @singledispatchmethod decorator. Note that the dispatch happens on the type of the first non-self or non-cls argument, create your function accordingly123456789101112class Negator: @singledispatchmethod def neg(self, arg): raise NotImplementedError(\"Cannot negate a\") @neg.register def _(self, arg: int): return -arg @neg.register def _(self, arg: bool): return not arg functools.update_wrapper(wrapper, wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES)The optional arguments are tuples to specify which attributes of the original function are assigned directly to the matching attributes on the wrapper function and which attributes of the wrapper function are updated with the corresponding attributes from the original function. The default values for these arguments are the module level constants WRAPPER_ASSIGNMENTS (which assigns to the wrapper function’s __module__, __name__, __qualname__, __annotations__ and __doc__, the documentation string) and WRAPPER_UPDATES (which updates the wrapper function’s __dict__, i.e. the instance dictionary).123456Parameters:1. wrapper: A wrapper function.2. wrapped: The function being wrapped over or the wrapped function.3. assigned: Attributes of the wrapped function which are assigned to the matching attributes of wrapper function as a tuple(optional argument).4. updated: Attributes of the wrapper function that are updated with respect to original function attributes as a tuple(optional argument).example can be seen as below123456789101112131415161718192021222324252627282930313233# Python program to demonstrate# ipdate)wrapper() methodimport functools as ft# Defining the decoratordef hi(func): def wrapper(): \"Hi has taken over Hello Documentation\" print(\"Hi geeks\") func() # Note The following Steps Clearly print(\"UPDATED WRAPPER DATA\") print(f'WRAPPER ASSIGNMENTS :{ft.WRAPPER_ASSIGNMENTS}') print(f'UPDATES : {ft.WRAPPER_UPDATES}') # Updating Metadata of wrapper # using update_wrapper ft.update_wrapper(wrapper, func) return wrapper@hidef hello(): \"this is the documentation of Hello Function\" print(\"Hey Geeks\")print(hello.__name__)print(hello.__doc__)help(hello) @functools.wraps(wrapped, assigned=WRAPPER_ASSIGNMENTS, updated=WRAPPER_UPDATES)This is a convenience function for invoking update_wrapper() as a function decorator when defining a wrapper function. It is equivalent to partial(update_wrapper, wrapped=wrapped, assigned=assigned, updated=updated)1234567891011121314151617181920&gt;&gt;&gt; from functools import wraps&gt;&gt;&gt; def my_decorator(f):... @wraps(f)... def wrapper(*args, **kwds):... print('Calling decorated function')... return f(*args, **kwds)... return wrapper...&gt;&gt;&gt; @my_decorator... def example():... \"\"\"Docstring\"\"\"... print('Called example function')...&gt;&gt;&gt; example()Calling decorated functionCalled example function&gt;&gt;&gt; example.__name__'example'&gt;&gt;&gt; example.__doc__'Docstring'","link":"/2020/07/02/common-modules-in-python-2-functools/"},{"title":"common_modules_in_python(6):bisect","text":"This module provides support for maintaining a list in sorted order without having to sort the list after each insertion. For long lists of items with expensive comparison operations, this can be an improvement over the more common approach. The module is called bisect because it uses a basic bisection algorithm to do its work. The source code may be most useful as a working example of the algorithm (the boundary conditions are already right!). logic - binary search Compare x with the middle element. If x matches with middle element, we return the mid index. Else If x is greater than the mid element, then x can only lie in right half subarray after the mid element. So we recur for right half. Else (x is smaller) recur for the left half.12345678910111213#Pseudocodefunction binary_search(A, n, T) is L := 0 R := n − 1 while L ≤ R do m := floor((L + R) / 2) if A[m] &lt; T then L := m + 1 else if A[m] &gt; T then R := m − 1 else: return m return unsuccessful function bisect.bisect_left(a, x, lo=0, hi=len(a))(find the location but not insert)Locate the insertion point for x in a to maintain sorted order. The parameters lo and hi may be used to specify a subset of the list which should be considered; by default the entire list is used. If x is already present in a, the insertion point will be before (to the left of) any existing entries. The return value is suitable for use as the first parameter to list.insert() assuming that a is already sorted.The returned insertion point i partitions the array a into two halves so that all(val &lt; x for val in a[lo:i]) for the left side and all(val &gt;= x for val in a[i:hi]) for the right side. bisect.bisect_right(a, x, lo=0, hi=len(a)) bisect.bisect(a, x, lo=0, hi=len(a))Similar to bisect_left(), but returns an insertion point which comes after (to the right of) any existing entries of x in a. The returned insertion point i partitions the array a into two halves so that all(val &lt;= x for val in a[lo:i]) for the left side and all(val &gt; x for val in a[i:hi]) for the right side. bisect.insort_left(a, x, lo=0, hi=len(a))Insert x in a in sorted order. This is equivalent to a.insert(bisect.bisect_left(a, x, lo, hi), x) assuming that a is already sorted. Keep in mind that the O(log n) search is dominated by the slow O(n) insertion step. bisect.insort_right(a, x, lo=0, hi=len(a)) bisect.insort(a, x, lo=0, hi=len(a))Similar to insort_left(), but inserting x in a after any existing entries of x. source code of bisect12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273\"\"\"Bisection algorithms.\"\"\"def insort_right(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the right of the rightmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" lo = bisect_right(a, x, lo, hi) a.insert(lo, x)def bisect_right(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt;= x, and all e in a[i:] have e &gt; x. So if x already appears in the list, a.insert(i, x) will insert just after the rightmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 # Use __lt__ to match the logic in list.sort() and in heapq if x &lt; a[mid]: hi = mid else: lo = mid+1 return lodef insort_left(a, x, lo=0, hi=None): \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted. If x is already in a, insert it to the left of the leftmost x. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" lo = bisect_left(a, x, lo, hi) a.insert(lo, x)def bisect_left(a, x, lo=0, hi=None): \"\"\"Return the index where to insert item x in list a, assuming a is sorted. The return value i is such that all e in a[:i] have e &lt; x, and all e in a[i:] have e &gt;= x. So if x already appears in the list, a.insert(i, x) will insert just before the leftmost x already there. Optional args lo (default 0) and hi (default len(a)) bound the slice of a to be searched. \"\"\" if lo &lt; 0: raise ValueError('lo must be non-negative') if hi is None: hi = len(a) while lo &lt; hi: mid = (lo+hi)//2 # Use __lt__ to match the logic in list.sort() and in heapq if a[mid] &lt; x: lo = mid+1 else: hi = mid return lo# Overwrite above definitions with a fast C implementationtry: from _bisect import *except ImportError: pass# Create aliasesbisect = bisect_rightinsort = insort_right© 2020 GitHub, Inc. referencehttps://docs.python.org/3/library/bisect.html","link":"/2020/07/12/common-modules-in-python-6-bisect/"},{"title":"tree algorithm in Python","text":"Definition: what is tree?two important features: connected no cycles a feature of tree: the number of edges is number of nodes - proof : remove the root node application: Graph Valid TreeCheck if a given graph is tree or not How to detect cycle in an undirected graph?We can either use BFS or DFS. For every visited vertex v, if there is an adjacent u such that u is already visited and u is not parent of v, then there is a cycle in graph. If we don’t find such an adjacent for any vertex, we say that there is no cycle (See Detect cycle in an undirected graph for more details). How to check for connectivity?Since the graph is undirected, we can start BFS or DFS from any vertex and check if all vertices are reachable or not. If all vertices are reachable, then graph is connected, otherwise not. 1234567891011121314151617181920212223242526import collections# BFS solution. Same complexity but faster version.class Solution(object): # @param {integer} n # @param {integer[][]} edges # @return {boolean} def validTree(self, n, edges): if len(edges) != n - 1: # Check number of edges. - there is the Definition of the tree return False # init node's neighbors in dict neighbors = collections.defaultdict(list) for u, v in edges: neighbors[u].append(v) neighbors[v].append(u) # BFS to check whether the graph is valid tree. q = collections.deque([0]) visited = set([0]) while q: curr = q.popleft() for node in neighbors[curr]: if node not in visited: visited.add(node) q.append(node) return len(visited) == n Vocabulary Node A node is a fundamental part of a tree. It can have a name, which we call the “key.” A node may also have additional information. We call this additional information the “payload.” While the payload information is not central to many tree algorithms, it is often critical in applications that make use of trees. Edge An edge is another fundamental part of a tree. An edge connects two nodes to show that there is a relationship between them. Every node (except the root) is connected by exactly one incoming edge from another node. Each node may have several outgoing edges. Root The root of the tree is the only node in the tree that has no incoming edges. Path A path is an ordered list of nodes that are connected by edges. For example, Mammal → Carnivora → Felidae → Felis → Domestica is a path. Children The set of nodes 𝑐 that have incoming edges from the same node to are said to be the children of that node. Parent A node is the parent of all the nodes it connects to with outgoing edges. Sibling Nodes in the tree that are children of the same parent are said to be siblings. Subtree A subtree is a set of nodes and edges comprised of a parent and all the descendants of that parent. Leaf Node A leaf node is a node that has no children. Level The level of a node 𝑛 is the number of edges on the path from the root node to 𝑛. Height The height of a tree is equal to the maximum level of any node in the tree. degree The degree of a tree is the maximum degree of a node in the tree. For example:If the maximum degree of a node is 2, then it is a binary tree. Likewise, if the maximum degree of a node is 3, then it becomes a ternary tree.P.S: Degree of a node is the maximum number of child nodes it can have. Binary treea binary tree is a tree data structure in which each node has at most two children, which are referred to as the left child and the right child. how to implement it in Python12345678910111213141516171819class BinaryTree: def __init__(self, value): self.value = value self.left_child = None self.right_child = None def insert_left(self, value): if self.left_child == None: self.left_child = BinaryTree(value) else: new_node = BinaryTree(value) new_node.left_child = self.left_child self.left_child = new_node def insert_right(self, value): if self.right_child == None: self.right_child = BinaryTree(value) else: new_node = BinaryTree(value) new_node.right_child = self.right_child self.right_child = new_node tree traversalthere are two options:Depth-First Search (DFS) and Breadth-First Search (BFS). DFSDepth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures.. We can simply begin from a node, then traverse its adjacent (or children) without caring about cycles. And if we begin from a single node (root), and traverse this way, it is guaranteed that we traverse the whole tree as there is no dis-connectivity,There are three ways in tree traversal: preorder Algorithm Preorder(tree) Visit the root. Traverse the left subtree, i.e., call Preorder(left-subtree) Traverse the right subtree, i.e., call Preorder(right-subtree) 1234567891011121314151617181920212223242526272829303132333435363738# Definition for a binary tree node.class TreeNode(object): def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right# recursiveclass Solution(object): def preorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" ret = [] def fn(node): if node: ret.append(node.val) fn(node.left) fn(node.right) fn(root) return ret# iterativeclass Solution(object): def preorderTraversal(self, root): if not root: return [] stack = [root] ret = [] while(stack): node = stack.pop() ret.append(node.val) if node.right: stack.append(node.right) if node.left: stack.append(node.left) return ret application: Leetcode 1028. Recover a Tree From Preorder Traversal Leetcode 144. Binary Tree Preorder Traversal Leetcode 971. Flip Binary Tree To Match Preorder Traversal Leetcode 589. N-ary Tree Preorder Traversal Leetcode 331. Verify Preorder Serialization of a Binary Tree inorder Algorithm Inorder(tree) Traverse the left subtree, i.e., call Inorder(left-subtree) Visit the root. Traverse the right subtree, i.e., call Inorder(right-subtree) 123456789101112class Node: def __init__(self,key): self.left = None self.right = None self.val = keyret = []def inOrderTraversal(root): if root: inOrderTraversal(root.left) ret.append(root.val) inOrderTraversal(root.right) - application: 1. Leetcode 94. Binary Tree Inorder Traversal 2. Leetcode 105. Construct Binary Tree from Preorder and Inorder Traversal postorder Algorithm Postorder(tree) Traverse the left subtree, i.e., call Postorder(left-subtree) Traverse the right subtree, i.e., call Postorder(right-subtree) Visit the root. 12345678910111213141516171819202122232425262728293031323334353637383940# Definition for a binary tree node.class TreeNode(object): def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right# recursiveclass Solution(object): def postorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" ret = [] def dfs(root): if root: dfs(root.left) dfs(root.right) ret.append(root.val) dfs(root) return ret#iterativeclass Solution(object): def postorderTraversal(self, root): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" if not root: return [] stack = [root] ret = [] while stack: cur = stack.pop() ret.append(cur.val) if cur.left: stack.append(cur.left) if cur.right: stack.append(cur.right) return ret[::-1] application: Leetcode 590. N-ary Tree Postorder Traversal Leetcode 145. Binary Tree Postorder Traversal Leetcode 106. Construct Binary Tree from Inorder and Postorder Traversal BFS123456789101112131415from collections import dequeclass Solution: def BFStraversal(self,root): if not root: return None stack = deque([root]) ret = [] while stack: cur = stack.pop() ret.append(cur.val) if cur.left: stack.appendleft(cur.left) if cur.right: stack.appendleft(cur.right) return ret full binary tree vs complete binary tree A full binary tree (sometimes proper binary tree or 2-tree) is a tree in which every node other than the leaves has two children.some proporty of full binary tree:1） The total number of nodes of full binary tree is at most $2^i -1 $2） And the number of non-leaf node is one less than the leaf node. proof 1:The number of nodes of each layer form a geometric sequence. Image we have a tree which has $i$ layers.The number of nodes in the last layer is $2^{i-1}$And to compute the total number of the tree, we could use the sum formula of geometric sequence: $\\frac{a_1 - a_nq}{1-q}$.So the result will be $2^{i}-1$. proof 2:We set the set of nodes with two child nodes is F and leaf node is L. For full tree, we only have these two kind of nodes.only leaf node can generate child nodes, so the total number of nodes can be represent 1 + 2F (1 is the root node)Also, the total number can be represented as F+L 1+2F = F+ Lso that L = 1+ F A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible AVL tree is a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees cannot be more than one for all nodes (for every node). binary search tree (BST)An important property of a Binary Search Tree is that the value of a Binary Search Tree nodeis larger than the value of the offspring of its left child, but smaller than the value of the offspring of its right child.123456789101112131415class BinarySearchTree: def __init__(self, value): self.value = value self.left_child = None self.right_child = None def insert_node(self, value): if value &lt;= self.value and self.left_child: self.left_child.insert_node(value) elif value &lt;= self.value: self.left_child = BinarySearchTree(value) elif value &gt; self.value and self.right_child: self.right_child.insert_node(value) else: self.right_child = BinarySearchTree(value) application: Leetcode 109. Convert Sorted List to Binary Search Tree Leetcode 96. Unique Binary Search Trees Leetcode 95. Unique Binary Search Trees II Leetcode 1008. Construct Binary Search Tree from Preorder Traversal Leetcode 235. Lowest Common Ancestor of a Binary Search Tree Leetcode 98. Validate Binary Search Tree Leetcode 108. Convert Sorted Array to Binary Search Tree trie defination :a trie, also called digital tree or prefix tree, is a kind of search tree—an ordered tree data structure used to store a dynamic set or associative array where the keys are usually strings.Here’s a trie that stores “David”, “Maria”, and “Mario”. Strengths Sometimes Space-Efficient. If you’re storing lots of words that start with similar patterns, tries may reduce the overall storage cost by storing shared prefixes once. Efficient Prefix Queries. Tries can quickly answer queries about words with shared prefixes, like: How many words start with “choco”? What’s the most likely next letter in a word that starts with “strawber”? Weaknesses Usually Space-Inefficient. Tries rarely save space when compared to storing strings in a set. ASCII characters in a string are one byte each. Each link between trie nodes is a pointer to an address—eight bytes on a 64-bit system. So, the overhead of linking nodes together often outweighs the savings from storing fewer characters. Not Standard. Most languages don’t come with a built-in trie implementation. You’ll need to implement one yourself. What happens if we have two words and one is a prefix of the other?we need to mark the word ending like using another node or True Here is an example1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#Leetcode 208:Implement Trie (Prefix Tree)class TrieNode: def __init__(self): self.children = {} self.endofWord = Falseclass Trie: def __init__(self): \"\"\" Initialize your data structure here. \"\"\" self.root = TrieNode() def insert(self, word: str) -&gt; None: \"\"\" Inserts a word into the trie. \"\"\" n = len(word) start = self.root for i in range(n): if word[i] not in start.children: start.children[word[i]] = TrieNode() start = start.children[word[i]] start.endofWord = True def search(self, word: str) -&gt; bool: \"\"\" Returns if the word is in the trie. \"\"\" n = len(word) start = self.root for i in range(n): if word[i] not in start.children: return False start = start.children[word[i]] return start.endofWord def startsWith(self, prefix: str) -&gt; bool: \"\"\" Returns if there is any word in the trie that starts with the given prefix. \"\"\" n = len(prefix) start = self.root for i in range(n): if prefix[i] not in start.children: return False start = start.children[prefix[i]] return True Radix treea radix tree (also radix trie or compact prefix tree) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. The result is that the number of children of every internal node is at most the radix r of the radix tree, where r is a positive integer and a power x of 2. segment tree defination:Segment Tree is a basically a binary tree used for storing the intervals or segments. Each node in the Segment Tree represents an interval. Consider an array of size N and a corresponding Segment Tree T.(It is AVL) The root of T will represent the whole array A[:N] . Each leaf in the Segment Tree T will represent a single element A[i] such that 0 &lt;= i &lt; N. The internal nodes in the Segment Tree represents the union of elementary intervals A[i:j] where 0&lt;= i&lt;j&lt; N. application:Sum of given range image: Binary Indexed Tree what is binary indexed treeAs the name suggests, it is to use arrays to simulate tree structures. Then a question arises, why not build trees directly? The answer is there is no need to build trees for problems that arrays can handle. It is similar to the structure of trie tree. For whatupdate and sum for a interval binary indexed tree vs segement treeThe problems that a binary indexed tree can solve can be solved with a line segment tree. What is the difference between the two?The coefficient of the tree array is much less. advantages vs disadvantagesThe complexity of modification and query is O(logN), and the coefficient is much less than that of the line segment tree, faster than the traditional array, and easy to write.The disadvantage is that it still cannot be solved when encountering complex interval problems, and the function is still limited. understand the binary indexed The black array represents the original array (replaced with A[i] below), and the red structure represents our tree-like array (replaced with C[i] below).The rules can be seen as follows 123456789C[1] = A[1];C[2] = A[1] + A[2];C[3] = A[3];C[4] = A[1] + A[2] + A[3] + A[4];C[5] = A[5];C[6] = A[5] + A[6];C[7] = A[7];C[8] = A[1] + A[2] + A[3] + A[4] + A[5] + A[6] + A[7] + A[8];C[i] = A[i - 2k+1] + A[i - 2k+2] + ... + A[i];//k is the length of consecutive zeros from the lowest bit to the highest bit in the binary of i More detail :https://www.youtube.com/watch?v=CWDQJGaN1gY operation sum 12345678 getSum(x): Returns the sum of the sub-array arr[0,...,x]// Returns the sum of the sub-array arr[0,...,x] using BITree[0..n], which is constructed from arr[0..n-1]1) Initialize the output sum as 0, the current index as x+1.2) Do following while the current index is greater than 0....a) Add BITree[index] to sum...b) Go to the parent of BITree[index]. The parent can be obtained by removing the last set bit from the current index, i.e., index = index - (index &amp; (-index))3) Return sum. get_sum_tree.png update 1234567 update(x, val): Updates the Binary Indexed Tree (BIT) by performing arr[index] += val// Note that the update(x, val) operation will not change arr[]. It only makes changes to BITree[]1) Initialize the current index as x+1.2) Do the following while the current index is smaller than or equal to n....a) Add the val to BITree[index]...b) Go to parent of BITree[index]. The parent can be obtained by incrementing the last set bit of the current index, i.e., index = index + (index &amp; (-index)) implementation 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# Python implementation of Binary Indexed Tree# Returns sum of arr[0..index]. This function assumes# that the array is preprocessed and partial sums of# array elements are stored in BITree[].def getsum(BITTree,i): s = 0 #initialize result # index in BITree[] is 1 more than the index in arr[] i = i+1 # Traverse ancestors of BITree[index] while i &gt; 0: # Add current element of BITree to sum s += BITTree[i] # Move index to parent node in getSum View i -= i &amp; (-i) return s# Updates a node in Binary Index Tree (BITree) at given index# in BITree. The given value 'val' is added to BITree[i] and# all of its ancestors in tree.def updatebit(BITTree , n , i ,v): # index in BITree[] is 1 more than the index in arr[] i += 1 # Traverse all ancestors and add 'val' while i &lt;= n: # Add 'val' to current node of BI Tree BITTree[i] = v # Update index to that of parent in update View i += i &amp; (-i)# Constructs and returns a Binary Indexed Tree for given# array of size n.def construct(arr, n): # Create and initialize BITree[] as 0 BITTree = [0]*(n+1) # Store the actual values in BITree[] using update() for i in range(n): updatebit(BITTree, n, i, arr[i]) # Uncomment below lines to see contents of BITree[] #for i in range(1,n+1): # print BITTree[i], return BITTree# Driver code to test above methodsfreq = [2, 1, 1, 3, 2, 3, 4, 5, 6, 7, 8, 9]BITTree = construct(freq,len(freq))print(\"Sum of elements in arr[0..5] is \" + str(getsum(BITTree,5)))freq[3] += 6updatebit(BITTree, len(freq), 3, 6)print(\"Sum of elements in arr[0..5]\"+ \" after update is \" + str(getsum(BITTree,5)))# This code is contributed by Raju Varshney example:Leetcode 307 Range Sum Query - MutableGiven an integer array nums, find the sum of the elements between indices i and j (i ≤ j), inclusive.The update(i, val) function modifies nums by updating the element at index i to val.1234567891011121314151617181920212223242526272829class NumArray(object): def __init__(self, nums): self.nums = nums self.N = len(self.nums) self.tree = [0] * (self.N + 1) ## optimize initiate BIT in O(n) for j in range(1, self.N+1): self.tree[j] += self.nums[j-1] if (j + (j &amp; (-j))) &lt;= self.N: self.tree[j + (j &amp; (-j))] += self.tree[j] def update(self, i, val): diff = val - self.nums[i] self.nums[i] = val i += 1 while i &lt;= self.N: self.tree[i] += diff i += (i &amp; (-i)) def sumRange(self, i, j): return self.getSum(j) - self.getSum(i - 1) def getSum(self, i): sm = 0 i += 1 while i &gt; 0: sm += self.tree[i] i -= (i &amp; (-i)) return sm Notice: distinguish the leaf node and the end of path123 1 /2 For example, Node 1 is the end of path but it is not the leaf node.Leetcode 112. Path Sum referencehttps://www.freecodecamp.org/news/all-you-need-to-know-about-tree-data-structures-bceacb85490c/https://runestone.academy/runestone/books/published/pythonds/Trees/toctree.htmlhttps://www.cnblogs.com/xenny/p/9739600.htmlhttps://www.geeksforgeeks.org/binary-indexed-tree-or-fenwick-tree-2/","link":"/2020/06/28/tree-algorithm-in-Python/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/05/09/hello-world/"},{"title":"sorting alogrithms in python","text":"The materials are based on geekforgeek and https://www.cnblogs.com/onepixel/articles/7674659.html bubble sortBubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in wrong order.If we rank n elements, we need n-1 comparisons, so K comparisons need n-k comparisons.The comparison times are: (n-1) + (n-2) +… + 1 = n * (n-1) / 2, so the time complexity of bubble sorting is O (n ^ 2).The worse case and average are both O (n ^ 2)best case is O(n) (if we use flag)Space complexity: 1 Algorithm Introduction: Compare adjacent elements, the former is larger than the latter (or the former is smaller than the latter) and change positions Repeat the work for each pair of adjacent elements from the beginning to the end. After this step is completed, the end is the largest or smallest number Repeat the above steps for the last element. Repeat steps 1-3 until sorting is completed to implement it in Python12345678910111213141516171819# increasing orderdef bubbleSort(aList): n = len(aList) for i in range(n-1): # the index of loop for j in range(1,n-i): # the index of element if aList[j]&lt; aList[j-1]: aList[j],aList[j-1] = aList[j-1],aList[j] return aListdef bubbleSortWithFlag(aList): n = len(aList) for i in range(n-1): # the index of loop flag = False for j in range(1,n-i): # the index of element if aList[j]&lt; aList[j-1]: flag = True aList[j],aList[j-1] = aList[j-1],aList[j] if not flag: return aList selection sortThe selection sort algorithm sorts an array by repeatedly finding the minimum element (considering ascending order) from unsorted part and putting it at the beginning. The algorithm maintains two subarrays in a given array. 1) The subarray which is already sorted.2) Remaining subarray which is unsorted. In every iteration of selection sort, the minimum element (considering ascending order) from the unsorted subarray is picked and moved to the sorted subarray. Algorithm Introduction: the smallest (large) element is found in the unsorted sequence and stored at the beginning of the sorted sequence ( we can still use the original array as the sorted sequence by swapping). continue to find the smallest (largest) element from the remaining unsorted elements, and put it at the end of the sorted sequence.Repeat the second step until all elements are sorted. Worst complexity: O(n^2)Average complexity:O(n^2)Best complexity: O(n^2)Space complexity: O(1) 12345678910111213def selectionSort(aList): n = len(aList) for i in range(n-1): for j in range(i,n): if j ==i: small_value = aList[j] small_index = j else: if aList[j] &lt; small_value: small_value = aList[j] small_index = j aList[i],aList[small_index] = aList[small_index],aList[i] return aList Insertion SortThe idea of insertion sorting is to maintain an ordered area and insert data one by one into the proper position of the ordered area until the whole array is ordered.Generally speaking, insertion sorting is implemented on array by in place. The specific algorithm is described as follows: Algorithm Introduction: Starting from the first element, the element can be considered to have been sorted; Take out the next element and scan from the back to the front in the sorted element sequence; If the element (sorted) is larger than the new element, move the element to the next location; Repeat step 3 until the ordered element is less than or equal to the position of the new element; After inserting the new element into the location;Repeat steps 2-5. Worst complexity: n^2Average complexity: n^2Best complexity: nSpace complexity: 1123456789101112def insertionSort(aList): n = len(aList) for i in range(1,n): # save the index and value of i temp = aList[i] j =i # find the location to put i while j&gt;0 and aList[j-1]&gt; temp: aList[j] = aList[j-1] j -=1 aList[j] = temp return aList shell sortshell sort is also called reduced incremental sort. Its essence is insertion sort. It just divides the sequence to be sorted into several subsequences according to some rules, and directly inserts and sorts these subsequences respectively. The embodiment of this rule is the selection of increment. If the selected increment is 1, then it is insertion sort. And each sorting of shell sort will make the whole sequence orderly. When the whole sequence is orderly, then insert sort with increment of 1 will improve the sorting efficiency.The commonly used increment is 2-span increment, so every time you sort, the increment is half of the last one, which is what we often call shell increment. Algorithm Introduction: Select an incremental sequence t1, t2 , tk, where ti &gt; tj, tk = 1; By the number k of incremental sequence, the sequence is sorted by k times; According to the corresponding increment ti, the sequence to be sorted is divided into several subsequences with length of m, and each subsequence is inserted and sorted directly. Only when the increment factor is 1, the whole sequence is treated as a table, and the table length is the length of the whole sequence. time complexity:O(nlogn)～O(n^2)space complexity: O(1) 12345678910111213141516def shellSort(aList): n = len(aList) # init gap gap = n//2 while gap &gt;=1: print(gap) # do the insertion sort based on different gaps for i in range(gap,n): temp = aList[i] j =i while j&gt;= gap and aList[j-gap]&gt;temp: aList[j] = aList[j-gap] j = j-gap aList[j] = temp gap = gap //2 return aList merge sortMerge sort is a divide and conquer algorithm. First divide the list into the smallest unit (1 element), then compare each element with the adjacent list to sort and merge the two adjacent lists. Finally all the elements are sorted. Algorithm Introduction: The input sequence of length n is divided into two subsequences of length n / 2; The two subsequences are sorted by merging; Merge two sorted subsequences into a final sorting sequence. Worst complexity: nlog(n)Average complexity: nlog(n)Best complexity: n*log(n)Space complexity: n123456789101112131415161718192021222324252627282930313233def merge(a,b): ret_list = [] a_i = 0 b_i =0 len_a = len(a) len_b = len(b) while a_i&lt;= len_a -1 and b_i&lt;= len_b -1: if a[a_i]&lt;= b[b_i]: ret_list.append(a[a_i]) a_i = a_i+1 else: ret_list.append(b[b_i]) b_i = b_i+1 while a_i &lt;=len_a -1: ret_list.append(a[a_i]) a_i = a_i+1 while b_i &lt;=len_b -1: ret_list.append(b[b_i]) b_i = b_i+1 return ret_listdef mergeSort(aList): n = len(aList) # divide n into two parts if n &gt;=2: mid = n//2 left = mergeSort(aList[:mid]) right =mergeSort(aList[mid:]) # merge it finally return merge(left,right) else: return aList quick sortQuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. There are many different versions of quickSort that pick pivot in different ways. Always pick first element as pivot. Always pick last element as pivot (implemented below) Pick a random element as pivot. Pick median as pivot. The key process in quickSort is partition(). Target of partitions is, given an array and an element x of array as pivot, put x at its correct position in sorted array and put all smaller elements (smaller than x) before x, and put all greater elements (greater than x) after x. All this should be done in linear time. Worst complexity: n^2Average complexity: nlog(n)Best complexity: nlog(n) 1234567891011121314151617def quickSort(aList): # choose the first element as pivot if len(aList)&lt;=1: return aList pivot = aList[0] left = [] right = [] for i in range(1,len(aList)): if aList[i]&lt;pivot: left.append(aList[i]) else: right.append(aList[i]) left = quickSort(left) right = quickSort(right) left.append(pivot) left.extend(right) return left heap sortHeap sort is a comparison based sorting technique based on Binary Heap data structure. It is similar to selection sort where we first find the maximum element and place the maximum element at the end. We repeat the same process for remaining element. What is Binary Heap?Let us first define a Complete Binary Tree. A complete binary tree is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible (Source Wikipedia) A Binary Heap is a Complete Binary Tree where items are stored in a special order such that value in a parent node is greater(or smaller) than the values in its two children nodes. The former is called as max heap and the latter is called min heap. The heap can be represented by binary tree or array. Why array based representation for Binary Heap?Since a Binary Heap is a Complete Binary Tree, it can be easily represented as array and array based representation is space efficient. If the parent node is stored at index I, the left child can be calculated by 2 I + 1 and right child by 2 I + 2 (assuming the indexing starts at 0). Heap Sort Algorithm for sorting in increasing order: Build a max heap from the input data. At this point, the largest item is stored at the root of the heap. Replace it with the last item of the heap followed by reducing the size of heap by 1. Finally, heapify the root of tree. Repeat above steps while size of heap is greater than 1. How to build the heap?Heapify procedure can be applied to a node only if its children nodes are heapified. So the heapification must be performed in the bottom up order. Lets understand with the help of an example:12345678910111213141516171819202122232425Input data: 4, 10, 3, 5, 1 4(0) / \\ 10(1) 3(2) / \\ 5(3) 1(4)The numbers in bracket represent the indices in the arrayrepresentation of data.Applying heapify procedure to index 1: 4(0) / \\ 10(1) 3(2) / \\5(3) 1(4)Applying heapify procedure to index 0: 10(0) / \\ 5(1) 3(2) / \\ 4(3) 1(4)The heapify procedure calls itself recursively to build heap in top down manner. Worst complexity: nlog(n)Average complexity: nlog(n)Best complexity: n*log(n)Space complexity: 1 we will start our heapify on the the last non-leaf node. To get it, we know that the index value of the last leaf node is n-1, and its parent node index value is [(n-1) - 1] / 2 = n / 2 - 1. And after we build a max heap, More detail Explanation on heap sort is https://www.cnblogs.com/chengxiao/p/6129630.html 12345678910111213141516171819202122def heapify(aList,n,i): left = 2*i + 1 right = 2*i +2 largest = i if left&lt;n and aList[left]&gt;aList[largest]: largest = left if right&lt;n and aList[right]&gt;aList[largest]: largest = right if largest != i: aList[largest],aList[i] = aList[i],aList[largest] heapify(aList,n,largest)def heapSort(aList): n = len(aList) #build the max heap for i in range(n//2 -1,-1,-1): heapify(aList,n,i) # get the element based on the order (from the root) for i in range(n-1,0,-1): aList[0],aList[i] = aList[i],aList[0] heapify(aList,i,0) return aList counting sortIt is applicable to a certain range of integer sorting. When the value range is not very large, its performance is even faster than those o (nlogn) sorting in some cases, such as fast sorting and merging sorting.Let us understand it with the help of an example 12345678910111213141516171819For simplicity, consider the data in the range 0 to 9.Input data: 1, 4, 1, 2, 7, 5, 2 1) Take a count array to store the count of each unique object. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 2 0 1 1 0 1 0 0 2) Modify the count array such that each element at each index stores the sum of previous counts. Index: 0 1 2 3 4 5 6 7 8 9 Count: 0 2 4 4 5 6 6 7 7 7The modified count array indicates the position of each object inthe output sequence. 3) Output each object from the input sequence followed by decreasing its count by 1. Process the input data: 1, 4, 1, 2, 7, 5, 2. Position of 1 is 2. Put data 1 at index 2 in output. Decrease count by 1 to place next data 1 at an index 1 smaller than this index. 12345678910111213141516def countingSort(aList): min_v = min(aList) max_v = max(aList) rag = max_v - min_v +1 n = len(aList) num_counter = {} for i in range(rag): num_counter[min_v+i] = 0 for i in range(n): num_counter[aList[i]] +=1 ret_list = [] for i in range(rag): ret_list.extend([min_v+i]*num_counter[min_v+ i]) return ret_list bucket sortBucket Sort is a sorting technique that sorts the elements by first dividing the elements into several groups called buckets. The elements inside each bucket are sorted using any of the suitable sorting algorithms or recursively calling the same algorithm.bucketSort(arr[], n) 1) Create n empty buckets (Or lists).2) Do following for every array element arr[i].…….a) Insert arr[i] into bucket[n*array[i]]3) Sort individual buckets using insertion sort.4) Concatenate all sorted buckets. If we assume that insertion in a bucket takes O(1) time then steps 1 and 2 of the above algorithm clearly take O(n) time. The O(1) is easily possible if we use a linked list to represent a bucket (In the following code, C++ vector is used for simplicity). Step 4 also takes O(n) time as there will be n items in all buckets. 123456789101112131415161718192021222324252627282930313233def insertionSort(b): for i in range(1, len(b)): up = b[i] j = i - 1 while j &gt;=0 and b[j] &gt; up: b[j + 1] = b[j] j -= 1 b[j + 1] = up return b def bucketSort(x): arr = [] slot_num = 10 # 10 means 10 slots, each # slot's size is 0.1 for i in range(slot_num): arr.append([]) # Put array elements in different buckets for j in x: index_b = int(slot_num * j) arr[index_b].append(j) # Sort individual buckets for i in range(slot_num): arr[i] = insertionSort(arr[i]) # concatenate the result k = 0 for i in range(slot_num): for j in range(len(arr[i])): x[k] = arr[i][j] k += 1 return x Radix SortRadix sort is an extension of bucket sort. Its basic idea is to cut integers into different numbers according to the number of digits, and then compare them according to each digit.The specific method is: unify all the values to be compared into the same digit length, and fill zero in front of the shorter digit. Then, start from the lowest order and sort one by one. In this way, from the lowest ranking to the completion of the highest ranking, the sequence becomes an ordered sequence. Algorithm Introduction: Gets the maximum number in the array and the number of digits of the maximun number ; if arr is the original array, and each bit is taken from the lowest bit to form a radix array; To count and sort the radix (use counting sort); 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def countingSort(arr, exp1): n = len(arr) # The output array elements that will have sorted arr output = [0] * (n) # initialize count array as 0 count = [0] * (10) # Store count of occurrences in count[] for i in range(0, n): index = (arr[i]/exp1) count[ (index)%10 ] += 1 # Change count[i] so that count[i] now contains actual # position of this digit in output array for i in range(1,10): count[i] += count[i-1] # Build the output array i = n-1 while i&gt;=0: index = (arr[i]/exp1) output[ count[ (index)%10 ] - 1] = arr[i] count[ (index)%10 ] -= 1 i -= 1 # Copying the output array to arr[], # so that arr now contains sorted numbers i = 0 for i in range(0,len(arr)): arr[i] = output[i]# Method to do Radix Sortdef radixSort(arr): # Find the maximum number to know number of digits max1 = max(arr) # Do counting sort for every digit. Note that instead # of passing digit number, exp is passed. exp is 10^i # where i is current digit number exp = 1 while max1/exp &gt; 0: countingSort(arr,exp) exp *= 10 TimsortThe alogrithm python used for sort and sorted is Timsort. the difference between sort and sorted, is that sorted will generate and return a new array Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort.We divide the Array into blocks known as Run. We sort those runs using insertion sort one by one and then merge those runs using combine function used in merge sort. If the size of Array is less than run, then Array get sorted just by using Insertion Sort. The size of run may vary from 32 to 64 depending upon the size of the array. Note that merge function performs well when sizes subarrays are powers of 2. The idea is based on the fact that insertion sort performs well for small arrays.Details of below implementation : We consider size of run as 32.We one by one sort pieces of size equal to runAfter sorting individual pieces, we merge them one by one. We double the size of merged subarrays after every iteration. Another version of timsort：https://segmentfault.com/a/1190000020280815 attribute value time complexity(best) O(n) time complexity(average) O(nlogn ) time complexity(worse) O(nlogn ) space complexity O(n) stable yes 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# Python3 program to perform TimSort. RUN = 32 # This function sorts array from left index to # to right index which is of size atmost RUN def insertionSort(arr, left, right): for i in range(left + 1, right+1): temp = arr[i] j = i - 1 while j &gt;= left and arr[j] &gt; temp : arr[j+1] = arr[j] j -= 1 arr[j+1] = temp # merge function merges the sorted runs def merge(arr, l, m, r): # original array is broken in two parts # left and right array len1, len2 = m - l + 1, r - m left, right = [], [] for i in range(0, len1): left.append(arr[l + i]) for i in range(0, len2): right.append(arr[m + 1 + i]) i, j, k = 0, 0, l # after comparing, we merge those two array # in larger sub array while i &lt; len1 and j &lt; len2: if left[i] &lt;= right[j]: arr[k] = left[i] i += 1 else: arr[k] = right[j] j += 1 k += 1 # copy remaining elements of left, if any while i &lt; len1: arr[k] = left[i] k += 1 i += 1 # copy remaining element of right, if any while j &lt; len2: arr[k] = right[j] k += 1 j += 1 # iterative Timsort function to sort the # array[0...n-1] (similar to merge sort) def timSort(arr, n): # Sort individual subarrays of size RUN for i in range(0, n, RUN): insertionSort(arr, i, min((i+31), (n-1))) # start merging from size RUN (or 32). It will merge # to form size 64, then 128, 256 and so on .... size = RUN while size &lt; n: # pick starting point of left sub array. We # are going to merge arr[left..left+size-1] # and arr[left+size, left+2*size-1] # After every merge, we increase left by 2*size for left in range(0, n, 2*size): # find ending point of left sub array # mid+1 is starting point of right sub array mid = left + size - 1 right = min((left + 2*size - 1), (n-1)) # merge sub array arr[left.....mid] &amp; # arr[mid+1....right] merge(arr, left, mid, right) size = 2*size The analysis of sorting algorithmshttps://www.geeksforgeeks.org/analysis-of-different-sorting-techniques/","link":"/2020/05/23/sorting-alogrithms-in-python/"},{"title":"regular expression in python","text":"The materials are mainly based on https://deerchao.cn/tutorials/regex/regex.htm the online toolhttps://regex101.com/ basic knowledge of regular expressionQuantifiers A* A sequence of zero or more ‘A’ A+ A sequence of one or more ‘A’ A? A sequence of one or zero ‘A’ A{2} A sequence of exactly two ‘A’} A{2,5} A sequence of two through five (inclusive) ‘A’ A{2,} A sequence of two or more ‘A’ Metacharacters . Any single character ^ Match the beginning of a string $ Match the end of a string ^foobar$ A string that consists exactly of the word ‘foobar’ \\ Quote the next metacharacter \\ $ A ‘$’ [] Character class [0-9] A number between ‘0’ and ‘9’ [a-f] A lower case letter between ‘a’ and ‘f’ [G-Z] An upper case letter between ‘G’ and ‘Z’ a-z Any character except characters between ‘a’ and ‘z’ [0-9a-zA-Z-] A hyphen or an alphanumeric character \\w match letters or numbers or underscores or Chines characters \\s matches any whitespace \\d match numbers \\b start or end of matching words \\W match any characters that are not letters, numbers, underscores or Chinese characters \\S matches any character that is not a space character \\D match any non numeric character \\B matching is not at the beginning or end of a word \\A Matches if the specified characters are at the start of a string the difference between ^$ vs \\b^Find the start of a line, $find the end of a line (without setting singleline)\\b match the beginning and end of a word (boundary) branch conditionsuse | to meet branch conditions. That is to say,There are several rules. If any one of them is satisfied, it should be regarded as a match\\(0\\d{2}\\)[- ]?\\d{8}|0\\d{2}[- ]?\\d{8} This expression matches the phone number of the 3-digit area code, where the area code can be enclosed in parentheses or not used. The area code and the local number can be separated by hyphens or spaces, or there can be no interval. You can try to extend this expression to also support 4-bit area code by using branching conditions. group(): it was mainly used for repeating multiple characters(\\d {1,3} .) {3} \\d {1,3} is a simple IP address matching expression. To understand this expression, analyze it in the following order: \\d {1,3} matches 1 to 3 digits, (\\d {1,3} .) {3} matches 3 digits plus an English period (the whole group is the group) three times, and finally adds a one to three digits (\\d{1,3}) backreferenceAfter specifying a subexpression with parentheses, the text matching the subexpression (that is, the content captured by this grouping) can be further processed in the expression. By default, each group will automatically have a group number. The rule is: from left to right, the group number of the first group is 1, the second group is 2, and so on. Backward references are used to repeatedly search for text that matches a previous group. For example, \\1 represents the text matched by group 1. capture expression meaning (exp) match exp and capture text into an automatically named group (?exp) match exp, and capture the text to the group named name, or write (? ‘name’exp) (?: exp) match exp, do not capture matching text, and do not assign group number to this group Zero-width assertionThe next four are used to find something before or after something (but not including it), that is, they are used to specify a location like \\b, ^, $, which should meet certain conditions (i.e. assertions), so they are also called zero width assertions expression meaning (?= exp) match the position in front of exp (? &lt; = exp) match position after exp (?! exp) match is not followed by exp (? &lt;! exp) matches positions that are not exp before \\b\\w+(?=ing\\b):Matches the front part of a word ending in ing (except for ing), such as I’m singing while you’re dancing. It matches sing and danc (?&lt;=\\bre)\\w+\\b will match the second half of the word beginning with re (except for re), for example, when looking for reading a book, it will match ading. \\d{3}(?!\\d)Match three digits, and the three digits cannot be followed by a number (?&lt;![a-z])\\d{7}:Match seven digits that are not preceded by lowercase letters. comment(?#comment) Greed and laziness *? repeat any time, but as few as possible +? repeat once or more, but as little as possible ?? Repeat 0 or 1 time, but as little as possible {n, m}? Repeat n to m times, but as little as possible {n,}? Repeat more than n times, but as few as possible how to implement regular expression in python re module (built-in) expression meaning findall: Returns a list containing all matches search: Returns a Match object if there is a match anywhere in the string split Returns a list where the string has been split at each match sub Replaces one or many matches with a string re.findall(pattern, string) 1234567891011# Program to extract numbers from a stringimport restring = 'hello 12 hi 89. Howdy 34'pattern = '\\d+'result = re.findall(pattern, string)print(result)# Output: ['12', '89', '34'] when findall meets group():123456import restring = \"1--3--4-5--6--7\"pattern = \"([-]*)([0-9]+)\"result = re.findall(pattern,string)print(result)# Output: [('', '1'), ('--', '3'), ('--', '4'), ('-', '5'), ('--', '6'), ('--', '7')] re.split(pattern, string,[maxsplit])maxsplit:maximum number of splits that will occur 12345678910import restring = 'Twelve:12 Eighty nine:89 Nine:9.'pattern = '\\d+'# maxsplit = 1# split only at the first occurrenceresult = re.split(pattern, string, 1)print(result) re.sub() 1234567891011121314151617# Program to remove all whitespacesimport re# multiline stringstring = 'abc 12\\de 23 \\n f45 6'# matches all whitespace characterspattern = '\\s+'# empty stringreplace = ''new_string = re.sub(pattern, replace, string)print(new_string)# Output: abc12de23f456 re.subn()The re.subn() is similar to re.sub() expect it returns a tuple of 2 items containing the new string and the number of substitutions made. re.search(pattern, str)123456import retxt = \"The rain in Spain\"x = re.search(\"\\s\", txt)print(\"The first white-space character is located in position:\", x.start()) search will return Match obejct match.group()The group() method returns the part of the string where there is a match. match.start()The start() function returns the index of the start of the matched substring match.end()end() returns the end index of the matched substring. match.span()function returns a tuple containing start and end index of the matched part. match.stringreturns the string passed into the function no need use re.compile","link":"/2020/06/10/regular-expression-in-python/"},{"title":"prefix,infix and postfix","text":"1. what is prefix,infix and postfix Infix notation: X + YOperators are written in-between their operands. This is the usual way we write expressions. An expression such as A * ( B + C ) / D is usually taken to mean something like: “First add B and C together, then multiply the result by A, then divide by D to give the final answer.” Postfix notation (also known as “Reverse Polish notation”): X Y +Operators are written after their operands.The infix expression given above is equivalent to A B C + * D / Prefix notation (also known as “Polish notation”): + X YOperators are written before their operands. The expressions given above are equivalent to / * A + B C D` And we use infix in real life but use prefix for computers 2. how to compute the result of prefix Steps:Traverse the expression from left to right: If it is a number, press num_ Stack. If it is an operator, take out num_ The first two elements in the stack, the second is the operands on the left side of the expression, calculate the value of the expression, and press the evaluation result into num_Stack. example:Leetcode 150. Evaluate Reverse Polish Notation1234567891011121314151617181920212223class Solution: def evalRPN(self, tokens): stack = [] for t in tokens: if t not in [\"+\", \"-\", \"*\", \"/\"]: stack.append(int(t)) else: r, l = stack.pop(), stack.pop() if t == \"+\": stack.append(l+r) elif t == \"-\": stack.append(l-r) elif t == \"*\": stack.append(l*r) else: # here take care of the case like \"1/-22\", # in Python 2.x, it returns -1, while in # Leetcode it should return 0 if l*r &lt; 0 and l % r != 0: stack.append(l//r+1) else: stack.append(l//r) return stack.pop() 3. convert prefix to infix steps: If it is a number, add it to output directly. If it is an operator, compare the toppest operator with Op_Stack The priority of the operator in the stack, if the priority is greater than that in Op_Stack, push it into Op_ StackOtherwise, pop all operators in the Op_Stack whose priority is greater than or equal to the priority of the operator are added to the output, and then push this operator into Op_ Stack If it is ( , press Op_ Stack If it is a ), pop all operators in front of the left bracket in the stack a to the outputRepeat the above steps. It will pop all the element in the stack in orderThe code can be seen as follows: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding:utf-8def priority(z): if z in ['×', '*', '/']: return 2 elif z in ['+', '-']: return 1def in2post(expr): \"\"\" :param expr: 前缀表达式 :return: 后缀表达式 Example： \"1+((2+3)×4)-5\" \"1 2 3 + 4 × + 5 -\" \"\"\" stack = [] # 存储栈 post = [] # 后缀表达式存储 for z in expr: if z not in ['×', '*', '/', '+', '-', '(', ')']: # 字符直接输出 post.append(z) print(1, post) else: if z != ')' and (not stack or z == '(' or stack[-1] == '(' or priority(z) &gt; priority(stack[-1])): # stack 不空；栈顶为（；优先级大于 stack.append(z) # 运算符入栈 elif z == ')': # 右括号出栈 while True: x = stack.pop() if x != '(': post.append(x) print(2, post) else: break else: # 比较运算符优先级，看是否入栈出栈 while True: if stack and stack[-1] != '(' and priority(z) &lt;= priority(stack[-1]): post.append(stack.pop()) print(3, post) else: stack.append(z) break while stack: # 还未出栈的运算符，需要加到表达式末尾 post.append(stack.pop()) return post And an example of application is Leetcode 224. Basic CalculatorHere is how I convert infix to postfix and compute it based on postfix 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Solution: def calculate(self, s: str) -&gt; int: postfix = [] o_stack=[] index = 0 while index &lt; len(s): c = s[index] if c.isdigit(): num = [] while index &lt; len(s) and s[index].isdigit(): num.append(s[index]) index = index +1 postfix.append(\"\".join(num)) continue else: if c ==')': while o_stack: cur = o_stack.pop() if cur == '(': break postfix.append(cur) elif c ==\"(\" or (( not o_stack or o_stack[-1] == \"(\") and (c==\"+\" or c ==\"-\")): o_stack.append(c) elif c ==\"+\" or c ==\"-\": while True: if o_stack and o_stack[-1] !='(': postfix.append(o_stack.pop()) else: o_stack.append(c) break index = index +1 while o_stack: postfix.append(o_stack.pop()) print(postfix) n_stack = [] for c in postfix: if c.isdigit(): n_stack.append(c) else: num1,num2 = int(n_stack.pop()), int(n_stack.pop()) if c ==\"+\": n_stack.append(num1+num2) elif c ==\"-\": n_stack.append(num2-num1) return n_stack[-1]","link":"/2020/10/11/prefix-infix-and-postfix/"},{"title":"application of binary search","text":"some common applications of binary searchSince I always make mistakes on the while loop of binary search, I list the codes for common scenario. find the exact value 1234567891011121314 def binarySearch(arr,target): arr.sort() left = 0 right = len(arr)-1 while left &lt;= right: mid = (left + right) //2 if arr[mid] == target: return mid elif arr[mid] &gt; target: right = mid -1 else: left = mid +1 return -1 find the closet value 1234567891011121314151617def findClosetElement(arr,target): arr.sort() left = 0 right = len(arr)-1 if target &gt;=arr[-1]: return right if target &lt;= arr[0]: return left while right - left &gt;1: mid = (right + left)//2 if arr[mid] == target: return mid elif arr[mid] &gt; target: right = mid else: left = mid return left if abs(arr[left] - target) &lt; abs(arr[right] - target) else right find the maximum value less than the target 12345678910def findMax(arr,target): arr.sort() left = 0 right = len(arr) -1 while left &lt; right: mid = (left + right)//2 if arr[mid]&gt;= target: right = mid -1 else: left = mid +1","link":"/2020/11/27/application-of-binary-search/"},{"title":"Multiprocessing Vs. Threading","text":"Multiple CPUs vs multi-core CPUshttps://www.zhihu.com/question/20998226 process vs Threadingone process can have multiple Threadingsprocess has independet memory while threadings share memoryhttps://www.ruanyifeng.com/blog/2013/04/processes_and_threads.html multitaskingOperating system can run multiple tasks at the same time, you are in the browser, while listening to MP3, while in Word to catch up with homework, this is multi-tasking.three modes of multi-tasking: Multiprocessing Multithreading Multiprocessing + Multithreading parallelism vs concurrencyParallelism: it means that at the same time, multiple instructions are executed on multiple processors at the same time;Concurrency: at the same time, only one instruction can be executed, but multiple process instructions are executed in rotation, which makes it possible to execute multiple processes at the same time. Multithreading in PythonFor Python3, we usually use threading module.One thing we must notice is that threading cannot provide real . The problem is just that it can’t use more than one of the available cores. This is due to something called the GIL(Global Interpreter Lock). Python threads still work for I/O bound tasks as opposed to CPU bound tasks which may cause deadlocks and race conditions. Many Python libraries solve this issue by using C extensions to bypass the GIL. Of course, this is all in the case of CPython.it is much better to use multiprocessing to get the benefit of all the cores. But there are much fewer cores than there are threads. Cores are valuable resources and take up a lot of memory. If you don’t mind dealing with IPC(Interprocess Communication), then it is a great solution. What is GIL (Global Interpreter Lock) It is connected with CPython It ensures that only one Python thread executes at any time The GIL prevents race conditions and ensures thread safety. A nice explanation of how the GIL helps in these areas can be found here. In short, this mutex is necessary mainly because CPython’s memory management is not thread-safe.https://python3.guide/python-concurrency/the-python-gil mechanism of GIL http://c.biancheng.net/view/5537.html How to deal with GIL Use multiprocessing Use another Python implementation except Cpython C extension programming technology.The main idea is to transfer computationally intensive tasks to C, independent of Python, and release the GIL in the C code while working. This can be done by inserting a special macro like the following in the C code.https://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p09_dealing_with_gil_stop_worring_about_it.html the module threadingan example from website basic example 1234567891011121314151617181920212223242526 import threading import time def foo(n): print('&gt;&gt;&gt;&gt;&gt;%s' %n) time.sleep(3) def bar(n): time.sleep(5) print('&gt;&gt;&gt;&gt;&gt;%s' %n) s = time.time() t1 = threading.Thread(target=foo,args=(2,)) t1.start() # .start() - activate thread t2 = threading.Thread(target=bar,args=(5,)) t2.setDaemon(True) # the main thread doesn't wait the child thread t2.start() t1.join() # wait one thread to be completed print('ending!') print('cost time:',time.time()-s) use class 123456789101112131415161718192021import threadingimport timeclass MyThread(threading.Thread): def __init__(self,name): threading.Thread.__init__(self) self.name = name # super().__init__() def run(self): print('OK') time.sleep(2) print('end',self.name)t1 = MyThread(\"t1\")t1.start()t2 = MyThread(\"t2\")t2.start()print('ending all') locklock is necessary even though we have GIL. Let we give an example. 1234567891011121314151617181920import threadingimport timebalance = 0def change_it_without_lock(n): global balance for i in range(100000): print(balance) balance = balance + n time.sleep(5) balance = balance - nthreads = [ threading.Thread(target=change_it_without_lock, args=(8,) ), threading.Thread(target=change_it_without_lock, args=(10,) )][t.start() for t in threads][t.join() for t in threads] From the unlock version, we will get value like 8,10 We could use mutex in the following case 1234567891011121314151617181920212223import threadingimport timebalance = 0lock = threading.Lock()def change_it_without_lock(n): global balance for i in range(100000): print(balance) lock.acquire() balance = balance + n time.sleep(5) balance = balance - n lock.release()threads = [ threading.Thread(target=change_it_without_lock, args=(8,) ), threading.Thread(target=change_it_without_lock, args=(10,) )][t.start() for t in threads][t.join() for t in threads] recursive lockMutex can be divided into recursive lock and non recursive lock. Recursive lock is also called reentrant lock, and non recursive lock is also called non reentrant lock.The same thread can acquire the same recursive lock multiple times without deadlock. If a thread acquires the same non recursive lock multiple times, a deadlock will occur.why we need Rlock Semaphoremutex is special case of Semaphore with value 1 1234567891011121314 import threading import timesemaphore = threading.Semaphore(10)def foo(): semaphore.acquire() print('OK') time.sleep(1) semaphore.release()for i in range(100): t = threading.Thread(target=foo,args=()) t.start() Condition The typical programming style using condition variables uses the lock to synchronize access to some shared state; threads that are interested in a particular change of state call wait() repeatedly until they see the desired state, while threads that modify the state call notify() or notify_all() when they change the state in such a way that it could possibly be a desired state for one of the waiters.Here are functions: 123456789101112 acquire(*args) - Acquire the underlying lock. release()- Release the underlying lock. wait(timeout=None)- Wait until notified or until a timeout occurs. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised. wait_for(predicate, timeout=None)- Wait until a condition evaluates to true. predicate should be a callable which result will be interpreted as a boolean value. A timeout may be provided giving the maximum time to wait. notify(n=1) - By default, wake up one thread waiting on this condition, if any. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised. notify_all()Wake up all threads waiting on this condition. This method acts like notify(), but wakes up all waiting threads instead of one. If the calling thread has not acquired the lock when this method is called, a RuntimeError is raised. Here is an example of condition Leetcode 1188. Design Bounded Blocking Queue EventThe event of threading is used to control the execution of other threads by the main thread. The event mainly provides three methods: wait, clear and set.Event handling mechanism: a “flag” is defined globally. If the value of “flag” is false, then when the program executes event.wait Method, if the flag value is true, then event.wait Method is not blocked.Here are some common functions:1234event.wait (timeout = none): the thread calling the method will be blocked. If the timeout parameter is set, the thread will stop blocking and continue to execute after timeout;event.set (): if the flag of event is set to true, all threads calling the wait method will be awakened;event.clear (): if the flag of event is set to false, all threads calling the wait method will be blocked;event.isSet (): judge whether the flag of event is true. Here is an application of it.123456789101112131415161718import threading,timeevent = threading.Event() # default value is falsedef foo(): while not event.is_set(): print('wait.....') event.wait(2) print('Connect to redis server')for i in range(5): t = threading.Thread(target=foo,args=()) t.start()print('attempt to start redis server')time.sleep(10)event.set() # set the flag as True multiprocessing in pythonUnlike multithreading, The multiprocessing in Python is real.We always use multiprocessing.Let we look at it: basic exmaple 1234567891011121314151617181920212223import timefrom multiprocessing import Processimport osdef run(name): while True: print(\" the child %s with pid run：%s\"%(name,os.getpid())) print(\"the id of the parent is %d\"%os.getppid()) time.sleep(2)if __name__ == \"__main__\": print(\"the parent process %d starts\" % os.getpid()) # create the child process and pass new arguments p = Process(target=run, args=(\"Ail\",)) p.start() # start child process while True: print(\"this is forever loop for see process status\") time.sleep(1) The output is like 1234the parent process 20686 startsthis is forever loop for see process statusthe child Ail with pid run：20687the id of the parent is 20686 We could use command ps aux | egrep '(python)'We could see script like that12koko 20687 0.0 0.0 4284264 1312 s001 S+ 7:25PM 0:00.08 python /Users/koko/Documents/prep/blog/test.pykoko 20686 0.0 0.0 4284264 2900 s001 S+ 7:25PM 0:00.16 python /Users/koko/Documents/prep/blog/test.py join function - make parent process wait the child process to end 1234567891011121314151617from multiprocessing import Processimport osdef run(): print(\"child process starts\") time.sleep(2) print(\"child process ends\")if __name__ == \"__main__\": print(\"parent process starts\") p = Process(target=run) p.start() p.join() # wait children process print(\"parent process ends\") Pool to handle lots of child process 123456789101112131415161718192021222324252627import randomfrom multiprocessing.pool import Poolfrom time import sleep, timeimport osdef run(name): print(\"child process named %s starts,and its ID is %d\" % (name, os.getpid())) start = time() sleep(random.choice([1, 2, 3, 4])) end = time() print(\"child process named %s starts,and its ID is %d. It spend times .2%f\" % (name, os.getpid(), end-start))if __name__ == \"__main__\": print(\"parent process starts\") # set size of pool, the default value is the number of cores p = Pool(8) for i in range(10): # put into pool p.apply_async(run, args=(i,)) # we must call close before join and close will let all no other process can joined p.close() # wait all the child process to end p.join() print(\"parent process ends\") the Communication of child process and parent process They cannot share the global variable. We will use Queue in multiprocessing. Queue will offer methods to write and read.Both end of queue can read and write.Here is an example 1234567891011121314151617181920from multiprocessing import Process, Queueimport os, timedef child1(w_value,q): q.put(w_value) r = q.get()# it will block print(\"child 1 read\",r)def child2(w_value,q): r = q.get() print(\"child 2 read \",r) q.put(w_value)if __name__ == \"__main__\": q = Queue() c1 = Process(target=child1, args=(1,q,)) c2 = Process(target=child2, args=(2,q,)) c1.start() c2.start() other functions in queue 123Queue.qsize() # return the size of queue Queue.empty() # check wether the queue is empty Queue.get_nowait() # Return an item if one is immediately available, else raise QueueEmpty. Another class Pipe can be used for this too. Also shared memory can be used to handle this problem. Data can be stored in a shared memory map using Value or Array. For example, the following code 1234567891011121314151617from multiprocessing import Process, Value, Arraydef f(n, a): n.value = 3.1415927 for i in range(len(a)): a[i] = -a[i]if __name__ == '__main__': num = Value('d', 0.0) arr = Array('i', range(10)) p = Process(target=f, args=(num, arr)) p.start() p.join() print(num.value) print(arr[:]) Lock we could use a lock to ensure Synchronization 1234567891011121314from multiprocessing import Process, Lockdef f(l, i): l.acquire() try: print('hello world', i) finally: l.release()if __name__ == '__main__': lock = Lock() for num in range(10): Process(target=f, args=(lock, num)).start() other functions count the number of core of computers1print('the number of cores is ' + str(multiprocessing.cpu_count())) All currently running child processes123456789101112import multiprocessingfrom multiprocessing import Processimport os def run(name): while True: print(\" the child %s with pid run：%s\"%(name,os.getpid())) print(\"the id of the parent is %d\"%os.getppid()) time.sleep(2)p = Process(target=run, args=(\"Ail\",))p.start()for p in multiprocessing.active_children(): print('the child process name is ' + p.name + ' id: ' + str(p.pid)) refhttps://stackoverflow.com/questions/44793371/is-multithreading-in-python-a-myth#:~:text=No%2C%20Python%20does%20have%20multithreading,GIL(Global%20Interpreter%20Lock).http://c.biancheng.net/view/5537.htmlhttps://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p09_dealing_with_gil_stop_worring_about_it.htmlhttps://www.jianshu.com/p/a5f10c152c20","link":"/2020/12/07/Multiprocessing-Vs-Threading/"},{"title":"check cycles in directed graph","text":"There are two ways to check whether there are cycles in the directed graph DFSThe first methos is use DFS to check whether the node will be visited.One trick is to use visited set to record all the123456789101112131415161718192021222324252627282930def checkCycle1(graph): in_degrees = dict((u,0) for u in graph) #初始化所有顶点入度为0 num = len(in_degrees) for u in graph: for v in graph[u]: in_degrees[v] += 1 #计算每个顶点的入度 start_node = [u for u in in_degrees if in_degrees[u] == 0] # 筛选入度为0的顶点 visited = set() def helper(n,stack): visited.add(n) if n not in graph: return False for neighbour in graph[n]: if n in stack: return True elif helper(neighbour,stack +[n]): return True return False flag = False for node in start_node: if node not in visited: visited.add(node) if helper(node,[]): print(\"there is a cycle \") return; print(\"there is no cycle\")checkCycle1({1:[2],2:[3],3:[]}) Topological sortingTo see the detail about Topological sorting can be seen in Topological-sortingThe code can be seen as follows:1234567891011121314151617181920def checkCycle2(graph): in_degrees = dict((u,0) for u in graph) #初始化所有顶点入度为0 num = len(in_degrees) for u in graph: for v in graph[u]: in_degrees[v] += 1 #计算每个顶点的入度 Q = [u for u in in_degrees if in_degrees[u] == 0] # 筛选入度为0的顶点 Seq = [] while Q: u = Q.pop() #默认从最后一个删除 Seq.append(u) for v in graph[u]: in_degrees[v] -= 1 #移除其所有出边 if in_degrees[v] == 0: Q.append(v) #再次筛选入度为0的顶点 if len(Seq) == num: #输出的顶点数是否与图中的顶点数相等 print(\"there is no cycle\") else: print(\"there is a cycle\")checkCycle2({1:[2],2:[1],3:[]}) refhttps://zhuanlan.zhihu.com/p/69858335","link":"/2020/12/24/check-cycles-in-directed-graph/"},{"title":"shortest path algorithm","text":"When we want to get the shortest from source node to dest node.Here are some ways and corresponding application scenarios. dijkstra algorithm-Suitable for:1.single source node shortest path2.the weight cannot be negative implemention1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import sys class Graph(): def __init__(self, vertices): self.V = vertices self.graph = [[0 for column in range(vertices)] for row in range(vertices)] def printSolution(self, dist): print (\"Vertex tDistance from Source\") for node in range(self.V): print (node, \"t\", dist[node]) # A utility function to find the vertex with # minimum distance value, from the set of vertices # not yet included in shortest path tree def minDistance(self, dist, sptSet): # Initilaize minimum distance for next node min = sys.maxsize # Search not nearest vertex not in the # shortest path tree for v in range(self.V): if dist[v] &lt; min and sptSet[v] == False: min = dist[v] min_index = v return min_index # Funtion that implements Dijkstra's single source # shortest path algorithm for a graph represented # using adjacency matrix representation def dijkstra(self, src): dist = [sys.maxsize] * self.V dist[src] = 0 sptSet = [False] * self.V for cout in range(self.V): # Pick the minimum distance vertex from # the set of vertices not yet processed. # u is always equal to src in first iteration u = self.minDistance(dist, sptSet) # Put the minimum distance vertex in the # shotest path tree sptSet[u] = True # Update dist value of the adjacent vertices # of the picked vertex only if the current # distance is greater than new distance and # the vertex in not in the shotest path tree for v in range(self.V): if self.graph[u][v] &gt; 0 and \\ sptSet[v] == False and \\ dist[v] &gt; dist[u] + self.graph[u][v]: dist[v] = dist[u] + self.graph[u][v] self.printSolution(dist) # Driver program g = Graph(9) g.graph = [[0, 4, 0, 0, 0, 0, 0, 8, 0], [4, 0, 8, 0, 0, 0, 0, 11, 0], [0, 8, 0, 7, 0, 4, 0, 0, 2], [0, 0, 7, 0, 9, 14, 0, 0, 0], [0, 0, 0, 9, 0, 10, 0, 0, 0], [0, 0, 4, 14, 10, 0, 2, 0, 0], [0, 0, 0, 0, 0, 2, 0, 1, 6], [8, 11, 0, 0, 0, 0, 1, 0, 7], [0, 0, 2, 0, 0, 0, 6, 7, 0] ]; g.dijkstra(0); Floyd algorithm Suitable for 1.shortest distances between every pair of vertices2.cannot deal with negative cycles( because it does not have the shortest path) logichttps://www.cnblogs.com/wangyuliang/p/9216365.html implementation 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# Python Program for Floyd Warshall Algorithm # Number of vertices in the graph V = 4 # Define infinity as the large enough value. This value will be # used for vertices not connected to each other INF = 99999 # Solves all pair shortest path via Floyd Warshall Algorithm def floydWarshall(graph): \"\"\" dist[][] will be the output matrix that will finally have the shortest distances between every pair of vertices \"\"\" \"\"\" initializing the solution matrix same as input graph matrix OR we can say that the initial values of shortest distances are based on shortest paths considering no intermediate vertices \"\"\" dist = map(lambda i : map(lambda j : j , i) , graph) \"\"\" Add all vertices one by one to the set of intermediate vertices. ---&gt; Before start of an iteration, we have shortest distances between all pairs of vertices such that the shortest distances consider only the vertices in the set {0, 1, 2, .. k-1} as intermediate vertices. ----&gt; After the end of a iteration, vertex no. k is added to the set of intermediate vertices and the set becomes {0, 1, 2, .. k} \"\"\" for k in range(V): # pick all vertices as source one by one for i in range(V): # Pick all vertices as destination for the # above picked source for j in range(V): # If vertex k is on the shortest path from # i to j, then update the value of dist[i][j] dist[i][j] = min(dist[i][j] , dist[i][k]+ dist[k][j] ) printSolution(dist) # A utility function to print the solution def printSolution(dist): print(\"Following matrix shows the shortest distances\\ between every pair of vertices\") for i in range(V): for j in range(V): if(dist[i][j] == INF): print(\"%7s\" %(\"INF\")) else: print(\"%7d\\t\" %(dist[i][j])) if j == V-1: print(\"\")# Driver program to test the above program # Let us create the following weighted graph \"\"\" 10 (0)-------&gt;(3) | /|\\ 5 | | | | 1 \\|/ | (1)-------&gt;(2) 3 \"\"\"graph = [[0,5,INF,10], [INF,0,3,INF], [INF, INF, 0, 1], [INF, INF, INF, 0] ] # Print the solution floydWarshall(graph); # This code is contributed by Nikhil Kumar Singh(nickzuck_007) SPFA Suitable for : graph with negative cycles2.detailshttps://zhuanlan.zhihu.com/p/58727559 implementation 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# Python3 implementation of SPFA from collections import deque # Graph is stored as vector of vector of pairs # first element of pair store vertex # second element of pair store weight graph = [[] for _ in range(100000)] # Function to add edges in the graph # connecting a pair of vertex(frm) and weight # to another vertex(to) in graph def addEdge(frm, to, weight): graph[frm].append([to, weight]) # Function to prshortest distance from source def print_distance(d, V): print(\"Vertex\",\"\\t\",\"Distance from source\") for i in range(1, V + 1): print(i,\"\\t\",d[i]) # Function to compute the SPF algorithm def shortestPathFaster(S, V): # Create array d to store shortest distance d = [10**9]*(V + 1) # Boolean array to check if vertex # is present in queue or not inQueue = [False]*(V + 1) d[S] = 0 q = deque() q.append(S) inQueue[S] = True while (len(q) &gt; 0): # Take the front vertex from Queue u = q.popleft() inQueue[u] = False # Relaxing all the adjacent edges of # vertex taken from the Queue for i in range(len(graph[u])): v = graph[u][i][0] weight = graph[u][i][1] if (d[v] &gt; d[u] + weight): d[v] = d[u] + weight # Check if vertex v is in Queue or not # if not then append it into the Queue if (inQueue[v] == False): q.append(v) inQueue[v] = True # Print the result print_distance(d, V) # Driver code if __name__ == '__main__': V = 5 S = 1 # Connect vertex a to b with weight w # addEdge(a, b, w) addEdge(1, 2, 1) addEdge(2, 3, 7) addEdge(2, 4, -2) addEdge(1, 3, 8) addEdge(1, 4, 9) addEdge(3, 4, 3) addEdge(2, 5, 3) addEdge(4, 5, -3) # Calling shortestPathFaster function shortestPathFaster(S, V) # This code is contributed by mohit kumar 29","link":"/2020/12/29/shortest-path-algorithm/"},{"title":"debugging in python","text":"There are mainly two ways to debug in python.The first one is use print directly.The other method is to use ipdb ipdbthe IPython-enabled Python Debugger, is a third party interative debugger with all pdb’s functionality and adds IPython support for the interactive shell, like tab completion, color support, magic functions and much more.we could use ipdb.set_trace() to insert breakpoint into code directly. help 123ipdb&gt; h # show all the commandsipdb&gt; h &lt;command name&gt; # show specific info of that command view 12345ipdb&gt; l #The default display is 5 lines before and after the current code positionipdb&gt; ll # show all the linesipdb&gt; l &lt;line number n1&gt; ,&lt;line number n2&gt; # show all the lines from line n1 to line n2 breakpoint1234567ipdb&gt; b &lt;line number n1&gt; # create a breakpoint at line n1ipdb&gt; disable &lt;line number n1&gt; # disable the breakpoint at line n1ipdb&gt; enable &lt;line number n1&gt; # enable the breakpoint at line n1ipdb&gt; cl &lt;line number n1&gt; # clear the breakpoint at line n1 execute 1234567891011ipdb&gt; n # n simply continues program execution to the next line in the current methodipdb&gt; s # steps to the very next line of executable code, whether it is inside a called method or just on the next lineipdb&gt; w # Print a stack trace, with the most recent frame at the bottom.An arrow indicates the \"current frame\", which determines the context of most commands. 'bt' is an alias for this command.ipdb&gt; c # c continues program execution until another breakpoint is hit or the program execution completesipdb&gt; a # a prints out all the arguments the current function receivedipdb&gt; r # continues execution until the current function returns","link":"/2021/05/09/debugging-in-python/"},{"title":"notes of  nginx learning (1)","text":"what is nginxNGINX is open source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. It started out as a web server designed for maximum performance and stability. In addition to its HTTP server capabilities, NGINX can also function as a proxy server for email (IMAP, POP3, and SMTP) and a reverse proxy and load balancer for HTTP, TCP, and UDP servers.main purpose: High performance static server reverse proxywhat is reverse proxy 12345678910 http request client ------------&gt; nginx --------&gt;http server &lt;------------ &lt;-------- http response imcp/pop request client ------------&gt; nginx --------&gt;mail server &lt;------------ &lt;-------- imcp/pop response why we need nginx Data volume grow, single-core cpu to multicore cpu Apache is inefficient in processing requests（ not suitable for concurrency,multi-core cpu) Apache vs Nginx Apache Nginx one process handles one request one process handles multiple request blocking non-blocking Common application scenarios static resourcesProvide services through the local file system reverse proxying （handle dynamic resources) high performance of nginx Cache acceleration Load Balance API serviceOpenResty Advantages of Nginx High concurrency and high performance Good scalability,Low coupling. asynchronous non-blocking event-driven model high reliability Business continuity（no need to shut down the machine to update the resources) Allow secondary development course:https://coding.imooc.com/class/chapter/405.html#Anchor","link":"/2021/08/23/notes-of-nginx-learning-1/"},{"title":"notes of nginx learning (2)","text":"multprocess vs multithreadmultithread Shared address space why nginx use mutliprocess rather than vsmultithreadneed robustness，if one thread ruin the address space, it will ruin the entire multithread program structure chart of nginx process 1234567master process cache manager (CM) cache loader(CL) worker process 1 worker process 2 worker process 3 ... master process doesn’t actually handle users’ requests It only manager the monitor the child processes. This design is good for hot deployment. when developing third-party module, it is better not modify the master process. Because if we cause error in the master process, it will influence the whole nginx. The communication between the child processes is through shared memory Which process can be managed through semaphore master process monitor worker process (CHLD) manage worker process receive semaphore(TERM,INT,HUP,USE1,USE2,WINCH) semophore command line fuction QUIT kill -3 $pid or kill -s SIGQUIT $pid graceful shutdown TERM, INT kill -15 $pid fast shutdown HUP kill -1 $pid changing configuration, keeping up with a changed time zone (only for FreeBSD and Linux), starting new worker processes with a new configuration, graceful shutdown of old worker processes USR1 kill -10 $pid re-opening log files USR2 kill -12 $pid upgrading an executable file WINCH kill -28 $pid graceful shutdown of worker processes worker process receive semaphore(TERM,INT,HUP,USE1,USE2,WINCH) semophore command line fuction TERM, INT kill -15 $pid fast shutdown QUIT kill -3 $pid graceful shutdown HUP kill -1 $pid changing configuration, keeping up with a changed time zone (only for FreeBSD and Linux), starting new worker processes with a new configuration, graceful shutdown of old worker processes USR1 kill -10 $pid re-opening log files USR2 kill -12 $pid upgrading an executable file WINCH kill -28 $pid abnormal termination for debugging (requires debug_points to be enabled) - It is not recommended. Because the master process will monitor all the worker process, if one worker process receive the SIGNTERM semaphore, even though this worker process will shut down, the master process will recreate a new worker process commend line reload (HUP) reopen(USE1) stop(TERM) quit(QUIT)example:nginx -s reloadIt is the same as send semaphore to master process reload process in semaphore Send HUP semaphore or use reload in command line Check if the syntax of the configuration file is correct master process will open new listening port master process will use the new configuration to generate new worker process(It is possible that old and new exists at the same time) master process sends the quit semaphore to previous worker process The old worker process will process the work task and disconnect and quit the process of hot upgrading replace old nginx file to new nginx file （not change the directory) send USR2 signal to master process master process modify the pid file by adding suffix .oldbin master process uses new nginx file to start a new master process(we need to verify that the new master process works well ) sends WINCH signal to old master make old work processes shut down. It will make the new request go to the new master ROLLBACK situation: (if the new master process not work) Send HUB signal to old master while sending QUIT to new master -REFER course:https://coding.imooc.com/class/chapter/405.html#Anchorhttps://en.wikipedia.org/wiki/Signal_(IPC)","link":"/2021/08/24/notes-of-nginx-learning-2/"},{"title":"notes of nginx learning (3)","text":"Nginx modular designHigh cohesion and low coupling modular structure core modular ngx_core ngx_errlog ngx_conf ngx_events ngx_epoll ngx_regex ngx_event standard http modular ngx_http_core ngx_http_charset others optional http modular ngx_http_gzip ngx_http_ssl others mail service modular ngx_mail_core ngx_mail_pop3 others third-party service modular rds_json_nginx lua_nginx others","link":"/2021/09/01/notes-of-nginx-learning-3/"},{"title":"notes of nginx learning (4)","text":"Configuration parameters for Nginx compilation and installation parameter meaning default value –prefix=*path* Specify the installation directory /usr/local/nginx –sbin-path=*path* sets the name of an NGINX executable file. This name is used only during installation. prefix/sbin/nginx. –conf-path=*path* sets the name of an nginx.conf configuration file prefix/conf/nginx.conf –pid-path=*path* sets the name of an nginx.pid file that will store the process ID of the main process. prefix/logs/nginx.pid –error-log-path=*path* sets the name of the primary error, warnings, and diagnostic file. prefix/logs/error.log. –http-log-path=*path* sets the name of the primary request log file of the HTTP server. prefix/logs/access.log –user=*name* sets the name of an unprivileged user whose credentials will be used by worker processes. nobody –group=*name* sets the name of a group whose credentials will be used by worker processes. a group name is set to the name of an unprivileged user –with-pcre=*path* sets the path to the sources of the PCRE library. The library is required for regular expressions support in the location directive and for the ngx_http_rewrite_module. –with-zlib=*path* sets the path to the sources of the zlib library.The library is required for the ngx_http_gzip_module. The parameter --with-$module is to add the module when compiling while --without-$module is compiling without that module. The default situation is without. Install and configure nginx in Mac os Download the source packageNginx http://nginx.org/en/download.htmlzlib http://zlib.net/pcre http://www.pcre.org/openssl https://www.openssl.org/source/ note: for pcre, we don’t use pcre2. Otherwise, we will meet the error src/core/ngx_regex.h:15:10: fatal error: 'pcre.h' file not found unzip the package 1234tar xf nginx-1.21.1.tar.gztar xf pcre-8.45.tar.gztar xf zlib-1.2.11.tar.gztar xf openssl-1.1.1l.tar.gz Now we can see the configuration file nginx-1.21.1/And to see the parameters of configuration we could 12cd nginx-1.21.1.tar.gz./configure --help understand the configure scripthttps://en.wikipedia.org/wiki/Configure_script compile and install Nginx 123./configure --with-zlib=../zlib-1.2.11 --with-pcre=../pcre-8.45 --with-openssl=../openssl-1.1.1lmakemake install And we see the configuration summary like that123456789101112131415161718Configuration summary + using PCRE library: ../pcre-8.45 + using OpenSSL library: ../openssl-1.1.1l + using zlib library: ../zlib-1.2.11 nginx path prefix: &quot;/usr/local/nginx&quot; nginx binary file: &quot;/usr/local/nginx/sbin/nginx&quot; nginx modules path: &quot;/usr/local/nginx/modules&quot; nginx configuration prefix: &quot;/usr/local/nginx/conf&quot; nginx configuration file: &quot;/usr/local/nginx/conf/nginx.conf&quot; nginx pid file: &quot;/usr/local/nginx/logs/nginx.pid&quot; nginx error log file: &quot;/usr/local/nginx/logs/error.log&quot; nginx http access log file: &quot;/usr/local/nginx/logs/access.log&quot; nginx http client request body temporary files: &quot;client_body_temp&quot; nginx http proxy temporary files: &quot;proxy_temp&quot; nginx http fastcgi temporary files: &quot;fastcgi_temp&quot; nginx http uwsgi temporary files: &quot;uwsgi_temp&quot; nginx http scgi temporary files: &quot;scgi_temp&quot; Run nginx1sudo /usr/local/nginx/sbin/nginx Check whether the nginx is successfully started by accessing 127.0.0.1And also we can use ps -ef| grep nginx And we could see other command of nginx by sudo /usr/local/nginx/sbin/nginx -h123456789101112Options: -?,-h : this help -v : show version and exit -V : show version and configure options then exit -t : test configuration and exit -T : test configuration, dump it and exit -q : suppress non-error messages during configuration testing -s signal : send signal to a master process: stop, quit, reopen, reload -p prefix : set prefix path (default: /usr/local/nginx/) -e filename : set error log file (default: logs/error.log) -c filename : set configuration file (default: conf/nginx.conf) -g directives : set global directives out of configuration file the structure of configuration filewe could find the configuration file in /usr/local/nginx/conf/nginx.conf The structure of configuration file can be seen as main blockConfigure directives that affect nginx globally. Generally there are user groups running the nginx server, nginx process pid storage paths, log storage paths,importing configuration file , number of worker processes allowed to be generated, etc. event blockConfigure the network connections that affect the nginx server or the user. There is a maximum number of connections per process, which event-driven model to pick to handle connection requests, whether to allow multiple network connections to be accepted at the same time, enable serialization of multiple network connections, etc. http blockYou can nest multiple server blocks, configure most of the features such as proxy, cache, log definition and third-party module configuration. Such as file introduction, mime-type definition, log customization, whether to use sendfile to transfer files, connection timeout, number of single connection requests, etc. server blockConfigure virtual host parameters.Each http block can contain multiple server blocks, and each server block is equivalent to a virtual host, which can have multiple hosts jointly providing services, together with a set of services that are logically close to each other externally. location blockConfigure the routing of requests and the processing of various pages modify the nginx.conf We could see user permission of nginx.conf by ls -llAnd to modify it, we need sudo emacs nginx.conf To make nginx.conf take effect, we need to reload it bysudo /usr/local/nginx/sbin/nginx -s reload To check the grammer of the config file we could sudo /usr/local/nginx/sbin/nginx -t refer https://www.nginx.com/resources/wiki/start/topics/tutorials/installoptions/https://www.cnblogs.com/54chensongxia/p/12938929.html#:~:text=Nginx%E7%9A%84%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6,%E9%97%B4%E4%B8%8D%E5%AD%98%E5%9C%A8%E6%AC%A1%E5%BA%8F%E5%85%B3%E7%B3%BB%E3%80%82http://nginx.org/en/docs/ngx_core_module.htmlhttps://nginx.org/en/docs/dirindex.html","link":"/2021/09/01/notes-of-nginx-learning-4/"},{"title":"notes of nginx learning (5)","text":"virtual hostThe virtual hosthttps://www.youtube.com/watch?v=MmMEbLCt_-cThe category of virtual host Virtual host based on IP Virtual host based on domain name Virtual host based on IPIt could be multiple IPs and ports.We will show an example with multiple ports. we need to modify nginx.conf by adding more server blocks because one server block corresponds to one virtual host.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061worker_processes auto;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name localhost; location / { #speicify the path for the virtual host root html/port_virtual_host1; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } # add a new server block server { listen 8080; server_name localhost; location / { root html/port_virtual_host2; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } # add a new server block server { listen 8081; server_name localhost; location / { root html/port_virtual_host3; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } }} After that, we need to reload the config file make the index.html for each virtual host 1234567cd /usr/local/nginx/htmlsudo mkdir port_virtual_host1sudo mkdir port_virtual_host2sudo mkdir port_virtual_host3sudo touch port_virtual_host1/index.html `sudo cp port_virtual_host1/index.html port_virtual_host2sudo cp port_virtual_host1/index.html port_virtual_host3 And then write some codes into all the index.html check whether it worksvisit localhost:80, localhost:8080,localhost:8081 Virtual host based on Server name we need to modify nginx.conf by adding more server blocks because one server block corresponds to one virtual host. 12345678910111213141516171819202122232425262728293031323334353637383940414243worker_processes auto;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name www.nginx_test1.com; location / { root html/servername_1; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } server { listen 80; server_name www.nginx_test2.com; location / { root html/servername_2; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } }} After that, we need to reload the config file mofify local host filewe need to open the host file firstly sudo atom /etc/hostsAnd then add the mapping from server name to ip address 12$myipaddress www.nginx_test1.com$myipaddress www.nginx_test2.com make the index.html for each virtual host 12345cd /usr/local/nginx/htmlsudo mkdir servername_1sudo mkdir servername_2sudo touch servername_1/index.html `sudo cp servername_1/index.html servername_2 And then write some codes into all the index.html check our setting is successfullyvisit www.nginx_test1.com and www.nginx_test2.com Details of server_name directive how to use server_name server_name name1 name2 ...;Server names can include an asterisk (“*”) replacing the first or last part of a name:server { server_name example.com *.example.com www.example.*; }It is also possible to use regular expressions in server names, preceding the name with a tilde (“~”):server { server_name www.example.com ~^www\\d+\\.example\\.com$; } what if we don’t specific the server name It will be regarded as the default server listening on that port. When Nginx receives an HTTP request on a port, it is handed off to the server listening on that port. If there are multiple servers listening on that port, it depends on the match order. If there is no matching server name, or if the Host in the request header is an IP address, it is handed off to the default server listening on that port. match priority In some cases, we could find match matched server_name. For example, we have the nginx.conf 123456789101112server { server_name www.nginx.org}server{ server_name www.nginx.*}server{ server_name *.nginx.org}server{ server_name ~^www\\d+\\.nginx\\.com$;} If the request from www.nginx.org,which one it will match.Actually, there is match priority. exact name longest wildcard name starting with an asterisk, e.g. *.example.org longest wildcard name ending with an asterisk, e.g. mail.* first matching regular expression (in order of appearance in a configuration file) referencehttps://blog.csdn.net/qq_35952638/article/details/100163824","link":"/2021/09/01/notes-of-nginx-learning-5/"},{"title":"notes of nginx learning (6)","text":"common directives in main block useruser [user] [group];Specify users and user groups that can run nginx servicesComment the user directive, or configure it to nobody so that all users can run itexample:user nobody nobody; pidpid file;Defines a file that will store the process ID of the main process.example:pid logs/nginx.pid worker_rlimit_nofileworker_rlimit_nofile number;Changes the limit on the maximum number of open files (RLIMIT_NOFILE) for worker processes. Used to increase the limit without restarting the main process. worker_rlimit_coreworker_rlimit_core size;Changes the limit on the largest size of a core file (RLIMIT_CORE) for worker processes. Used to increase the limit without restarting the main process. worker_processesworker_processes number | auto;The optimal value depends on many factors including (but not limited to) the number of CPU cores, the number of hard disk drives that store data, and load pattern. When one is in doubt, setting it to the number of available CPU cores would be a good start (the value “auto” will try to autodetect it). worker_cpu_affinityworker_cpu_affinity cpumask ...;worker_cpu_affinity auto [cpumask];Binds worker processes to the sets of CPUs. Each CPU set is represented by a bitmask of allowed CPUs. There should be a separate set defined for each of the worker processes. By default, worker processes are not bound to any specific CPUs.example: 12worker_processes 4; worker_cpu_affinity 0001 0010 0100 1000; binds each worker process to a separate CPU, worker_priorityworker_processes number | auto;Defines the number of worker processes. worker_shutdown_timeoutworker_shutdown_timeout time;Configures a timeout for a graceful shutdown of worker processes. When the time expires, nginx will try to close all the connections currently open to facilitate shutdown. timer_resolutiontimer_resolution interval;Reduces timer resolution in worker processes, thus reducing the number of gettimeofday()system calls made. By default,gettimeofday() is called each time a kernel event is received. With reduced resolution, gettimeofday() is only called once per specified interval.example:timer_resolution 100ms; daemondaemon on | off;Determines whether nginx should become a daemon. Mainly used during development. lock_filelock_file file;nginx uses the locking mechanism to implement accept_mutex and serialize access to shared memory. On most systems the locks are implemented using atomic operations, and this directive is ignored. On other systems the “lock file” mechanism is used. This directive specifies a prefix for the names of lock files.example:lock_file logs/nginx.lock; common directives in event block useuse method;Specifies the connection processing method to use. There is normally no need to specify it explicitly, because nginx will by default use the most efficient method. worker_connectionsworker_connections number;Sets the maximum number of simultaneous connections that can be opened by a worker process. accept_mutexaccept_mutex on | off;When only one request arrives at a given moment, multiple sleeping processes are woken up at the same time, but only one process can get a connection. If the number of processes waking up at a time is too large, this can affect some of the system performance. This can be a problem with multiple processes on an Nginx server.If accept_mutex is enabled, worker processes will accept new connections by turn. Otherwise, all worker processes will be notified about new connections.The default value is on accept_mutex_delayaccept_mutex_delay time;If accept_mutex is enabled, specifies the maximum time during which a worker process will try to restart accepting new connections if another worker process is currently accepting new connections. multi_acceptmulti_accept on | off;If multi_accept is disabled, a worker process will accept one new connection at a time. Otherwise, a worker process will accept all new connections at a time. other directives in location block the diff of root and alias root path; Sets the root directory for requests. alias path; Defines a replacement for the specified location. The diff of two directives can be seen： root context:http, server, location, if in location while alias context:location for root no need to end with /, for alias, must end with / path if the setting for root is 123location /picture{ root /opt/nginx/html/picture } if we request a picture wwww.test.com/picture/1.jpg,we will find the picture in /opt/nginx/html/picture/picture/1.jpg if the setting for alias is 123location /picture{ alias /opt/nginx/html/picture } if we request a picture wwww.test.com/picture/1.jpg,we will find the picture in /opt/nginx/html/picture/1.jpg the usage of location location [ = | ~ | ~* | ^~ ] uri { ... }location @name { ... }Sets configuration depending on a request URI.The matching is performed against a normalized URI, after decoding the text encoded in the “%XX” form, resolving references to relative path components “.” and “..”, and possible compression of two or more adjacent slashes into a single slash.Nginx location block section have a search order, a modifier, an implicit match type and an implicit switch to whether stop the search on match or not. the following array describe it for regex. 1234567891011Nginx location:# --------------------------------------------------------------------------------------------------------------------------------------------# Search-Order Modifier Description Match-Type Stops-search-on-match# --------------------------------------------------------------------------------------------------------------------------------------------# 1st = The URI must match the specified pattern exactly Simple-string Yes# 2nd ^~ The URI must begin with the specified pattern Simple-string Yes# 3rd (None) The URI must begin with the specified pattern Simple-string No# 4th ~ The URI must be a case-sensitive match to the specified Rx Perl-Compatible-Rx Yes (first match) # 4th ~* The URI must be a case-insensitive match to the specified Rx Perl-Compatible-Rx Yes (first match)# N/A @ Defines a named location block. Simple-string Yes# -------------------------------------------------------------------------------------------------------------------------------------------- And here is pseudo-code from https://github.com/detailyang/nginx-location-match-visible to help understand the order 12345678910111213141516def match(uri): rv = None if uri in exact_match: return exact_match[uri] if uri in prefix_match: if prefix_match[uri] is '^~': return prefix_match[uri] else: rv = prefix_match[uri] if uri in regex_match: return regex_match[uri] return rv Note that there is a difference between path ends with / and path ends without /For example, we have two similar location blocks1234567# location block 1location /test {}# location block 2location /test/{}For the first location block, it will first find file in the path /test/index.html(It will treat /test as directory). If cannot find, it will find the file in the path /test(It will treat /test as file).For the first location block, it will first find file in the path /test/index.html(It will treat /test as directory). If cannot find, it will return 404 rather than treating /test as file. the usage of stub_status If we want to use directive stub_status, we need to recompile our nginx with it. 123./configure --with-zlib=../zlib-1.2.11 --with-pcre=../pcre-8.45 --with-openssl=../openssl-1.1.1l --with-http_stub_status_modulemakemake install If not, we will face error unknown directive &quot;stub_status&quot; we need to modify the nginx.conf 123location /stub_status { stub_status;} The context of stub_status: server, locationAlso, we need to reload the configuration. use stub_status stub_status will make basic status information will be accessible from the surrounding location.In that case, we could visit www.nginx_test1.com/stub_status (we set the server_name is www.nginx_test1.com)And the page look like:1234Active connections: 1server accepts handled requests 2 2 4Reading: 0 Writing: 1 Waiting: 0 It contains following status information status information details Active connections The current number of active client connections including Waiting connections. accepts The total number of accepted client connections. handled The total number of handled connections. Generally, the parameter value is the same as accepts unless some resource limits have been reached (for example, the worker_connections limit). requests The total number of client requests. Reading The current number of connections where nginx is reading the request header. Writing The current number of connections where nginx is writing the response back to the client. Waiting The current number of idle client connections waiting for a request refer https://stackoverflow.com/questions/59846238/guide-on-how-to-use-regex-in-nginx-location-block-sectionhttps://github.com/detailyang/nginx-location-match-visible","link":"/2021/09/07/notes-of-nginx-learning-6/"},{"title":"notes of nginx learning (7)","text":"Look into Module ngx_http_limit_conn_module The ngx_http_limit_conn_module module is used to limit the number of connections per the defined key, in particular, the number of connections from a single IP address.Not all connections are counted. A connection is counted only if it has a request being processed by the server and the whole request header has already been read.In default, the nginx is compiled with http_limit_conn_module . If we don’t want that, we need to use --without-http_limit_conn_module.This module uses shared memory for every worker processNow, let’s look at commonly used directives. limit_conn_zone limit_conn_zone key zone=name:size;Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state includes the current number of connections. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.Context: httpFor example:limit_conn_zone $binary_remote_addr zone=addr:10m;where $binary_remote_addr is the embedded variable which is client address in a binary form, value’s length is always 4 bytes for IPv4 addresses or 16 bytes for IPv6 addresses limit_conn_status limit_conn_status code;Sets the status code to return in response to rejected requests.Default: limit_conn_status 503;Context: http, server, location limit_conn_log_level info limit_conn_log_level info | notice | warn | error;Sets the desired logging level for cases when the server limits the number of connections.Default: limit_conn_log_level error;Context: http, server, location limit_conn limit_conn zone number;Sets the shared memory zone and the maximum allowed number of connections for a given key value. When this limit is exceeded, the server will return the error in reply to a requestContext:http, server, locationFor example,123456limit_conn_zone $binary_remote_addr zone=addr:10m;server { location /download/ { limit_conn addr 1; }allow only one connection per an IP address at a time. Another example12345678limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m;server { ... limit_conn perip 10; limit_conn perserver 100;}There could be several limit_conn directives. the below configuration will limit the number of connections to the server per a client IP and, at the same time, the total number of connections to the virtual server. Another directive we might use is limit_rate, even though it is not in the http_limit_conn_module, it is in the core module. limit_rate rate;Limits the rate of response transmission to a client. The rate is specified in bytes per second. The zero value disables rate limiting. The limit is set per a request, and so if a client simultaneously opens two connections, the overall rate will be twice as much as the specified limit.Context: http, server, location, if in locationDefault: limit_rate 0;Parameter value can contain variables (1.17.0). It may be useful in cases where rate should be limited depending on a certain condition: 123456map $slow $rate { 1 4k; 2 8k;}limit_rate $rate; Look into Modulengx_http_limit_req_moduleThe ngx_http_limit_req_module module is used to limit the request processing rate per a defined key, in particular, the processing rate of requests coming from a single IP address.In default, the nginx is compiled with http_http_limit_req_module . If we don’t want that, we need to use --without-http_limit_req_module.This module uses shared memory for every worker processNow, let’s look at commonly used directives.The limitation is done using the leaky bucket method.Suppose we have a bucket in which we are pouring water in a random order but we have to get water in a fixed rate, for this we will make a hole at the bottom of the bucket. It will ensure that water coming out is in a some fixed rate, and also if bucket will full we will stop pouring in it.The input rate can vary, but the output rate remains constant. Similarly, in networking, a technique called leaky bucket can smooth out bursty traffic. Bursty chunks are stored in the bucket and sent out at an average rate.Now, let’s look at commonly used directives. limit_req_zone Sets parameters for a shared memory zone that will keep states for various keys. In particular, the state stores the current number of excessive requests. The key can contain text, variables, and their combination. Requests with an empty key value are not accounted.limit_req_zone key zone=name:size rate=rate [sync]; Here is an examplelimit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;Here, the states are kept in a 10 megabyte zone “one”, and an average request processing rate for this zone cannot exceed 1 request per second. limit_req_status limit_req_status code;Sets the status code to return in response to rejected requests.Default: limit_req_status 503;Context: http, server, location limit_req_log_level limit_req_log_level info | notice | warn | error;Sets the desired logging level for cases when the server refuses to process requests due to rate exceeding, or delays request processing. Logging level for delays is one point less than for refusals; for example, if “limit_req_log_level notice” is specified, delays are logged with the info level.Default: limit_req_log_level error;Context: http, server, location limit_req limit_req zone=name [burst=number] [nodelay | delay=number]; Sets the shared memory zone and the maximum burst size of requests. If the requests rate exceeds the rate configured for a zone, their processing is delayed such that requests are processed at a defined rate. Excessive requests are delayed until their number exceeds the maximum burst size in which case the request is terminated with an error. By default, the maximum burst size is equal to zero. Here is an example123456limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;server { location /search/ { limit_req zone=one burst=5; }allow not more than 1 request per second at an average, with bursts not exceeding 5 requests. the diff of http_limit_conn_module and http_limit_req_module http_limit_conn_module focus on TCP connection while http_limit_req_module focus on http requests. Look into Module ngx_http_access_module The ngx_http_access_module module allows limiting access to certain client addresses.In default, the nginx is compiled with http_access_module . If we don’t want that, we need to use --without-http_access_module. Access can also be limited by password(Module ngx_http_auth_basic_module), by the result of subrequest(ngx_http_auth_request_module), or by JWT(ngx_http_auth_jwt_module). Simultaneous limitation of access by address and by password is controlled by the satisfy directive. Here are some directives. allow allow address | CIDR | unix: | all;Allows access for the specified network or address. If the special value unix: is specified (1.5.1), allows access for all UNIX-domain sockets.Context: http, server, location, limit_exceptCIDR is Classless Inter-Domain Routing. eg:198.51.100.0/22example:1234allow 192.168.1.0allow 192.168.1.0/24;allow 10.1.1.0/16;allow 2001:0db8::/32; deny address | CIDR | unix: | all;Denies access for the specified network or address. If the special value unix: is specified (1.5.1), denies access for all UNIX-domain sockets.Context: http, server, location, limit_exceptDenies access for the specified network or address. If the special value unix: is specified (1.5.1), denies access for all UNIX-domain sockets.example:12deny 192.168.1.1;deny all; Look into Module ngx_http_auth_basic_module The ngx_http_auth_basic_module module allows limiting access to resources by validating the user name and password using the “HTTP Basic Authentication” protocol. In default, the nginx is compiled with http_auth_basic_module . If we don’t want that, we need to use --without-http_auth_basic_module. Here are some directives. auth_basic auth_basic string | off;Enables validation of user name and password using the “HTTP Basic Authentication” protocol. The string can be custom and will be displayed in the pop-up window auth_basic_user_file auth_basic_user_file file;Specifies a file that keeps user names and passwords, in the following format:1234# commentname1:password1name2:password2:commentname3:password3Since in most cases, the password stored has been encrypted. We could use package htpasswd. For convenience, I use online tool https://hostingcanada.org/htpasswd-generator/ and store the value in /usr/local/nginx/auth/.htpasswd. Then we could modify the nginx.conf123456location / { root html/servername_1; index index.html index.htm; auth_basic &quot;test username and password&quot;; auth_basic_user_file /usr/local/nginx/auth/.htpasswd;} Under this circumstance, when we visit localhost, it pop up a log in window to request me input username and password. If our input can match the record in .htpasswd, it will show the page. Look into ngx_http_auth_request_module The ngx_http_auth_request_module module (1.5.4+) implements client authorization based on the result of a subrequest. If the subrequest returns a 2xx response code, the access is allowed. If it returns 401 or 403, the access is denied with the corresponding error code. Any other response code returned by the subrequest is considered an error. The work mechanism looks like:123456789 response &lt;------------------------------------------------ | request if 2XX |client ----------&gt;Authentication Server;----------&gt;Server | if not 2xx | like 4xx | &lt;--------------------In default, the nginx is compiled with http_auth_request_module . If we want that, we need to use --with-http_auth_request_module. Let us look at some directives auth_request auth_request uri | off;Enables authorization based on the result of a subrequest and sets the URI to which the subrequest will be sent.Default: auth_request off;Context: http, server, locationHere is an example1234567891011location /private/ { auth_request /auth; ...}location = /auth { proxy_pass ... proxy_pass_request_body off; proxy_set_header Content-Length &quot;&quot;; proxy_set_header X-Original-URI $request_uri;}If the /auth return with status code 2XX, then the request will be sent to /private. If not, it will directly return to client. auth_request_set auth_request_set $variable value;Sets the request variable to the given value after the authorization request completes. The value may contain variables from the authorization request, such as $upstream_http_*.Context: http, server, location refer:https://nginx.org/en/docs/https://www.geeksforgeeks.org/leaky-bucket-algorithm/","link":"/2021/09/07/notes-of-nginx-learning-7/"},{"title":"notes of nginx learning (8)","text":"review the status 1xx Informational2xx Successful3xx Redirection4xx Client Error5xx Server Error tips: how to pronounce 1xx,2xxhttps://english.stackexchange.com/questions/404182/how-to-pronounce-1xx-and-4xx Difference between HTTP 1.0 and HTTP 1.1 HTTP/1.0 did not define any 1xx status codes.servers MUST NOT send a 1xx response to an HTTP/1.0 client except under experimental conditions. HTTP/1.1 has 303,307. Let we look into 3XX redirection. code function permanent or temporary allow cache can change request method version 301 Moved Permanently permanent redirection Yes Yes HTTP/1.0 302 Found temporary redirection No Yes HTTP/1.0 303 See Other temporary redirection No Yes HTTP/1.1 307 Temporary Redirect temporary redirection No No HTTP/1.1 308 Permanent Redirect permanent redirection Yes No experimental Look into Module ngx_http_rewrite_moduleThe ngx_http_rewrite_module module is used to change request URI using PCRE(Perl Compatible Regular Expressions) regular expressions, return redirects, and conditionally select configurations. Let us look at the directives return return code [text];return the code ,if it specifies the text, the reponse will contain text in bodyfor example:return 200 &quot;return 200 HTTP code&quot; return code URL;return the code and url. It is mainly used for 3XX redirection .for example: return 302 /bbs return URL;return the url .But the url must start with httpContext: server, location, iffor example:return http://192.168.1.0/bbs Any directives under return won’t be handled. rewrite rewrite regex replacement [flag];If the specified regular expression matches a request URI, URI is changed as specified in the replacement string. The rewrite directives are executed sequentially in order of their appearance in the configuration file. It is possible to terminate further processing of the directives using flags. If a replacement string starts with “http://”, “https://”, or “$scheme”, the processing stops and the redirect is returned to a client.Context: server, location, ifAn optional flag parameter can be one of: flag explanation last After this rule is matched, continue to match new location rewrite rules by the changed URI break Use the URL after rewriting directly, no longer matching the other rewrite statement in location redirect returns a temporary redirect with the 302 code;used if a replacement string does not start with “http://”, “https://”, or “$scheme”; permanent returns a permanent redirect with the 301 code. To better distinguish last and break. Let us look at an example.the nginx.conf will be:1234567891011location /first { rewrite /first(.*) /second$1 last; return 200 'first'; } location /second { rewrite /second(.*) /third$1 break; return 200 'second'; } location /third { return 200 'third'; }Here are some examples and results we could get12curl http://127.0.0.1/first/3.txt#thirdfirst/3.txt =&gt; second/3.txt =&gt;third/3.txt=&gt; then it will go to location /third rather than jumping to return 200 'second'; 12curl http://127.0.0.1/second/3.txt#third second/3.txt =&gt;third/3.txt=&gt; then it will go to location /third rather than jumping to return 200 'second';12curl http://127.0.0.1/third/3.txt#thirdthird/3.txt =&gt; go to location /third If we made a slight change in the configuration file, the result will change.123456789101112location /first { rewrite /first(.*) /second$1 last; return 200 'first'; } location /second { # we remove the break flag rewrite /second(.*) /third$1; return 200 'second'; } location /third { return 200 'third'; } Here are some examples and results we could get12curl http://127.0.0.1/first/3.txt#secondfirst/3.txt =&gt; second/3.txt =&gt;third/3.txt=&gt; then it will go to return 200 'second since there is no break 12curl http://127.0.0.1/second/3.txt#third second/3.txt =&gt;third/3.txt=&gt; then it will go to return 200 'second since there is no break12curl http://127.0.0.1/third/3.txt#thirdthird/3.txt =&gt; go to location /third if if (condition) { ... }The specified condition is evaluated. If true, this module directives specified inside the braces are executed, and the request is assigned the configuration inside the if directive. Configurations inside the if directives are inherited from the previous configuration level.Here are many cases of conditions condition explanation $variable use as variable, when variable equals to zero or false = /!= equal / not equal ~/~= regular expression match / non-regular expression match ~* regular expression, case insensitive -f/!-f checking of a file existence -d/!-d checking of a directory existence -e/!-e checking of a file, directory, or symbolic link existence -e/!-e checking for an executable file examples:12345678910111213141516171819if ($http_user_agent ~ MSIE) { rewrite ^(.*)$ /msie/$1 break;}if ($http_cookie ~* &quot;id=([^;]+)(?:;|$)&quot;) { set $id $1;}if ($request_method = POST) { return 405;}if ($slow) { limit_rate 10k;}if ($invalid_referer) { return 403;} Look into Module ngx_http_autoindex_module The ngx_http_autoindex_module module processes requests ending with the slash character (‘/’) and produces a directory listing. Usually a request is passed to the ngx_http_autoindex_module module when the ngx_http_index_module module cannot find an index file. Let us look at the directives. autoindex autoindex on | off;Enables or disables the directory listing output.Default: autoindex off;Context: http, server, location autoindex_exact_size autoindex_exact_size on | off;For the HTML format, specifies whether exact file sizes should be output in the directory listing, or rather rounded to kilobytes, megabytes, and gigabytes.Default: autoindex_exact_size on;Context: http, server, location autoindex_format autoindex_format html | xml | json | jsonp;Sets the format of a directory listing.Default: autoindex_format html;Context: http, server, location autoindex_localtime autoindex_localtime on | off;Default: autoindex_localtime off;Context: http, server, locationFor the HTML format, specifies whether times in the directory listing should be output in the local time zone or UTC. referhttps://www.w3.org/Protocols/rfc2616/rfc2616-sec10.htmlhttps://www.jianshu.com/p/74bf196b63cchttps://nginx.org/en/docs/http/ngx_http_autoindex_module.html","link":"/2021/09/07/notes-of-nginx-learning-8/"},{"title":"notes of nginx learning (9)","text":"the category of nginx TCP connection variables; variable detail $remote_addr Client IP address $remote_por Client port $server_addr Server IP address $server_port Server Port $server_protocol Server protocol $binary_remote_addr Client IP address in binary format $connection The serial number of the TCP connection, incremented $connection_request Number of current requests for TCP connection $proxy_protocol_addr If the proxy_protocol protocol is used, the address in the protocol is returned, otherwise it is null. $proxy_protocol_port If the proxy_protocol protocol is used, then the port in the protocol is returned, otherwise it returns null. HTTP request variables; variable detail $uri The URL of the request, without parameters $request_uri URL of the request, with parameters $request_method request method $request_length The length of the request, including the request line, request header, and request body $args The full parameter string $arg_name name Specific parameter value $is_args If the URL has arguments, return ? otherwise returns null $query_string Same as args $remote_user The user name passed in by the HTTP Basic Authentication protocol $host First look at the request line, then look at the request header, and finally look for server_name $http_user_agent User browser $http_referer The link from which the request came $http_via Pass through a layer of proxy servers, add the information of the corresponding proxy server $http_x_forwarded_for Get the user’s real IP $http_cookie user cookie Variables generated by nginx processing of HTTP requests; variable detail $request_time The amount of time spent processing the request $request_completion Returns OK when the request is completed, otherwise returns null $server_name Match the server_name value of the request $https return on if https is enabled, otherwise return null $request_filename The full path to the file to be accessed on the disk file system $document_root The path to the folder generated by the URI and root/alias rules $realpath_root Replaces the soft link in document_root with the real path $limit_rate Returns the speed limit at response time nginx return response variables; nginx internal variables; referencehttps://www.cnblogs.com/xiongfanyong/articles/13928494.html","link":"/2021/09/15/notes-of-nginx-learning-9/"},{"title":"notes of nginx learning (10)","text":"what is Reverse proxy server what is Reverse proxy server The reverse proxy server is located between the user and the actual server and provides a relay service for requests and responses.For the user, access to the reverse proxy server is access to the actual server.Reverse proxy can reduce server load consumption and increase efficiency the advantages of reverse proxy server hide the actual server Horizontal expansion of back-end dynamic services Separation of dynamic resources and static resources to improve system robustness Dynamic and Static Separation Deploy static website resources (HTML, JavaScript, CSS, img, etc.) separately from backend applications to improve the speed of user access to static code and reduce access to backend applicationsOne approach is to deploy the static resources on nginx(sometimes we could put static resources on CDN), and the backend project to the application server. Supported protocols when using Nginx as a reverse proxy 12345678910111213# transportation layerclient-----&gt; nginx -----&gt; server tcp tcp udp udpclient-----&gt; nginx -----&gt; server http fastcgi scgi uwsgi http grpc memcached websocket Look into module ngx_http_upstream_module The ngx_http_upstream_module module is used to define groups of servers that can be referenced by the proxy_pass, fastcgi_pass, uwsgi_pass, scgi_pass, memcached_pass, and grpc_pass directives. By default, the nginx is compiled with http_upstream_module . If we don’t want that, we need to use --without-http_upstream_module. Let us look at directives upstream upstream name { ... }Defines a group of servers in a block. Servers can listen on different ports. In addition, servers listening on TCP and UNIX-domain sockets can be mixed.Context: httpHere is the example:1234567upstream backend { server backend1.example.com weight=5; server 127.0.0.1:8080 max_fails=3 fail_timeout=30s; server unix:/tmp/backend3; server backup1.example.com backup;} server server address [parameters];Defines the address and other parameters of a server. The address can be specified as a domain name or IP address, with an optional port, or as a UNIX-domain socket path specified after the “unix:” prefix. If a port is not specified, the port 80 is used. A domain name that resolves to several IP addresses defines multiple servers at once.Context: upstream Here is the parameters parameter detail weight=number sets the weight of the server, by default, 1. The weight is larger, the nginx will giver more requests to the server max_conns=number limits the maximum number of simultaneous active connections to the proxied server.Default value is zero, meaning there is no limit. If the server group does not reside in the shared memory, the limitation works per each worker process. max_fails=number sets the number of unsuccessful attempts to communicate with the server that should happen in the duration set by the fail_timeout parameter to consider the server unavailable for a duration also set by the fail_timeout parameter. fail_timeout=time the time during which the specified number of unsuccessful attempts to communicate with the server should happen to consider the server unavailable; and the period of time the server will be considered unavailable. By default, the parameter is set to 10 seconds||backup|marks the server as a backup server. It will be passed requests when the primary servers are unavailable.||down|marks the server as a backup server. It will be passed requests when the primary servers are unavailable.||resolve|monitors changes of the IP addresses that correspond to a domain name of the server, and automatically modifies the upstream configuration without the need of restarting nginx (1.5.12). The server group must reside in the shared memory. In order for this parameter to work, the resolver directive must be specified in the http block or in the corresponding upstream block.||route=string|sets the server route name.||service=name|enables resolving of DNS SRV records and sets the service name (1.9.13). In order for this parameter to work, it is necessary to specify the resolve parameter for the server and specify a hostname without a port number.||slow_start=time|sets the time during which the server will recover its weight from zero to a nominal value, when unhealthy server becomes healthy, or when the server becomes available after a period of time it was considered unavailable. Default value is zero, i.e. slow start is disabled.||drain|puts the server into the “draining” mode (1.13.6). In this mode, only requests bound to the server will be proxied to it.| zone zone name [size];Defines the name and size of the shared memory zone that keeps the group’s configuration and run-time state that are shared between worker processes. Several groups may share the same zone. In this case, it is enough to specify the size only once.Context: upstream keepalive keepalive connections;The connections parameter sets the maximum number of idle keepalive connections to upstream servers that are preserved in the cache of each worker process. When this number is exceeded, the least recently used connections are closed.Context: upstreamFor example:keepalive 32 keepalive_requests keepalive_requests numberSets the maximum number of requests that can be served through one keepalive connection. After the maximum number of requests is made, the connection is closed.Default: keepalive_requests 1000;Context: upstream keepalive_timeout keepalive_timeout timeout;Sets a timeout during which an idle keepalive connection to an upstream server will stay open.Default: keepalive_timeout 60s;Context: upstream And some other directives are related to load balance.hash,ip_hash,least_conn,least_time,random. We will talk about it later. queue[in commercial version only ] queue number [timeout=time];If an upstream server cannot be selected immediately while processing a request, the request will be placed into the queue. The directive specifies the maximum number of requests that can be in the queue at the same time.Context: upstreamFor example:queue 100 timeout =30s Look into proxy_pass directive proxy_pass is in ngx_http_proxy_module moduleThe ngx_http_proxy_module module allows passing requests to another server.By default, it is compiled with ngx_http_proxy_module.If we don’t want to use the module --without-ngx_http_proxy_module proxy_pass URL;Context: location, if in location, limit_exceptSets the protocol and address of a proxied server and an optional URI to which a location should be mapped. As a protocol, “http” or “https” can be specified. The address can be specified as a domain name or IP address, and an optional port: proxy_pass http://localhost:8000/uri/;or as a UNIX-domain socket path specified after the word “unix” and enclosed in colons: proxy_pass http://unix:/tmp/backend.socket:/uri/;If a domain name resolves to several addresses, all of them will be used in a round-robin fashion. In addition, an address can be specified as a server group. Let us look at the difference between URL ends with / and without / If the url ends without /,nginx will modify the url by deleting the location url in proxy url.If the url ends without /,nginx will not modify the URL and directly send it to upstream server. For example,if users use url bbs/abc/test.html,if the config file is123location /bbs/{ proxy_pass http://127.0.0.1:8000}the url arrive at upstream server will be bbs/abc/test.html. if the config file is123location /bbs/{ proxy_pass http://127.0.0.1:8000/}the url arrive at upstream server will be abc/test.html. An example of reverse proxy by nginx I don’t want to directly change the nginx.conf every time. let us introduce a directive include include file | mask;Includes another file, or files matching the specified mask, into configuration. Included files should consist of syntactically correct directives and blocks.Context: any So to create a new server block, we just need a include directive in nginx.conf like include servers/*.conf;. So every time we want to create a new server, we only need to create a file in servers. create a file named proxy.conf in server folder 123456789101112upstream backend { # the server can provide function server 127.0.0.1:8000;}server { listen 8081; server_name localhost; location /helloworld { proxy_pass http://backend/v1/users/helloworld; }} Notice: we may need to add backend to allow_hosts in django(since the url is provided by django). Or we will face error Invalid HTTP_HOST header: 'backend' Then we could visit localhost:8081/helloworld Some directives related to proxy proxy_request_buffering proxy_request_buffering on | off;Enables or disables buffering of a client request body. When buffering is enabled, the entire request body is read from the client before sending the request to a proxied server. It meets the high throughput requirements and it is suitable when the concurrent processing capability of upstream services is low When buffering is disabled, the request body is sent to the proxied server immediately as it is received. In this case, the request cannot be passed to the next server if nginx already started sending the request body. It will reduce the responses time and save disk space. When HTTP/1.1 chunked transfer encoding is used to send the original request body, the request body will be buffered regardless of the directive value unless HTTP/1.1 is enabled for proxying.Default: proxy_request_buffering on;Context: http, server, location client_max_body_size client_max_body_size size;Sets the maximum allowed size of the client request body. If the size in a request exceeds the configured value, the 413 (Request Entity Too Large) error is returned to the client. Please be aware that browsers cannot correctly display this error. Setting size to 0 disables checking of client request body size.Default: client_max_body_size 1m;Context: http, server, location client_body_buffer_size client_body_buffer_size size;Sets buffer size for reading client request body. In case the request body is larger than the buffer, the whole body or only its part is written to a temporary file(defined by client_body_temp_path). By default, buffer size is equal to two memory pages. This is 8K on x86, other 32-bit platforms, and x86-64. It is usually 16K on other 64-bit platforms. Default: client_body_buffer_size 8k|16k;Context: http, server, location client_body_temp_path client_body_temp_path path [level1 [level2 [level3]]];Default: client_body_temp_path client_body_temp;Context: http, server, locationDefines a directory for storing temporary files holding client request bodies. Up to three-level subdirectory hierarchy can be used under the specified directory. For example, in the following configuration client_body_temp_path /spool/nginx/client_temp 1 2; a path to a temporary file might look like this: /spool/nginx/client_temp/7/45/00000123457 Here, if there are values ​​for level1,2,3, it means that there are subdirectories at level 1, level 2, and level 3.Directory names are named by numbers, so the specific value here is the number of digits representing the directory name client_body_in_single_buffer client_body_in_single_buffer on | off;Determines whether nginx should save the entire client request body in a single buffer. The directive is recommended when using the $request_body variable, to save the number of copy operations involved.Default: client_body_in_single_buffer off;Context: http, server, location client_body_in_file_only client_body_in_file_only on | clean | off; Determines whether nginx should save the entire client request body into a file. This directive can be used during debugging, or when using the $request_body_file variable, or the $r-&gt;request_body_file method of the module ngx_http_perl_module.When set to the value on, temporary files are not removed after request processing.The value clean will cause the temporary files left after request processing to be removed. Default: client_body_in_file_only off;Context: http, server, location client_body_timeout client_body_timeout time;Default: client_body_timeout 60s;Context: http, server, locationDefines a timeout for reading client request body. The timeout is set only for a period between two successive read operations, not for the transmission of the whole request body. If a client does not transmit anything within this time, the request is terminated with the 408 (Request Time-out) error. How does nginx change the request message sent upstream server when we use nginx for revese proxy Look at some directives may work proxy_method proxy_method method;Specifies the HTTP method to use in requests forwarded to the proxied server instead of the method from the client request. Parameter value can contain variables (1.11.6).Context: http, server, location proxy_http_version proxy_http_version 1.0 | 1.1;Sets the HTTP protocol version for proxying. By default, version 1.0 is used. Version 1.1 is recommended for use with keepalive connections and NTLM authentication.Default: proxy_http_version 1.0;Context: http, server, location proxy_set_header proxy_set_header field value;Allows redefining or appending fields to the request header passed to the proxied server.Default:12proxy_set_header Host $proxy_host;`proxy_set_header Connection close;Context: http, server, location proxy_pass_request_headers proxy_pass_request_headers on | off;Indicates whether the header fields of the original request are passed to the proxied server.Default: proxy_pass_request_headers on;Context: http, server, location proxy_set_body proxy_set_body value;Allows redefining the request body passed to the proxied server. The value can contain text, variables, and their combination.Context: http, server, location proxy_pass_request_body proxy_pass_request_body on | off;Default: proxy_pass_request_body on;Context: http, server, locationIndicates whether the original request body is passed to the proxied server. Details of establishing a connection between Nginx and upstream server in a proxy scenario Let we look at some directives proxy_connect_timeout proxy_connect_timeout time;Defines a timeout for establishing a connection with a proxied server. It should be noted that this timeout cannot usually exceed 75 secondsDefault: proxy_connect_timeout 60s;Context: http, server, location proxy_socket_keepalive proxy_socket_keepalive on | off;Configures the “TCP keepalive” behavior for outgoing connections to a proxied server. By default, the operating system’s settings are in effect for the socket. If the directive is set to the value “on”, the SO_KEEPALIVE socket option is turned on for the socket.Default: proxy_socket_keepalive off;Context: http, server, location Note: there is a difference between HTTP KEEP-ALIVE and TCP KEEPALIVE.https://www.jianshu.com/p/cd71efa49d63 proxy_send_timeout proxy_send_timeout time;Sets a timeout for transmitting a request to the proxied server. The timeout is set only between two successive write operations, not for the transmission of the whole request. If the proxied server does not receive anything within this time, the connection is closed.Default: proxy_send_timeout 60s;Context: http, server, location proxy_ignore_client_abort on | off; proxy_ignore_client_abort on | off;Default: proxy_ignore_client_abort off;Determines whether the connection with a proxied server should be closed when a client closes the connection without waiting for a response.Context: http, server, location referencehttps://nginx.org/en/docs/dirindex.html","link":"/2021/09/15/notes-of-nginx-learning-10/"},{"title":"notes of nginx learning (11)","text":"what is Load Balance Efficiently distributing incoming network traffic across a group of backend servers, also known as a server farm or server pool. The mechanism can be seen as1234 requestuser -------&gt; load balancer ------&gt; server 1 &lt;------- ------&gt; server 2 response ------&gt; server 3 Some directives we may use for load Balance upstream name { ... }server address [parameters];keepalive connections;keepalive_requests number;keepalive_timeoutqueue how to realize the load balancer in nginx Firstly, let we create a file named loadbalancer.conf(we will include it in nginx.conf)The file looks like:123456789101112131415161718192021222324252627282930313233343536373839upstream servers { server 127.0.0.1:8021 weight=20; server 127.0.0.1:8022 weight=5; server 127.0.0.1:8023 weight=1 ;}server { listen 8080; server_name localhost; location /loadbalancer { proxy_pass http://servers; }}server { listen 8021; location / { return 200 'the port is 8021'; }}server { listen 8022; location / { return 200 'the port is 8022'; }}server { listen 8023; location / { return 200 'the port is 8023'; }}After reloading the configuration file, we could use curl localhost:8080/loadbalancer to test the change. We could see that the port is 8021 appears most, followed by the port is 8022 because of the weight we set. load balance algorithm - round robin Round Robin – Requests are distributed evenly across the servers, with server weights taken into consideration. This method is used by default.The example:123456upstream servers { server 127.0.0.1:8021 weight=20; server 127.0.0.1:8022 weight=5; server 127.0.0.1:8023 weight=1;} load balance algorithm - hash algorithm what is hash function A hash function is any function that can be used to map data of arbitrary size to fixed-size values. And it is irreversible The most important thing about hashing is that: The same input must yield the same output; Different inputs are likely to produce different outputs The purpose of a hash algorithm is to verify that the original data has not been tampered with directive used in load balance algorithm based on hashing. hash hash key [consistent];Specifies a load balancing method for a server group where the client-server mapping is based on the hashed key value.The key can contain text, variables, and their combinations. Note that adding or removing a server from the group may result in remapping most of the keys to different servers.Context: upstream ip_hash ip_hash; Specifies that a group should use a load balancing method where requests are distributed between servers based on client IP addresses. The first three octets of the client IPv4 address, or the entire IPv6 address, are used as a hashing key. The method ensures that requests from the same client will always be passed to the same server except when this server is unavailable.Context: upstream to implement the load balance algorithm in nginx Firstly, let we create a file named balance_hash.conf(we will include it in nginx.conf)The file looks like: 123456789101112131415161718192021222324upstream backend { server 127.0.0.1:8081; server 127.0.0.1:8082; hash $request_uri;}server { listen 8080; server_name localhost; location /hash { proxy_pass http://backend; }}server { listen 8081; location / { return 200 'it listens on port 8081'; }}server { listen 8082; location / { return 200 'it listens on port 8082'; }} In this file, we use $request_uri as the key for hashing. And we could test like curl localhost:8080/hash/apple.html or curl localhost:8080/hash/test.html to see whether the same url will be handled by the same server. load balance algorithm - Least Connections A request is sent to the server with the least number of active connections, again with server weights taken into consideration:example: 12345 upstream backend { least_conn; server backend1.example.com; server backend2.example.com;} fault-tolerant mechanism in load balancer When one backend server doesn’t give expected result to nginx, it will send the request to another server.Here are some directives we may use. proxy_next_upstream proxy_next_upstream error | timeout | invalid_header | http_500 | http_502 | http_503 | http_504 | http_403 | http_404 | http_429 | non_idempotent | off ...;Default: proxy_next_upstream error timeout;Context: http, server, location parameter detail error an error occurred while establishing a connection with the server, passing a request to it, or reading the response header; timeout a timeout has occurred while establishing a connection with the server, passing a request to it, or reading the response header; invalid_header a server returned an empty or invalid response; http_500 a server returned a response with the code 500; http_502 a server returned a response with the code 502; http_503 a server returned a response with the code 503; http_504 a server returned a response with the code 504; http_403 a server returned a response with the code 403; http_404 a server returned a response with the code 404; http_429 a server returned a response with the code 429; non_idempotent normally, requests with a non-idempotent method (POST, LOCK, PATCH) are not passed to the next server if a request has been sent to an upstream server (1.9.13); enabling this option explicitly allows retrying such requests; off disables passing a request to the next server. For exampleproxy_next_upstream error timeout http_500; proxy_next_upstream_timeout proxy_next_upstream_timeout time;Limits the time during which a request can be passed to the next server. The 0 value turns off this limitation.Default: proxy_next_upstream_timeout 0;Context: http, server, location proxy_next_upstream_tries proxy_next_upstream_tries number;Limits the number of possible tries for passing a request to the next server. The 0 value turns off this limitation.Default: proxy_next_upstream_tries 0;Context: http, server, location proxy_intercept_errors proxy_intercept_errors on | off;Determines whether proxied responses with codes greater than or equal to 300 should be passed to a client or be intercepted and redirected to nginx for processing with the error_page directive.If you have set proxy_next_upstream, it will not work.Default: proxy_intercept_errors off;Context: http, server, location referencehttps://nginx.org/en/docs/dirindex.html","link":"/2021/09/15/notes-of-nginx-learning-11/"},{"title":"defer in go","text":"what is deferA defer statement defers the execution of a function until the surrounding function returns.eg:defer fmt.Print(&quot;Hello World&quot;) execution of deferThe order of execution of defer is: defer latest will be executed first.The execution order of defer is after return, but before the return value is returned to the caller, so using defer can serve the purpose of modifying the return value.Here is an example123456789101112func test4() (x int) { defer func() { x = 10 }() x = 7 return 9}func main() { fmt.Print(test4()) // 10}Here is an example with a little difference12345678910111213func test4() int { var x int defer func() { x = 10 }() x = 7 return x}func main() { fmt.Print(test4())//9}Here is the detail of return and defer Assign x to the return value (which can be interpreted as Go automatically creating a parameter retValue, equivalent to executing retValue = x) Check if there is a defer, and if so, execute return the retValue just created In that case, for the first example, since we define the return parameter name at vary beginning, it will not create a retValue.So the defer directly change the return value. While the second example just change the value of x rather than the return value. And even though when panic happens, defer will execute. When os.Exit() was hit, defer will not execute. the parameter in deferAn example from stackoverflow.123456789func test4() (x int) { defer func(n int) { fmt.Printf(\"in defer x as parameter: x = %d\\n\", n) fmt.Printf(\"in defer x after return: x = %d\\n\", x) }(x) x = 7 return 9} invoke defer, evaluating the value of n, n = x = 0, so x as parameter is 0. execute defer, pushing func(n int)(0) onto stack. execute func(n int)(0) after return 9, n in fmt.Printf(&quot;in defer x as parameter: x = %d\\n&quot;, n) has been evaluated, x in fmt.Printf(&quot;in defer x after return: x = %d\\n&quot;, x) will be evaluated now, x is 9。 Some notes: the parameter’s value has been set when define the defer pass parameter to function is value-copy. x and n has different address It is related to Function closures. Anonymous function that references variables declared outside of the function itself. Anonymous function can gain and change the value of references variables. the order of defer and err handler mattersThe code below will panic because we fail to open the file but still want to close it.12345678910func do() error { res, err := http.Get(\"http://notexists\") defer res.Body.Close() if err != nil { return err } // ..code... return nil}//panic: runtime error: invalid memory address or nil pointer dereferenceWe slightly change the below code1234567891011func do() error { res, err := http.Get(\"http://notexists\") if res != nil { defer res.Body.Close() } if err != nil { return err } // ..code... return nil} some tricks when using Close and defer don’t close twiceHere is an example close twice. As we’ve seen before, when defers run, only the last variable gets used. So, f variable will become the last one (another-book.txt). And, both defers will see it as the last one. 12345678910111213141516171819202122func do() error { f, err := os.Open(\"book.txt\") if err != nil { return err } defer func() { if err := f.Close(); err != nil { // log etc } }() // ..code... f, err = os.Open(\"another-book.txt\") if err != nil { return err } defer func() { if err := f.Close(); err != nil { // log etc } }() return nil} check the error when closingwe’d better write like this 12345defer func() { if err := f.Close(); err != nil { // log etc } }() referencehttps://www.jianshu.com/p/79c029c0bd58https://blog.learngoprogramming.com/5-gotchas-of-defer-in-go-golang-part-iii-36a1ab3d6ef1","link":"/2021/11/20/defer-in-go/"},{"title":"parallel in go","text":"what is goroutineA goroutine is a lightweight thread managed by the Go runtime.go f(x, y, z) will start a goroutine for function f. how goroutine actually workhttps://riteeksrivastava.medium.com/a-complete-journey-with-goroutines-8472630c7f5c how goroutine communicate - channelch = make(chan int)Only int type can be sent to channelBy default, sends and receives block until the other side is ready. This allows goroutines to synchronize without explicit locks or condition variables.Channels can be buffered. Provide the buffer length as the second argument to make to initialize a buffered channel:ch := make(chan int, 100) wait in goroutine use channel 12345678910111213func main() { c := make(chan bool, 100) for i := 0; i &lt; 100; i++ { go func(i int) { fmt.Println(i) c &lt;- true }(i) } for i := 0; i &lt; 100; i++ { &lt;-c }} This method is a little better but sacrifices some flexibility. For instance, it introduces some additional weirdness in the case that we don’t actually know how many goroutines we want to spin up ahead of time. use waitGroup() 1234567891011121314func main() { wg := sync.WaitGroup{} for i := 0; i &lt; 10; i++ { wg.Add(1) go func(i int) { fmt.Println(i) wg.Done() }(i) } wg.Wait() fmt.Println(\"end\")}} return value in goroutineWe cannot get the return value from goroutine. In that case, we could use channel to get the value.To get the error return from the goroutine we could use errgroup 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import ( \"sync\" \"runtime\" \"fmt\" \"errors\" \"golang.org/x/sync/errgroup\")func try() (bool, error) { errg := new(errgroup.Group) s := []int{0,1,2,3,4,5} for i, val := range s { i := i val := val errg.Go(func() error { return func(i int, val int) error { if i == 3 { return errors.New(\"one error\") } else { return nil } }(i, val) }) } if err := errg.Wait(); err != nil { // handle error } return true, nil }``` `func (g *Group) Go(f func() error)`Go calls the given function in a new goroutine.The first call to return a non-nil error cancels the group; its error will be returned by Wait.`func (g *Group) Wait() error`Wait blocks until all function calls from the Go method have returned, then returns the first non-nil error (if any) from them.## mutex and lock in go Go's standard library provides mutual exclusion with `sync.Mutex` and its two methods:`Lock``Unlock` // SafeCounter is safe to use concurrently.type SafeCounter struct { mu sync.Mutex v map[string]int} // Inc increments the counter for the given key.func (c *SafeCounter) Inc(key string) { c.mu.Lock() // Lock so only one goroutine at a time can access the map c.v. c.v[key]++ c.mu.Unlock()}123Also golang support lock for read and lock for write lock for read: multiple goroutine could read while none can write lock for write: only one can write, none can readsync.RWMutex{} // create mutex for read-write lock func (*RWMutex) Lock // lock for write func (*RWMutex) Unlock // unlock for write func (*RWMutex) RLock // lock for read func (*RWMutex) RUnlock // unlock for read``` map in gorouineBefore Golang 1.6, concurrent read is OK, concurrent write is not OK, but write and concurrent read is OK. Since Golang 1.6, map cannot be read when it’s being written.We have some solutions to this: use sync.Map() instead of map use mutex when read or write. atom.Valueshttps://studygolang.com/articles/23242?fr=sidebarside note:when we need to check the type when we load value from atom. referencehttps://nathanleclaire.com/blog/2014/02/15/how-to-wait-for-all-goroutines-to-finish-executing-before-continuing/","link":"/2021/11/27/parallel-in-go/"},{"title":"blueprint in flask","text":"what is blueprintA Blueprint object works similarly to a Flask application object, but it is not actually an application. Rather it is a blueprint of how to construct or extend an application. why use blueprintblueprint is a very useful way to modularize your application.When multiple people collaborate, each person is responsible for writing different parts, but they write codes in the same file (view.py), causing the problem of ignoring other people’s commits when updating the file and possibly overwriting other people’s code, this is where we decided to use blueprints to solve this problem, each person only needs to focus on the code file they are responsible for.The structure of from12345678910111213Appserver/ ├── App │ ├── static/ │ ├── templates/ │ ├── __init__.py │ └── view.py # write routes here ├── run.py ├── config.py ├── requirements └── README.md ``` toAppserver/├── app│ ├── static/│ ├── templates/│ ├── json_test/│ ├── init.py│ ├── store_info.py # blueprint│ ├── user_info.py # blueprint│ ├── order_info.py # blueprint│ ├── store_by_id.py # blueprint│ ├── orders_by_user_id.py # blueprint│ ├── type_search.py # blueprint│ ├── user_login.py # blueprint│ ├── models.py│ ├── createdb.py│ └── view.py # register blueprint├── config.py├── run.py├── requirements└── README.md 123456## How to use it 1. Making a Flask Blueprint```pythonfrom flask import Blueprintexample_blueprint = Blueprint('example_blueprint', __name__) Note that in the above code, some arguments are specified when creating the Blueprint object. The first argument, &quot;example_blueprint&quot;, is the Blueprint’s name, which is used by Flask’s routing mechanism. The second argument, __name__, is the Blueprint’s import name, which Flask uses to locate the Blueprint’s resources.There are other optional arguments that you can provide to alter the Blueprint’s behavior: |paramters | meaning||static_folder| the folder where the Blueprint’s static files can be found||static_url_path| the URL to serve static files from||template_folder| the folder containing the Blueprint’s templates||url_prefix| the path to prepend to all of the Blueprint’s URLs||subdomain| the subdomain that this Blueprint’s routes will match on by default||url_defaults| a dictionary of default values that this Blueprint’s views will receive||root_path| the Blueprint’s root directory path, whose default value is obtained from the Blueprint’s import name| use the decorator123@example_blueprint.route('/')def index(): return \"This is an example app\" You decorate index() using example_blueprint.route and associate the function to the URL /. Blueprint objects also provide other methods that you may find useful: .errorhandler() to register an error handler function.before_request() to execute an action before every request.after_request() to execute an action after every request.app_template_filter() to register a template filter at the application level referemcehttps://blog.csdn.net/m0_38088298/article/details/8086694","link":"/2021/12/12/blueprint-in-flask/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"linked list","slug":"linked-list","link":"/tags/linked-list/"},{"name":"Graph","slug":"Graph","link":"/tags/Graph/"},{"name":"tool","slug":"tool","link":"/tags/tool/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"dynamic programming","slug":"dynamic-programming","link":"/tags/dynamic-programming/"},{"name":"sort","slug":"sort","link":"/tags/sort/"},{"name":"computing","slug":"computing","link":"/tags/computing/"},{"name":"bit manipulation","slug":"bit-manipulation","link":"/tags/bit-manipulation/"},{"name":"collections","slug":"collections","link":"/tags/collections/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"sorting","slug":"sorting","link":"/tags/sorting/"},{"name":"regular expression","slug":"regular-expression","link":"/tags/regular-expression/"},{"name":"multiprocess","slug":"multiprocess","link":"/tags/multiprocess/"},{"name":"multithread","slug":"multithread","link":"/tags/multithread/"},{"name":"binary search","slug":"binary-search","link":"/tags/binary-search/"},{"name":"graph","slug":"graph","link":"/tags/graph/"},{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"learning note","slug":"learning-note","link":"/tags/learning-note/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"blueprint","slug":"blueprint","link":"/tags/blueprint/"},{"name":"flask","slug":"flask","link":"/tags/flask/"}],"categories":[{"name":"technology","slug":"technology","link":"/categories/technology/"},{"name":"math","slug":"technology/math","link":"/categories/technology/math/"},{"name":"math","slug":"math","link":"/categories/math/"}]}